{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to Euclidean Tensor Analysis Amuthan A. Ramabathiran , Dept. of Aerospace Engineering, IIT Bombay. WORK IN PROGRESS! These notes are in a continuous process of rewriting at the moment, and will evolve over the course of this semester. Please check back to see the updated version. The expected time of completion of the notes is by the end of October, 2019. Note! These notes use mathematical equations rendered by MathJax. Depending on your browser, or the number of tabs open on your browser, there might be a delay in loading the equations in the right fonts. About these notes These notes are primarily meant for students taking AE 639: Continuum Mechanics at IIT Bombay . Note that these notes are in progress , and are likely to change significantly until they reach a stable version. Please send any comments/questions/corrections to amuthan at aero.iitb.ac.in . Please feel free to write to at amuthan at aero.iitb.ac.in in case you have something you wish to say. You can find more about my research interests and details of other courses I teach on my IIT Bombay webpage .","title":"Home"},{"location":"#introduction-to-euclidean-tensor-analysis","text":"Amuthan A. Ramabathiran , Dept. of Aerospace Engineering, IIT Bombay. WORK IN PROGRESS! These notes are in a continuous process of rewriting at the moment, and will evolve over the course of this semester. Please check back to see the updated version. The expected time of completion of the notes is by the end of October, 2019. Note! These notes use mathematical equations rendered by MathJax. Depending on your browser, or the number of tabs open on your browser, there might be a delay in loading the equations in the right fonts.","title":"Introduction to Euclidean Tensor Analysis"},{"location":"#about-these-notes","text":"These notes are primarily meant for students taking AE 639: Continuum Mechanics at IIT Bombay . Note that these notes are in progress , and are likely to change significantly until they reach a stable version. Please send any comments/questions/corrections to amuthan at aero.iitb.ac.in . Please feel free to write to at amuthan at aero.iitb.ac.in in case you have something you wish to say. You can find more about my research interests and details of other courses I teach on my IIT Bombay webpage .","title":"About these notes"},{"location":"calculus/","text":"This appendix reviews elementary topics in single and multivariable calculus that are necessary to understand the theory of Euclidean tensor analysis developed in the main part of the notes. Single variable calculus We start with a brief review of single variable calculus. The primary object of study are functions of the form f:I \\subseteq \\mathbb{R} \\to \\mathbb{R} that map an open interval I = (a,b) \\subseteq \\mathbb{R} into the set of real numbers. Thus given any x \\in I , f(x) is a real number. Continuity Recall that f:I \\to \\mathbb{R} is said to be continuous at x \\in I if, given any \\epsilon > 0 , there exists \\delta > 0 such that |y - x| < \\delta \\quad\\Rightarrow\\quad |f(y) - f(x)| < \\epsilon. The idea is that as the point y approaches x , the value of the function f(y) approaches f(x) . In other words, there are no abrupt jumps in the function value at x . Note that the foregoing definition can be written equivalently as \\lim_{y \\to x} f(y) = f(x). The function is said to be continuous over the whole of I if it is continuous at every x \\in I . The set of all continuous functions on I is denoted as C^0(I,\\mathbb{R}) . Differentiation The function f:I \\to \\mathbb{R} is said to be differentiable at x \\in I if the limit \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} exists. In this case, the is limit is called the derivative of f at x and is denoted variously as f'(x), \\quad \\dot{f}(x), \\quad \\frac{df(x)}{dx}, \\quad Df(x), depending on the context. The function f is said to be differentiable on I if it is differentiable at every x \\in I . The set of all differentiable functions on I whose derivative is also continuous on I is denoted as C^1(I,\\mathbb{R}) . Every differentiable function is continuous. To see this suppose that f:I \\to \\mathbb{R} is a differentiable function. Then, at any x \\in I , it can be seen that \\lim_{y \\to x} f(y) - f(x) = \\lim_{y \\to x} (y - x)\\frac{f(y) - f(x)}{y - x} = 0 \\cdot f'(x) = 0. This shows that \\lim_{y \\to x} f(y) = f(x) , thereby establishing the claim. Note in particular that C^1(I,\\mathbb{R}) \\subsetneq C^0(I,\\mathbb{R}) . Higher order derivatives can be defined similarly. For instance, a differentiable function f:I \\to \\mathbb{R} is said to be twice differentiable at x \\in I if the function f'(x):I \\to \\mathbb{R} is differentiable at x . In this case, the second derivative of f at x is written, variously, as f''(x), \\quad \\ddot{f}(x), \\quad \\frac{d^2f(x)}{dx^2}, \\quad D^2f(x). The function f:I \\to \\mathbb{R} is said to be twice differentiable on I if its second derivative exists at every x \\in I . The set of all twice differentiable functions on I whose second derivative is also continuous on I is denoted as C^2(I,\\mathbb{R}) . Third and higher derivatives of f are defined in an analogous manner. For instance, the set C^k(I,\\mathbb{R}) consists of all functions f:I \\to \\mathbb{R} that are k times differentiable on I , and whose k^{\\text{th}} derivative is continuous on I . The function f:I \\to \\mathbb{R} is said to be smooth on I if it is an element of C^k(I,\\mathbb{R}) for every k \\in \\mathbb{N} . Properties of the derivative We will now recall a few elementary properties of the derivative. The derivative is a linear operation in the following sense: given any differentiable functions f:I \\to \\mathbb{R} , g:I \\to \\mathbb{R} and constants a,b \\in \\mathbb{R} , it follows from an elementary argument from the definition of the derivative that (a f + b g)'(x) = a f'(x) + b g'(x). The product rule of differentiation states that (fg)'(x) = f'(x) g(x) + f(x) g'(x). The product rule is also a simple consequence of the definition of the derivative. An important property of the derivative pertains to composition of functions. For simplicity, let us consider differentiable functions f, g: \\mathbb{R} \\to \\mathbb{R} . Then, the chain rule of differentiation states that (f \\circ g)'(x) = f'(g(x)) g'(x). Proof To see this, note that \\begin{split} (f \\circ g)'(x) &= \\lim_{h \\to 0} \\frac{f(g(x + h)) - f(g(x))}{h}\\\\ &= \\lim_{h \\to 0} \\frac{f(g(x + h)) - f(g(x))}{g(x + h) - g(x)}\\frac{g(x + h)) - g(x)}{h}. \\end{split} Noting that the differentiability of g implies that \\lim_{h \\to 0} g(x + h) - g(x) = 0, and introducing the notation y = g(x + h) - g(x) , we see that \\begin{split} (f \\circ g)'(x) &= \\lim_{y \\to 0} \\frac{f(g(x) + y) - f(g(x))}{y}\\lim_{h \\to 0}{g(x + h) - g(x)}\\frac{g(x + h)) - g(x)}{h}\\\\ &= f'(g(x))g'(x). \\end{split} This proves the chain rule of differentiation. Example As a simple application of the chain rule, consider the case of a bijective function f:\\mathbb{R} \\to \\mathbb{R} such that both f and f^{-1} are differentiable. The chain rule of differentiation can be used to related the derivatives of f and f^{-1} as follows: for any x \\in \\mathbb{R} , (f^{-1} \\circ f)(x) = x \\quad\\Rightarrow\\quad (f^{-1})'(f(x))f'(x) = 1 \\quad\\Rightarrow\\quad (f^{-1})'(f(x)) = \\frac{1}{f'(x)}. A similar expression can be obtained by differentiating the identity (f \\circ f^{-1})(x) = x . Multivariable calculus Let us now present certain elementary facts about multivariable calculus. To keep the discussion elementary, let us consider first the case of a real valued function of two real variables f:\\mathbb{R}^2 \\to \\mathbb{R} . The development here parallels that presented in the single variable case. The general case of a function of many variables is considered subsequently. Continuity The function f:R \\subseteq \\mathbb{R}^2 \\to \\mathbb{R} , where R is the open rectangle (x_1,x_2) \\times (y_1,y_2) \\subseteq \\mathbb{R}^2 , is said to be continuous at (x,y) \\in R if, for every \\epsilon > 0 , there exists a \\delta > 0 such that \\sqrt{(\\bar{x} - x)^2 + (\\bar{y} - y)^2} < \\delta \\quad\\Rightarrow\\quad |f(\\bar{x},\\bar{y}) - f(x,y)| \\le \\epsilon. Notice how the Euclidean distance figures in this definition. As before, the definition of continuity can be equivalently restated as \\lim_{(\\bar{x},\\bar{y}) \\to (x,y)} f(\\bar{x},\\bar{y}) = f(x,y). The intuitive explanation is the same as before: if f is continuous at (x,y) \\in R , then the value of f close to (x,y) is close to f(x,y) . In other words, there are no gaps in the range of f . The function f is said to be continuous on R if it is continuous at every (x,y) \\in R . In this case, the set of all continuous functions on \\mathbb{R} is written as C^0(R,\\mathbb{R}) . Differentiability Let us now study the differentiability of the function f:R \\to \\mathbb{R} . Rather than develop the general theory (this is presented in the main part of these notes), the presentation here provides an elementary account of the partial derivatives of f . The function f:R \\to \\mathbb{R} takes two real arguments and returns a real number. Fixing one of the arguments, we get a real-valued function of a real variable, which can be differentiated as studied before, in the context of single variable calculus. In the present case, fixing one of the arguments and letting the other vary results in two different limits: \\lim_{h \\to 0} \\frac{f(x + h,y) - f(x,y)}{h}, \\qquad \\lim_{h \\to 0} \\frac{f(x,y + h) - f(x,y)}{h}, where (x,y) \\in R and h \\in \\mathbb{R} . When these limits exist, they are called the partial derivatives of f . Specifically, the partial derivatives of f:R \\to \\mathbb{R} at (x,y) \\in \\mathbb{R} are defined as \\begin{split} \\frac{\\partial f(x,y)}{\\partial x} &= \\lim_{h \\to 0} \\frac{f(x + h,y) - f(x,y)}{h},\\\\ \\frac{\\partial f(x,y)}{\\partial y} &= \\lim_{h \\to 0} \\frac{f(x,y + h) - f(x,y)}{h}. \\end{split} The following alternative notations will be used for partial derivatives: \\begin{split} \\frac{\\partial f(x,y)}{\\partial x} &= \\partial_x f(x,y) = \\partial_1 f(x,y) = D_x f(x,y) = D_1 f(x,y),\\\\ \\frac{\\partial f(x,y)}{\\partial y} &= \\partial_y f(x,y) = \\partial_2 f(x,y) = D_y f(x,y) = D_2 f(x,y). \\end{split} The foregoing discussion can be generalized to functions of the form \\mathsf{f}:\\mathbb{R}^n \\to \\mathbb{R}^m . Note that the function \\mathsf{f} can be written as a collection of m functions \\mathsf{f} = (f_1, \\ldots, f_m), where each f_i:\\mathbb{R}^n \\to \\mathbb{R} , i = 1, \\ldots, m is a function that takes an m -tuple of real numbers and returns a real number. The partial derivaties of \\mathsf{f} at \\mathsf{x} = (x_1, \\ldots, x_m) \\in \\mathbb{R}^m can be arranged in the form of the m \\times n matrix \\begin{bmatrix} \\partial_1 f_1(\\mathsf{x}) & \\ldots & \\partial_n f_1(\\mathsf{x})\\\\ \\vdots & \\ddots & \\vdots\\\\ \\partial_1 f_m(\\mathsf{x}) & \\ldots & \\partial_n f_m(\\mathsf{x}) \\end{bmatrix}, called the Jacobian of \\mathsf{f} at \\mathsf{x} . TO DO ... Riemann integration TO DO ...","title":"Appendix - Calculus"},{"location":"calculus/#single-variable-calculus","text":"We start with a brief review of single variable calculus. The primary object of study are functions of the form f:I \\subseteq \\mathbb{R} \\to \\mathbb{R} that map an open interval I = (a,b) \\subseteq \\mathbb{R} into the set of real numbers. Thus given any x \\in I , f(x) is a real number.","title":"Single variable calculus"},{"location":"calculus/#continuity","text":"Recall that f:I \\to \\mathbb{R} is said to be continuous at x \\in I if, given any \\epsilon > 0 , there exists \\delta > 0 such that |y - x| < \\delta \\quad\\Rightarrow\\quad |f(y) - f(x)| < \\epsilon. The idea is that as the point y approaches x , the value of the function f(y) approaches f(x) . In other words, there are no abrupt jumps in the function value at x . Note that the foregoing definition can be written equivalently as \\lim_{y \\to x} f(y) = f(x). The function is said to be continuous over the whole of I if it is continuous at every x \\in I . The set of all continuous functions on I is denoted as C^0(I,\\mathbb{R}) .","title":"Continuity"},{"location":"calculus/#differentiation","text":"The function f:I \\to \\mathbb{R} is said to be differentiable at x \\in I if the limit \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} exists. In this case, the is limit is called the derivative of f at x and is denoted variously as f'(x), \\quad \\dot{f}(x), \\quad \\frac{df(x)}{dx}, \\quad Df(x), depending on the context. The function f is said to be differentiable on I if it is differentiable at every x \\in I . The set of all differentiable functions on I whose derivative is also continuous on I is denoted as C^1(I,\\mathbb{R}) . Every differentiable function is continuous. To see this suppose that f:I \\to \\mathbb{R} is a differentiable function. Then, at any x \\in I , it can be seen that \\lim_{y \\to x} f(y) - f(x) = \\lim_{y \\to x} (y - x)\\frac{f(y) - f(x)}{y - x} = 0 \\cdot f'(x) = 0. This shows that \\lim_{y \\to x} f(y) = f(x) , thereby establishing the claim. Note in particular that C^1(I,\\mathbb{R}) \\subsetneq C^0(I,\\mathbb{R}) . Higher order derivatives can be defined similarly. For instance, a differentiable function f:I \\to \\mathbb{R} is said to be twice differentiable at x \\in I if the function f'(x):I \\to \\mathbb{R} is differentiable at x . In this case, the second derivative of f at x is written, variously, as f''(x), \\quad \\ddot{f}(x), \\quad \\frac{d^2f(x)}{dx^2}, \\quad D^2f(x). The function f:I \\to \\mathbb{R} is said to be twice differentiable on I if its second derivative exists at every x \\in I . The set of all twice differentiable functions on I whose second derivative is also continuous on I is denoted as C^2(I,\\mathbb{R}) . Third and higher derivatives of f are defined in an analogous manner. For instance, the set C^k(I,\\mathbb{R}) consists of all functions f:I \\to \\mathbb{R} that are k times differentiable on I , and whose k^{\\text{th}} derivative is continuous on I . The function f:I \\to \\mathbb{R} is said to be smooth on I if it is an element of C^k(I,\\mathbb{R}) for every k \\in \\mathbb{N} .","title":"Differentiation"},{"location":"calculus/#properties-of-the-derivative","text":"We will now recall a few elementary properties of the derivative. The derivative is a linear operation in the following sense: given any differentiable functions f:I \\to \\mathbb{R} , g:I \\to \\mathbb{R} and constants a,b \\in \\mathbb{R} , it follows from an elementary argument from the definition of the derivative that (a f + b g)'(x) = a f'(x) + b g'(x). The product rule of differentiation states that (fg)'(x) = f'(x) g(x) + f(x) g'(x). The product rule is also a simple consequence of the definition of the derivative. An important property of the derivative pertains to composition of functions. For simplicity, let us consider differentiable functions f, g: \\mathbb{R} \\to \\mathbb{R} . Then, the chain rule of differentiation states that (f \\circ g)'(x) = f'(g(x)) g'(x). Proof To see this, note that \\begin{split} (f \\circ g)'(x) &= \\lim_{h \\to 0} \\frac{f(g(x + h)) - f(g(x))}{h}\\\\ &= \\lim_{h \\to 0} \\frac{f(g(x + h)) - f(g(x))}{g(x + h) - g(x)}\\frac{g(x + h)) - g(x)}{h}. \\end{split} Noting that the differentiability of g implies that \\lim_{h \\to 0} g(x + h) - g(x) = 0, and introducing the notation y = g(x + h) - g(x) , we see that \\begin{split} (f \\circ g)'(x) &= \\lim_{y \\to 0} \\frac{f(g(x) + y) - f(g(x))}{y}\\lim_{h \\to 0}{g(x + h) - g(x)}\\frac{g(x + h)) - g(x)}{h}\\\\ &= f'(g(x))g'(x). \\end{split} This proves the chain rule of differentiation. Example As a simple application of the chain rule, consider the case of a bijective function f:\\mathbb{R} \\to \\mathbb{R} such that both f and f^{-1} are differentiable. The chain rule of differentiation can be used to related the derivatives of f and f^{-1} as follows: for any x \\in \\mathbb{R} , (f^{-1} \\circ f)(x) = x \\quad\\Rightarrow\\quad (f^{-1})'(f(x))f'(x) = 1 \\quad\\Rightarrow\\quad (f^{-1})'(f(x)) = \\frac{1}{f'(x)}. A similar expression can be obtained by differentiating the identity (f \\circ f^{-1})(x) = x .","title":"Properties of the derivative"},{"location":"calculus/#multivariable-calculus","text":"Let us now present certain elementary facts about multivariable calculus. To keep the discussion elementary, let us consider first the case of a real valued function of two real variables f:\\mathbb{R}^2 \\to \\mathbb{R} . The development here parallels that presented in the single variable case. The general case of a function of many variables is considered subsequently.","title":"Multivariable calculus"},{"location":"calculus/#continuity_1","text":"The function f:R \\subseteq \\mathbb{R}^2 \\to \\mathbb{R} , where R is the open rectangle (x_1,x_2) \\times (y_1,y_2) \\subseteq \\mathbb{R}^2 , is said to be continuous at (x,y) \\in R if, for every \\epsilon > 0 , there exists a \\delta > 0 such that \\sqrt{(\\bar{x} - x)^2 + (\\bar{y} - y)^2} < \\delta \\quad\\Rightarrow\\quad |f(\\bar{x},\\bar{y}) - f(x,y)| \\le \\epsilon. Notice how the Euclidean distance figures in this definition. As before, the definition of continuity can be equivalently restated as \\lim_{(\\bar{x},\\bar{y}) \\to (x,y)} f(\\bar{x},\\bar{y}) = f(x,y). The intuitive explanation is the same as before: if f is continuous at (x,y) \\in R , then the value of f close to (x,y) is close to f(x,y) . In other words, there are no gaps in the range of f . The function f is said to be continuous on R if it is continuous at every (x,y) \\in R . In this case, the set of all continuous functions on \\mathbb{R} is written as C^0(R,\\mathbb{R}) .","title":"Continuity"},{"location":"calculus/#differentiability","text":"Let us now study the differentiability of the function f:R \\to \\mathbb{R} . Rather than develop the general theory (this is presented in the main part of these notes), the presentation here provides an elementary account of the partial derivatives of f . The function f:R \\to \\mathbb{R} takes two real arguments and returns a real number. Fixing one of the arguments, we get a real-valued function of a real variable, which can be differentiated as studied before, in the context of single variable calculus. In the present case, fixing one of the arguments and letting the other vary results in two different limits: \\lim_{h \\to 0} \\frac{f(x + h,y) - f(x,y)}{h}, \\qquad \\lim_{h \\to 0} \\frac{f(x,y + h) - f(x,y)}{h}, where (x,y) \\in R and h \\in \\mathbb{R} . When these limits exist, they are called the partial derivatives of f . Specifically, the partial derivatives of f:R \\to \\mathbb{R} at (x,y) \\in \\mathbb{R} are defined as \\begin{split} \\frac{\\partial f(x,y)}{\\partial x} &= \\lim_{h \\to 0} \\frac{f(x + h,y) - f(x,y)}{h},\\\\ \\frac{\\partial f(x,y)}{\\partial y} &= \\lim_{h \\to 0} \\frac{f(x,y + h) - f(x,y)}{h}. \\end{split} The following alternative notations will be used for partial derivatives: \\begin{split} \\frac{\\partial f(x,y)}{\\partial x} &= \\partial_x f(x,y) = \\partial_1 f(x,y) = D_x f(x,y) = D_1 f(x,y),\\\\ \\frac{\\partial f(x,y)}{\\partial y} &= \\partial_y f(x,y) = \\partial_2 f(x,y) = D_y f(x,y) = D_2 f(x,y). \\end{split} The foregoing discussion can be generalized to functions of the form \\mathsf{f}:\\mathbb{R}^n \\to \\mathbb{R}^m . Note that the function \\mathsf{f} can be written as a collection of m functions \\mathsf{f} = (f_1, \\ldots, f_m), where each f_i:\\mathbb{R}^n \\to \\mathbb{R} , i = 1, \\ldots, m is a function that takes an m -tuple of real numbers and returns a real number. The partial derivaties of \\mathsf{f} at \\mathsf{x} = (x_1, \\ldots, x_m) \\in \\mathbb{R}^m can be arranged in the form of the m \\times n matrix \\begin{bmatrix} \\partial_1 f_1(\\mathsf{x}) & \\ldots & \\partial_n f_1(\\mathsf{x})\\\\ \\vdots & \\ddots & \\vdots\\\\ \\partial_1 f_m(\\mathsf{x}) & \\ldots & \\partial_n f_m(\\mathsf{x}) \\end{bmatrix}, called the Jacobian of \\mathsf{f} at \\mathsf{x} . TO DO ...","title":"Differentiability"},{"location":"calculus/#riemann-integration","text":"TO DO ...","title":"Riemann integration"},{"location":"curvilinear_coordinates/","text":"The notion of a general coordinate system was introduced at the beginning of this section. All the coordinate computations thus far have been restricted to the global Cartesian coordinate system on \\mathbb{R}^3 . There are, however, many applications where the use of special coordinate systems significantly simplify the calculations. It is thus of interest to study how the calculus of tensor fields on \\mathbb{R}^3 can be developed within a general coordinate setting. Before discussing the general case of tensor fields on U , it is instructive to consider the case of a smooth vector field \\mathsf{v}:U \\to TU . Given any \\mathsf{x} \\in U , the vector \\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U admits the following representation with respect to the Cartesian coordinate system on \\mathbb{R}^3 : \\mathsf{v}(\\mathsf{x}) = \\sum v_i(\\mathsf{x})\\mathsf{e}_i. The question that naturally arises is how to represent \\mathsf{v} using a general coordinate system on U . In particular, if each tangent space T_{\\mathsf{x}}U is equipped with a different basis, then the vector field \\mathsf{v} can be locally expressed with respect to the corresponding basis. What will become evident soon is that given a general coordinate system on U , it is possible to construct a natural set of basis vectors for each tangent space, called the coordinate basis , and which in turn can be used to build a systematic calculus using these curvilinear coordinates. Coordinate basis Recall that a coordinate system on \\mathbb{R}^3 is a pair (U,\\mathsf\\phi) where U is an open subset of \\mathbb{R}^3 and \\mathsf\\phi:U \\to \\mathbb{R}^3 is a diffeomorphism from U onto \\mathbb{R}^3 . The curvilinear coordinates of any \\mathsf{x} \\in U are then defined as \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) \\in \\mathbb{R}^3 . Recall that this is a simplified notation for \\mathsf{y} = \\mathsf\\phi(\\mathsf{x}) . The inverse of this relation is written as \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) . It is convenient to introduce the notation V = \\mathsf\\phi(U) \\subseteq \\mathbb{R}^3 to denote the image of U under \\mathsf\\phi . Remark Note that depending on the choice of coordinate system, the triads of real numbers \\mathsf{x} = (x_1, x_2, x_3) and \\mathsf{y} = (y_1, y_2, y_3) could represent the same point in U . Specifically, if \\mathsf{y} = \\mathsf\\phi(\\mathsf{x}) , or if \\mathsf{x} = \\mathsf\\phi^{-1}(\\mathsf{y}) , then they indeed represent the same point. It is conventional to work with the the coordinates \\mathsf{x} when working in a Cartesian coordinate setting, and the coordinates \\mathsf{y} , for the same point, when using curvilinear coordinates. A coordinate curve at \\mathsf{x} \\in \\mathbb{R}^3 is a map of the form \\mathsf{c}_i:I_{\\delta} \\to \\mathbb{R}^3 , where I_\\delta = [-\\delta, \\delta] \\subseteq \\mathbb{R} for some \\delta > 0 , such that, for any t \\in I_\\delta , \\begin{split} \\mathsf{c}_1(t) &= \\mathsf{x}(y_1 + t, y_2, y_3),\\\\ \\mathsf{c}_2(t) &= \\mathsf{x}(y_1, y_2 + t, y_3),\\\\ \\mathsf{c}_3(t) &= \\mathsf{x}(y_1, y_2, y_3 + t), \\end{split} where \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) . Note that \\mathsf{c}_i(0) = \\mathsf{x} for i=1,2,3 . The tangent vector \\mathsf{g}_i(\\mathsf{y}) to the coordinate curve \\mathsf{c}_i at t = 0 is defined as the i^{th} coordinate tangent vector : \\mathsf{g}_i(\\mathsf{y}) = \\dot{\\mathsf{c}}_i(0) = \\sum \\frac{\\partial x_j(\\mathsf{y})}{\\partial y_i} \\mathsf{e}_j. Note that each \\mathsf{g}_i(\\mathsf{y}) \\in T_{\\mathsf{x}(\\mathsf{y})}U . Further, the triad of tangent vectors (\\mathsf{g}_i(\\mathsf{y})) is linearly independent: to see this, note that if for some real numbers a_1, a_2, a_3 \\in \\mathbb{R} , \\sum a_i \\mathsf{g}_i(\\mathsf{y}) = \\mathsf{0} , then \\sum a_i \\frac{\\partial x_j(\\mathsf{y})}{\\partial y_i} \\mathsf{e}_j = \\mathsf{0} \\quad\\Rightarrow\\quad \\sum a_i \\frac{\\partial x_j(\\mathsf{y})}{\\partial y_i} = 0, \\; i = 1,2,3. The implication in the equation above follows from the fact that the triad of vectors (\\mathsf{e}_i) is linearly independent. Arranging these three equations in matrix form, it follows from the fact that \\text{det}\\begin{bmatrix} \\partial_1 x_1(\\mathsf{y}) & \\partial_2 x_1(\\mathsf{y}) & \\partial_3 x_1(\\mathsf{y})\\\\ \\partial_1 x_2(\\mathsf{y}) & \\partial_2 x_2(\\mathsf{y}) & \\partial_3 x_2(\\mathsf{y})\\\\ \\partial_1 x_3(\\mathsf{y}) & \\partial_2 x_3(\\mathsf{y}) & \\partial_3 x_3(\\mathsf{y})\\\\ \\end{bmatrix} \\neq 0 that a_1 = a_2 = a_3 = 0 . This shows that the triad (\\mathsf{g}_i(\\mathsf{y})) is a basis of T_{\\mathsf{x}(\\mathsf{y})}U , and is called the coordinate basis of T_{\\mathsf{x}(\\mathsf{y})}U with respect to the coordinate system (U,\\mathsf\\phi) . At any \\mathsf{y} \\in V , it is easy to compute the scalars g_{ij}(\\mathsf{y}) = \\mathsf{g}_i(\\mathsf{y})\\cdot\\mathsf{g}_j(\\mathsf{y}) : g_{ij}(\\mathsf{y}) = \\sum \\frac{\\partial x_k(\\mathsf{y})}{\\partial y_i}\\frac{\\partial x_k(\\mathsf{y})}{\\partial y_j}. Note that it is not the case, in general, that g_{ij} = \\delta_{ij} . This shows that the basis (\\mathsf{g}_i(\\mathsf{y})) of T_{\\mathsf{x}(\\mathsf{y})}U is not necessarily an orthonormal basis. Given a smooth vector field \\mathsf{v}:U \\to TU , the vector \\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U can now be expressed in terms of the corresponding coordinate basis (\\mathsf{g}_i(\\mathsf{y})) , where \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) \\in V , as follows: \\mathsf{v}(\\mathsf{x}) = \\sum \\hat{v}_i(\\mathsf{y}) \\mathsf{g}_i(\\mathsf{y}). Notice how both the component fields \\hat{v}_i and the basis vectors \\mathsf{g}_i depend on \\mathsf{y} . This is in contrast to the Cartesian representation of \\mathsf{v} where the basis vectors do not depend on the tangent space under consideration. The change of basis rules discussed earlier can be used here to study how the curvilinear components (\\hat{v}_i) of \\mathsf{v} are related to its Cartesian components (v_i) : if \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) , where \\mathsf{x} \\in U , then \\sum v_i(\\mathsf{x}) \\mathsf{e}_i = \\sum \\hat{v}_i(\\mathsf{y})\\mathsf{g}_i(\\mathsf{y}) \\quad\\Rightarrow\\quad v_i(\\mathsf{x}) = \\sum \\frac{\\partial x_i(\\mathsf{y})}{\\partial y_j} \\hat{v}_j(\\mathsf{y}). The inverse relation can be calculated by inverting this equation. This is more elegantly achieved by the use of the reciprocal basis to (\\mathsf{g}_i(\\mathsf{y}) , which is presented next. Reciprocal coordinate basis The coordinate basis (\\mathsf{g}_i(\\mathsf{y})) to the tangent space at \\mathsf{x}(\\mathsf{y}) \\in U is in general not orthornormal, as was noted in the previous section. It is therefore pertinent to compute its reciprocal basis to simplify various computations. A useful relation that helps in establishing many of the results in this section stems from the observation that the coordinate relations \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) and \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) are inverses of each other, whence \\mathsf{x}(\\mathsf{y}(\\mathsf{x})) = \\mathsf{x}, \\quad\\text{or}\\quad x_i(\\mathsf{y}(\\mathsf{x})) = x_i. Differentiating this expression with respect to x_j immediately yields the identity \\sum \\frac{\\partial x_i(\\mathsf{y})}{\\partial y_k}\\frac{\\partial y_k(\\mathsf{x})}{\\partial x_j} = \\delta_{ij}. Similarly, the identity \\mathsf{y}(\\mathsf{x}(\\mathsf{y})) = \\mathsf{y} yields the relation \\sum \\frac{\\partial y_i(\\mathsf{y})}{\\partial x_k}\\frac{\\partial x_k(\\mathsf{x})}{\\partial y_j} = \\delta_{ij} These two identies state the expected fact that the matrices whose (i,j)^{\\text{th}} entries are \\partial_j x_i(\\mathsf{y}) and \\partial_j y_i(\\mathsf{x}) are inverses of each other. These identities can be used to derive a variety of useful results. First, note that \\mathsf{g}_i(\\mathsf{y}) = \\sum \\frac{\\partial x_j(\\mathsf{y})}{\\partial y_i} \\mathsf{e}_j \\quad\\Rightarrow\\quad \\mathsf{e}_i = \\sum \\frac{\\partial y_j(\\mathsf{x})}{\\partial x_i}\\mathsf{g}_j(\\mathsf{y}). Second, note that \\begin{split} \\delta_{ij} &= \\sum \\frac{\\partial y_i(\\mathsf{y})}{\\partial x_k}\\frac{\\partial x_k(\\mathsf{x})}{\\partial y_j}\\\\ &= \\left(\\sum \\frac{\\partial y_i(\\mathsf{x})}{\\partial x_k}\\mathsf{e}_k\\right)\\cdot\\left(\\sum \\frac{\\partial x_l(\\mathsf{y})}{\\partial y_j}\\mathsf{e}_l\\right)\\\\ &= \\left(\\sum \\frac{\\partial y_i(\\mathsf{x})}{\\partial x_k}\\mathsf{e}_k\\right)\\cdot\\mathsf{g}_j(\\mathsf{y}). \\end{split} This calculation shows at once that the vector (\\mathsf{g}^i(\\mathsf{y})) defined as \\mathsf{g}^i(\\mathsf{y}) = \\sum \\frac{\\partial y_i(\\mathsf{x})}{\\partial x_k}\\mathsf{e}_k \\in T_{\\mathsf{x}}U, satisfy the relations \\mathsf{g}^i(\\mathsf{y}) \\cdot \\mathsf{g}_j(\\mathsf{y}) = \\delta_{ij}. It is thus evident that the vectors (\\mathsf{g}_i(\\mathsf{y})) thus defined constitute the reciprocal basis to the coordinate basis (\\mathsf{g}_i(\\mathsf{y})) of T_{\\mathsf{x}}U , and is called the reciprocal coordinate basis of T_{\\mathsf{x}}U . The reciprocal coordinate basis (\\mathsf{g}^i(\\mathsf{y})) of T_{\\mathsf{x}(\\mathsf{y})}U can be used to compute the scalars g^{ij}(\\mathsf{y}) = \\mathsf{g}^i(\\mathsf{y}) \\cdot \\mathsf{g}^j(\\mathsf{y}) : g^{ij}(\\mathsf{y}) = \\sum \\frac{\\partial y_i(\\mathsf{x})}{\\partial x_a}\\frac{\\partial y_j(\\mathsf{x})}{\\partial x_a}. It is straightforward to verify that \\sum g^{ij}(\\mathsf{y})g_{jk}(\\mathsf{y}) = \\delta_{ik} . As an example of the usefulness of the reciprocal basis, the change in coordinate representation of a vector field is revisited. Suppose that \\mathsf{v}:U \\to TU is a smooth vector field on U . The coordinate representation of \\mathsf{v} using the Cartesian coordinate system and the coordinate system (U,\\mathsf\\phi) are related as follows: \\sum v_i(\\mathsf{x}) \\mathsf{e}_i = \\sum \\hat{v}_i(\\mathsf{y}) \\mathsf{g}_i(\\mathsf{y}) , where \\mathsf{x} \\in U and \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) . Using the reciprocal coordinate basis just introduced, it follows after a simple computation that \\hat{v}_i(\\mathsf{y}) = \\sum \\frac{\\partial y_i(\\mathsf{x})}{\\partial x_j} v_j(\\mathsf{x}). This is the inverse of the relation obtained in the previous section that expressed the Cartesian components v_i in terms of the curvilinear components \\hat{v}_i . Christoffel symbols The discussion thus far can be summarized as follows: the choice of a coordinate system (U,\\mathsf\\phi) naturally suggests a choice of basis (\\mathsf{g}_i(\\mathsf{y})) , where \\mathsf{y} \\in V , for T_{\\mathsf{x}(\\mathsf{y})}U called the coordinate basis. A natural question to ask at this juncture is how the coordinate basis vectors \\mathsf{g}_i(\\mathsf{y}) as \\mathsf{y} varies over V . To answer this question, note that \\begin{split} \\mathsf{g}_i(\\mathsf{y}) = \\sum \\frac{\\partial x_k(\\mathsf{y})}{\\partial y_i} \\mathsf{e}_k \\quad\\Rightarrow\\quad \\frac{\\partial \\mathsf{g}_i(\\mathsf{y})}{\\partial y_j} &= \\frac{\\partial}{\\partial x_j}\\left(\\sum \\frac{\\partial x_k(\\mathsf{y})}{\\partial y_i} \\mathsf{e}_k\\right)\\\\ &= \\sum \\frac{\\partial^2 x_a(\\mathsf{y})}{\\partial y_i \\partial y_j} \\mathsf{e}_a\\\\ &= \\sum \\frac{\\partial^2 x_a(\\mathsf{y})}{\\partial y_i \\partial y_j} \\frac{\\partial y_k(\\mathsf{x})}{\\partial x_a} \\mathsf{g}_k(\\mathsf{y}), \\end{split} where \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) . Introducing the Christoffel symbols \\Gamma^k_{ij}:V \\to \\mathbb{R} as \\Gamma^k_{ij}(\\mathsf{y}) = \\sum \\frac{\\partial^2 x_a(\\mathsf{y})}{\\partial y_i \\partial y_j} \\frac{\\partial y_k(\\mathsf{x})}{\\partial x_a}, where \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) . Note that each \\Gamma^k_{ij} is a scalar field on V , but they do not form the components of a third order tensor field. Using the Christoffel symbols, the foregoing equation can be written compactly as \\frac{\\partial \\mathsf{g}_i(\\mathsf{y})}{\\partial y_j} = \\sum \\Gamma^k_{ij} \\mathsf{g}_k(\\mathsf{y}). The Christoffel symbols thus provide a means to study how the coordinate basis vectors change when moving from one tangent space to a neighboring one. It also follows from this result that \\Gamma^k_{ij}(\\mathsf{y}) = \\mathsf{g}^k(\\mathsf{y}) \\cdot \\frac{\\partial \\mathsf{g}_i(\\mathsf{y})}{\\partial y_j}. This equation clearly demonstrates that in the special case of the Cartesian coordinate system, the Christoffel symbols vanish identically since the basis vectors do not change when moving from one tangent space to another. Remark In the general theory of differentiable manifolds, the appropriate generalization of the ideas discussed leads to the notion of a connection on the manifold. Riemann curvature tensor ... Gradient and Divergence Let \\mathsf{v}:U \\to TU be a smooth vector field over an open subset U \\subseteq \\mathbb{R}^3 . Recall from the earlier discussion that all the key differential quantities related to \\mathsf{v} are obtained from the covariant derivative of \\mathsf{v} . Given a coordinate system (U,\\mathsf\\phi) , the curvilinear coordinate representation of the covariant derivative of \\mathsf{v} is obtained as follows: for any \\mathsf{x} \\in U and \\mathsf{w} \\in T_{\\mathsf{x}}U , with \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) , \\begin{split} \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{v}(\\mathsf{x} + t\\mathsf{w})\\\\ &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\sum \\hat{v}_i(\\mathsf{y} + t\\hat{\\mathsf{w}}) \\mathsf{g}_i(\\mathsf{y} + t\\hat{\\mathsf{w}})\\\\ &= \\sum \\left(\\partial_j \\hat{v}_i(\\mathsf{y}) \\hat{w}_j \\mathsf{g}_i(\\mathsf{y}) + \\hat{v}_i(\\mathsf{y}) \\partial_j \\mathsf{g}_i(\\mathsf{y})\\hat{w}_j\\right)\\\\ &= \\sum \\left(\\partial_j \\hat{v}_i(\\mathsf{y}) + \\Gamma^i_{kj}(\\mathsf{y})\\hat{v}_k(\\mathsf{y})\\right)\\hat{w}_j \\mathsf{g}_i(\\mathsf{y}). \\end{split} In the derivation above, \\hat{\\mathsf{w}} = (\\hat{w}_1, \\hat{w}_2, \\hat{w}_3) where \\{\\hat{w}_i\\} are constants such that \\mathsf{w} = \\sum \\hat{w}_i \\mathsf{g}_i(\\mathsf{y}) . The final expression for the covariant derivative \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) just derived can rewritten as follows: \\sum \\left(\\partial_j \\hat{v}_i(\\mathsf{y}) + \\Gamma^i_{kj}(\\mathsf{y})\\hat{v}_k(\\mathsf{y})\\right)\\hat{w}_j \\mathsf{g}_i(\\mathsf{y}) = \\left(\\sum \\left(\\partial_j \\hat{v}_i(\\mathsf{y}) + \\Gamma^i_{kj}(\\mathsf{y})\\hat{v}_k(\\mathsf{y})\\right)\\mathsf{g}_i(\\mathsf{y}) \\otimes \\mathsf{g}^j(\\mathsf{y})\\right) \\cdot \\left(\\sum \\hat{w}_l \\mathsf{g}_l(\\mathsf{y})\\right). It immediately follows from this equation that the gradient of the vector field \\mathsf{v} can be written in curvilinear coordinates as \\nabla \\mathsf{v}(\\mathsf{x}) = \\sum \\left(\\partial_j \\hat{v}_i(\\mathsf{y}) + \\Gamma^i_{kj}(\\mathsf{y})\\hat{v}_k(\\mathsf{y})\\right)\\mathsf{g}_i(\\mathsf{y}) \\otimes \\mathsf{g}^j(\\mathsf{y}). In the expression above, \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) . The divergence of \\mathsf{v} is obtained by contracting the gradient: \\nabla \\cdot \\mathsf{v}(\\mathsf{x}) = \\sum \\left(\\partial_i \\hat{v}_i(\\mathsf{y}) + \\Gamma^i_{ki}(\\mathsf{y})\\hat{v}_k(\\mathsf{y})\\right). Notice how the expressions for the gradient and divergence and divergence derived here reduce to their Cartesian forms when the Christoffel symbols are set to zero. The gradient and divergence of tensor fields over U are considered next. Rather than providing the corresponding expressions for a general tensor field, the special case of a second order tensor field \\mathsf{A}:U \\to \\otimes^2 TU is considered here. The starting point is, as before, the covariant derivative of \\mathsf{A} at \\mathsf{x} \\in U along \\mathsf{w} \\in T_{\\mathsf{x}}U . This is computed as follows: \\begin{split} \\nabla_{\\mathsf{w}}\\mathsf{A}(\\mathsf{x}) &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\hat{A}_{ij}(\\mathsf{y} + t\\hat{\\mathsf{w}}) \\, \\mathsf{g}_i(\\mathsf{y} + t\\hat{\\mathsf{w}}) \\otimes \\mathsf{g}_j(\\mathsf{y} + t\\hat{\\mathsf{w}})\\\\ &= \\sum \\left(\\partial_k \\hat{A}_{ij}(\\mathsf{y}) + \\Gamma^i_{ak}(\\mathsf{y})\\hat{A}_{aj}(\\mathsf{y}) + \\Gamma^j_{bk}(\\mathsf{y})\\hat{A}_{ib}(\\mathsf{y})\\right) \\hat{w}_k \\, \\mathsf{g}_i(\\mathsf{y}) \\otimes \\mathsf{g}_j(\\mathsf{y}). \\end{split} The gradient of \\mathsf{A} can be easily seen from the above calculation as \\nabla \\mathsf{A}(\\mathsf{x}) = \\sum \\left(\\partial_k \\hat{A}_{ij}(\\mathsf{y}) + \\Gamma^i_{ak}(\\mathsf{y})\\hat{A}_{aj}(\\mathsf{y}) + \\Gamma^j_{bk}(\\mathsf{y})\\hat{A}_{ib}(\\mathsf{y})\\right) \\mathsf{g}_i(\\mathsf{y}) \\otimes \\mathsf{g}_j(\\mathsf{y}) \\otimes \\mathsf{g}^k(\\mathsf{y}). The divergence of \\mathsf{A} is computed easily from this expression as \\nabla \\cdot \\mathsf{A}(\\mathsf{x}) = \\sum \\left(\\partial_j \\hat{A}_{ij}(\\mathsf{y}) + \\Gamma^i_{aj}(\\mathsf{y})\\hat{A}_{aj}(\\mathsf{y}) + \\Gamma^j_{bj}(\\mathsf{y})\\hat{A}_{ib}(\\mathsf{y})\\right) \\mathsf{g}_i(\\mathsf{y}) The curvilinear coordinate representation of the gradient and divergence of a tensor field of arbitrary order over U can be computed using a straightforward extension of the ideas presented above. Curl The curl of the vector field \\mathsf{v}:U \\to TU can be computed from its divergence using the definition \\mathsf{w} \\cdot \\nabla \\times \\mathsf{v}(\\mathsf{x}) = \\nabla \\cdot (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) , where \\mathsf{x} \\in U and \\mathsf{w}:U \\to TU is a constant vector field. Employing a curvilinear coordinate system (U,\\mathsf\\phi) , the vector \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} \\in T_{\\mathsf{x}}U can be written as \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} = \\left(\\sum \\hat{v}_i(\\mathsf{y}) \\mathsf{g}_i(\\mathsf{y})\\right) \\times \\left(\\sum \\hat{w}_j \\mathsf{g}_j(\\mathsf{y})\\right) = \\sum \\hat{v}_i(\\mathsf{y})\\hat{w}_j \\mathsf{g}_i(\\mathsf{y}) \\times \\mathsf{g}_j(\\mathsf{y}), where \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) . To evaluate \\mathsf{g}_i(\\mathsf{y}) \\times \\mathsf{g}_j(\\mathsf{y}) , it is helpful first to introduce a few useful relations. Introducing the notation J_{ij}(\\mathsf{y}) = \\frac{\\partial x_i(\\mathsf{y})}{\\partial y_j}, for the elements of the Jacobian matrix for the change of coordinate system from (U,\\mathsf\\phi) to the Cartesian coordinate system, it follows from a simple calculation that g_{ij}(\\mathsf{y}) = \\sum \\frac{\\partial x_k(\\mathsf{y})}{\\partial y_i}\\frac{\\partial x_k(\\mathsf{y})}{\\partial y_j} = \\sum J_{ki}(\\mathsf{y})J_{kj}(\\mathsf{y}). Denoting by J(\\mathsf{y}) the determinant of the matrix whose (i,j)^{\\text{th}} entry is J_{ij}(\\mathsf{y}) , and by \\text{det }\\mathsf{g}(\\mathsf{x}) the determinant of the matrix whose (i,j)^{\\text{th}} entry is g_{ij}(\\mathsf{y}) , it follows from the previous equation that \\text{det }\\mathsf{g}(\\mathsf{x}) = J(\\mathsf{y})^2 \\quad\\Rightarrow\\quad J(\\mathsf{y}) = \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})}. Note that in the equation above, \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) . To understand the rationale behind this notation, it is useful to introduce a second order tensor field \\mathsf{g}:U \\to \\otimes^2 U on U called the metric on U , defined as follows: for any \\mathsf{x} \\in U , \\mathsf{g}(\\mathsf{x}) = \\sum \\delta_{ij} \\mathsf{e}_i \\otimes \\mathsf{e}_j = \\sum g_{ij}(\\mathsf{y}) \\, \\mathsf{g}^i(\\mathsf{y}) \\otimes \\mathsf{g}^j(\\mathsf{y}). The idea behind the metric is to provide a smooth extension of the inner product over every tangent space. Indeed, for any \\mathsf{x} \\in U , given any \\mathsf{u},\\mathsf{w} \\in T_{\\mathsf{x}}U , it follows that \\sum u_i w_i = \\mathsf{u} \\cdot \\mathsf{w} = \\mathsf{g}(\\mathsf{x})(\\mathsf{u},\\mathsf{w}) = \\sum g_{ij}(\\mathsf{y}) \\hat{u}_i \\hat{w}_j. Using the ideas just developed, the definition of the Levi-Civita tensor on \\mathbb{R}^3 , and the definition of the determinant, it follows that \\mathsf{g}_i(\\mathsf{y}) \\cdot \\mathsf{g}_j(\\mathsf{y}) \\times \\mathsf{g}_k(\\mathsf{y}) = \\sum \\epsilon_{abc} \\frac{\\partial x_a(\\mathsf{y})}{\\partial y_i}\\frac{\\partial x_b(\\mathsf{y})}{\\partial y_j}\\frac{\\partial x_c(\\mathsf{y})}{\\partial y_k} = \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\, \\epsilon_{ijk}. This shows at once that \\mathsf{g}_j(\\mathsf{y}) \\times \\mathsf{g}_k(\\mathsf{y}) = \\sum \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\, \\epsilon_{ijk} \\, \\mathsf{g}^i(\\mathsf{y}). Returning to the computation of the vector \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} in curvilinear coordinates initiated in the beginning of this section, it follows that \\begin{split} \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} &= \\sum \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{y})} \\, \\epsilon_{ijk} \\hat{v}_i(\\mathsf{y})\\hat{w}_j(\\mathsf{y}) \\, \\mathsf{g}^k(\\mathsf{y})\\\\ &= \\sum \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{y})} \\, \\epsilon_{ijk} \\, g^{ka}(\\mathsf{y}) \\, \\hat{v}_i(\\mathsf{y})\\hat{w}_j(\\mathsf{y}) \\, \\mathsf{g}_a(\\mathsf{y}). \\end{split} The divergence of \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} can now be computed in curvilinear coordinates as \\begin{split} \\nabla \\cdot (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) &= \\sum \\epsilon_{ijk} \\left(\\partial_a \\left(g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\, \\hat{v}_i(\\mathsf{y})\\right) + g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\, \\Gamma^b_{ab}(\\mathsf{y}) \\hat{v}_i(\\mathsf{y})\\right)\\hat{w}_j\\\\ &= \\left(\\sum \\hat{w}_c\\mathsf{g}_c(\\mathsf{y})\\right) \\cdot \\left(\\sum \\epsilon_{ijk} \\left(\\partial_a \\left(g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})}\\right) \\hat{v}_i(\\mathsf{y}) \\right.\\right.\\\\ & \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left. + g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\left(\\partial_a \\hat{v}_i(\\mathsf{y}) + \\Gamma^b_{ab}(\\mathsf{y}) \\hat{v}_i(\\mathsf{y})\\right)\\right) \\mathsf{g}^j(\\mathsf{y})\\right). \\end{split} The curvilinear coordinate expression for the curl of the vector field \\mathsf{v} follows at once from this calculation that \\nabla \\times \\mathsf{v}(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\left(\\partial_a \\left(g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})}\\right) \\hat{v}_i(\\mathsf{y}) + g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\left(\\partial_a \\hat{v}_i(\\mathsf{y}) + \\Gamma^b_{ab}(\\mathsf{y}) \\hat{v}_i(\\mathsf{y})\\right)\\right) \\mathsf{g}^j(\\mathsf{y}). The curl of tensor fields of higher order is computed along the same lines. The final expressions are not provided here since they have cumbersome algebraic forms in curvilinear coordinates. Integration Suppose that f:U \\to \\mathbb{R} is a scalar field on an open subset U \\subseteq \\mathbb{R}^3 . Given a coordinate system (U,\\mathsf\\phi) on \\mathbb{R}^3 , the integral of f over U is defined as follows: \\begin{split} \\int_U f(\\mathsf{x})\\,dv &= \\int_U f(x_1, x_2, x_3)\\,dx_1 dx_2 dx_3\\\\ &= \\int_{\\mathsf\\phi(U)} (f \\circ \\mathsf\\phi^{-1})(y_1, y_2, y_3) \\, \\text{det}\\left(\\frac{\\partial \\mathsf{x}}{\\partial \\mathsf{y}}\\right) dy_1 dy_2 dy_3\\\\ &= \\int_{\\mathsf\\phi(U)} (f \\circ \\mathsf\\phi^{-1})(y_1, y_2, y_3) \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\, dy_1 dy_2 dy_3. \\end{split} In the equation above \\partial \\mathsf{x}/\\partial \\mathsf{y} denotes the matrix whose (i,j)^{\\text{th}} component is \\partial x_i(\\mathsf{y})/\\partial y_j . This result follows from the change of variables formula for multiple integrals. Special coordinate systems ...","title":"Curvilinear Coordinates"},{"location":"curvilinear_coordinates/#coordinate-basis","text":"Recall that a coordinate system on \\mathbb{R}^3 is a pair (U,\\mathsf\\phi) where U is an open subset of \\mathbb{R}^3 and \\mathsf\\phi:U \\to \\mathbb{R}^3 is a diffeomorphism from U onto \\mathbb{R}^3 . The curvilinear coordinates of any \\mathsf{x} \\in U are then defined as \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) \\in \\mathbb{R}^3 . Recall that this is a simplified notation for \\mathsf{y} = \\mathsf\\phi(\\mathsf{x}) . The inverse of this relation is written as \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) . It is convenient to introduce the notation V = \\mathsf\\phi(U) \\subseteq \\mathbb{R}^3 to denote the image of U under \\mathsf\\phi . Remark Note that depending on the choice of coordinate system, the triads of real numbers \\mathsf{x} = (x_1, x_2, x_3) and \\mathsf{y} = (y_1, y_2, y_3) could represent the same point in U . Specifically, if \\mathsf{y} = \\mathsf\\phi(\\mathsf{x}) , or if \\mathsf{x} = \\mathsf\\phi^{-1}(\\mathsf{y}) , then they indeed represent the same point. It is conventional to work with the the coordinates \\mathsf{x} when working in a Cartesian coordinate setting, and the coordinates \\mathsf{y} , for the same point, when using curvilinear coordinates. A coordinate curve at \\mathsf{x} \\in \\mathbb{R}^3 is a map of the form \\mathsf{c}_i:I_{\\delta} \\to \\mathbb{R}^3 , where I_\\delta = [-\\delta, \\delta] \\subseteq \\mathbb{R} for some \\delta > 0 , such that, for any t \\in I_\\delta , \\begin{split} \\mathsf{c}_1(t) &= \\mathsf{x}(y_1 + t, y_2, y_3),\\\\ \\mathsf{c}_2(t) &= \\mathsf{x}(y_1, y_2 + t, y_3),\\\\ \\mathsf{c}_3(t) &= \\mathsf{x}(y_1, y_2, y_3 + t), \\end{split} where \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) . Note that \\mathsf{c}_i(0) = \\mathsf{x} for i=1,2,3 . The tangent vector \\mathsf{g}_i(\\mathsf{y}) to the coordinate curve \\mathsf{c}_i at t = 0 is defined as the i^{th} coordinate tangent vector : \\mathsf{g}_i(\\mathsf{y}) = \\dot{\\mathsf{c}}_i(0) = \\sum \\frac{\\partial x_j(\\mathsf{y})}{\\partial y_i} \\mathsf{e}_j. Note that each \\mathsf{g}_i(\\mathsf{y}) \\in T_{\\mathsf{x}(\\mathsf{y})}U . Further, the triad of tangent vectors (\\mathsf{g}_i(\\mathsf{y})) is linearly independent: to see this, note that if for some real numbers a_1, a_2, a_3 \\in \\mathbb{R} , \\sum a_i \\mathsf{g}_i(\\mathsf{y}) = \\mathsf{0} , then \\sum a_i \\frac{\\partial x_j(\\mathsf{y})}{\\partial y_i} \\mathsf{e}_j = \\mathsf{0} \\quad\\Rightarrow\\quad \\sum a_i \\frac{\\partial x_j(\\mathsf{y})}{\\partial y_i} = 0, \\; i = 1,2,3. The implication in the equation above follows from the fact that the triad of vectors (\\mathsf{e}_i) is linearly independent. Arranging these three equations in matrix form, it follows from the fact that \\text{det}\\begin{bmatrix} \\partial_1 x_1(\\mathsf{y}) & \\partial_2 x_1(\\mathsf{y}) & \\partial_3 x_1(\\mathsf{y})\\\\ \\partial_1 x_2(\\mathsf{y}) & \\partial_2 x_2(\\mathsf{y}) & \\partial_3 x_2(\\mathsf{y})\\\\ \\partial_1 x_3(\\mathsf{y}) & \\partial_2 x_3(\\mathsf{y}) & \\partial_3 x_3(\\mathsf{y})\\\\ \\end{bmatrix} \\neq 0 that a_1 = a_2 = a_3 = 0 . This shows that the triad (\\mathsf{g}_i(\\mathsf{y})) is a basis of T_{\\mathsf{x}(\\mathsf{y})}U , and is called the coordinate basis of T_{\\mathsf{x}(\\mathsf{y})}U with respect to the coordinate system (U,\\mathsf\\phi) . At any \\mathsf{y} \\in V , it is easy to compute the scalars g_{ij}(\\mathsf{y}) = \\mathsf{g}_i(\\mathsf{y})\\cdot\\mathsf{g}_j(\\mathsf{y}) : g_{ij}(\\mathsf{y}) = \\sum \\frac{\\partial x_k(\\mathsf{y})}{\\partial y_i}\\frac{\\partial x_k(\\mathsf{y})}{\\partial y_j}. Note that it is not the case, in general, that g_{ij} = \\delta_{ij} . This shows that the basis (\\mathsf{g}_i(\\mathsf{y})) of T_{\\mathsf{x}(\\mathsf{y})}U is not necessarily an orthonormal basis. Given a smooth vector field \\mathsf{v}:U \\to TU , the vector \\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U can now be expressed in terms of the corresponding coordinate basis (\\mathsf{g}_i(\\mathsf{y})) , where \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) \\in V , as follows: \\mathsf{v}(\\mathsf{x}) = \\sum \\hat{v}_i(\\mathsf{y}) \\mathsf{g}_i(\\mathsf{y}). Notice how both the component fields \\hat{v}_i and the basis vectors \\mathsf{g}_i depend on \\mathsf{y} . This is in contrast to the Cartesian representation of \\mathsf{v} where the basis vectors do not depend on the tangent space under consideration. The change of basis rules discussed earlier can be used here to study how the curvilinear components (\\hat{v}_i) of \\mathsf{v} are related to its Cartesian components (v_i) : if \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) , where \\mathsf{x} \\in U , then \\sum v_i(\\mathsf{x}) \\mathsf{e}_i = \\sum \\hat{v}_i(\\mathsf{y})\\mathsf{g}_i(\\mathsf{y}) \\quad\\Rightarrow\\quad v_i(\\mathsf{x}) = \\sum \\frac{\\partial x_i(\\mathsf{y})}{\\partial y_j} \\hat{v}_j(\\mathsf{y}). The inverse relation can be calculated by inverting this equation. This is more elegantly achieved by the use of the reciprocal basis to (\\mathsf{g}_i(\\mathsf{y}) , which is presented next.","title":"Coordinate basis"},{"location":"curvilinear_coordinates/#reciprocal-coordinate-basis","text":"The coordinate basis (\\mathsf{g}_i(\\mathsf{y})) to the tangent space at \\mathsf{x}(\\mathsf{y}) \\in U is in general not orthornormal, as was noted in the previous section. It is therefore pertinent to compute its reciprocal basis to simplify various computations. A useful relation that helps in establishing many of the results in this section stems from the observation that the coordinate relations \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) and \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) are inverses of each other, whence \\mathsf{x}(\\mathsf{y}(\\mathsf{x})) = \\mathsf{x}, \\quad\\text{or}\\quad x_i(\\mathsf{y}(\\mathsf{x})) = x_i. Differentiating this expression with respect to x_j immediately yields the identity \\sum \\frac{\\partial x_i(\\mathsf{y})}{\\partial y_k}\\frac{\\partial y_k(\\mathsf{x})}{\\partial x_j} = \\delta_{ij}. Similarly, the identity \\mathsf{y}(\\mathsf{x}(\\mathsf{y})) = \\mathsf{y} yields the relation \\sum \\frac{\\partial y_i(\\mathsf{y})}{\\partial x_k}\\frac{\\partial x_k(\\mathsf{x})}{\\partial y_j} = \\delta_{ij} These two identies state the expected fact that the matrices whose (i,j)^{\\text{th}} entries are \\partial_j x_i(\\mathsf{y}) and \\partial_j y_i(\\mathsf{x}) are inverses of each other. These identities can be used to derive a variety of useful results. First, note that \\mathsf{g}_i(\\mathsf{y}) = \\sum \\frac{\\partial x_j(\\mathsf{y})}{\\partial y_i} \\mathsf{e}_j \\quad\\Rightarrow\\quad \\mathsf{e}_i = \\sum \\frac{\\partial y_j(\\mathsf{x})}{\\partial x_i}\\mathsf{g}_j(\\mathsf{y}). Second, note that \\begin{split} \\delta_{ij} &= \\sum \\frac{\\partial y_i(\\mathsf{y})}{\\partial x_k}\\frac{\\partial x_k(\\mathsf{x})}{\\partial y_j}\\\\ &= \\left(\\sum \\frac{\\partial y_i(\\mathsf{x})}{\\partial x_k}\\mathsf{e}_k\\right)\\cdot\\left(\\sum \\frac{\\partial x_l(\\mathsf{y})}{\\partial y_j}\\mathsf{e}_l\\right)\\\\ &= \\left(\\sum \\frac{\\partial y_i(\\mathsf{x})}{\\partial x_k}\\mathsf{e}_k\\right)\\cdot\\mathsf{g}_j(\\mathsf{y}). \\end{split} This calculation shows at once that the vector (\\mathsf{g}^i(\\mathsf{y})) defined as \\mathsf{g}^i(\\mathsf{y}) = \\sum \\frac{\\partial y_i(\\mathsf{x})}{\\partial x_k}\\mathsf{e}_k \\in T_{\\mathsf{x}}U, satisfy the relations \\mathsf{g}^i(\\mathsf{y}) \\cdot \\mathsf{g}_j(\\mathsf{y}) = \\delta_{ij}. It is thus evident that the vectors (\\mathsf{g}_i(\\mathsf{y})) thus defined constitute the reciprocal basis to the coordinate basis (\\mathsf{g}_i(\\mathsf{y})) of T_{\\mathsf{x}}U , and is called the reciprocal coordinate basis of T_{\\mathsf{x}}U . The reciprocal coordinate basis (\\mathsf{g}^i(\\mathsf{y})) of T_{\\mathsf{x}(\\mathsf{y})}U can be used to compute the scalars g^{ij}(\\mathsf{y}) = \\mathsf{g}^i(\\mathsf{y}) \\cdot \\mathsf{g}^j(\\mathsf{y}) : g^{ij}(\\mathsf{y}) = \\sum \\frac{\\partial y_i(\\mathsf{x})}{\\partial x_a}\\frac{\\partial y_j(\\mathsf{x})}{\\partial x_a}. It is straightforward to verify that \\sum g^{ij}(\\mathsf{y})g_{jk}(\\mathsf{y}) = \\delta_{ik} . As an example of the usefulness of the reciprocal basis, the change in coordinate representation of a vector field is revisited. Suppose that \\mathsf{v}:U \\to TU is a smooth vector field on U . The coordinate representation of \\mathsf{v} using the Cartesian coordinate system and the coordinate system (U,\\mathsf\\phi) are related as follows: \\sum v_i(\\mathsf{x}) \\mathsf{e}_i = \\sum \\hat{v}_i(\\mathsf{y}) \\mathsf{g}_i(\\mathsf{y}) , where \\mathsf{x} \\in U and \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) . Using the reciprocal coordinate basis just introduced, it follows after a simple computation that \\hat{v}_i(\\mathsf{y}) = \\sum \\frac{\\partial y_i(\\mathsf{x})}{\\partial x_j} v_j(\\mathsf{x}). This is the inverse of the relation obtained in the previous section that expressed the Cartesian components v_i in terms of the curvilinear components \\hat{v}_i .","title":"Reciprocal coordinate basis"},{"location":"curvilinear_coordinates/#christoffel-symbols","text":"The discussion thus far can be summarized as follows: the choice of a coordinate system (U,\\mathsf\\phi) naturally suggests a choice of basis (\\mathsf{g}_i(\\mathsf{y})) , where \\mathsf{y} \\in V , for T_{\\mathsf{x}(\\mathsf{y})}U called the coordinate basis. A natural question to ask at this juncture is how the coordinate basis vectors \\mathsf{g}_i(\\mathsf{y}) as \\mathsf{y} varies over V . To answer this question, note that \\begin{split} \\mathsf{g}_i(\\mathsf{y}) = \\sum \\frac{\\partial x_k(\\mathsf{y})}{\\partial y_i} \\mathsf{e}_k \\quad\\Rightarrow\\quad \\frac{\\partial \\mathsf{g}_i(\\mathsf{y})}{\\partial y_j} &= \\frac{\\partial}{\\partial x_j}\\left(\\sum \\frac{\\partial x_k(\\mathsf{y})}{\\partial y_i} \\mathsf{e}_k\\right)\\\\ &= \\sum \\frac{\\partial^2 x_a(\\mathsf{y})}{\\partial y_i \\partial y_j} \\mathsf{e}_a\\\\ &= \\sum \\frac{\\partial^2 x_a(\\mathsf{y})}{\\partial y_i \\partial y_j} \\frac{\\partial y_k(\\mathsf{x})}{\\partial x_a} \\mathsf{g}_k(\\mathsf{y}), \\end{split} where \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) . Introducing the Christoffel symbols \\Gamma^k_{ij}:V \\to \\mathbb{R} as \\Gamma^k_{ij}(\\mathsf{y}) = \\sum \\frac{\\partial^2 x_a(\\mathsf{y})}{\\partial y_i \\partial y_j} \\frac{\\partial y_k(\\mathsf{x})}{\\partial x_a}, where \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) . Note that each \\Gamma^k_{ij} is a scalar field on V , but they do not form the components of a third order tensor field. Using the Christoffel symbols, the foregoing equation can be written compactly as \\frac{\\partial \\mathsf{g}_i(\\mathsf{y})}{\\partial y_j} = \\sum \\Gamma^k_{ij} \\mathsf{g}_k(\\mathsf{y}). The Christoffel symbols thus provide a means to study how the coordinate basis vectors change when moving from one tangent space to a neighboring one. It also follows from this result that \\Gamma^k_{ij}(\\mathsf{y}) = \\mathsf{g}^k(\\mathsf{y}) \\cdot \\frac{\\partial \\mathsf{g}_i(\\mathsf{y})}{\\partial y_j}. This equation clearly demonstrates that in the special case of the Cartesian coordinate system, the Christoffel symbols vanish identically since the basis vectors do not change when moving from one tangent space to another. Remark In the general theory of differentiable manifolds, the appropriate generalization of the ideas discussed leads to the notion of a connection on the manifold.","title":"Christoffel symbols"},{"location":"curvilinear_coordinates/#riemann-curvature-tensor","text":"...","title":"Riemann curvature tensor"},{"location":"curvilinear_coordinates/#gradient-and-divergence","text":"Let \\mathsf{v}:U \\to TU be a smooth vector field over an open subset U \\subseteq \\mathbb{R}^3 . Recall from the earlier discussion that all the key differential quantities related to \\mathsf{v} are obtained from the covariant derivative of \\mathsf{v} . Given a coordinate system (U,\\mathsf\\phi) , the curvilinear coordinate representation of the covariant derivative of \\mathsf{v} is obtained as follows: for any \\mathsf{x} \\in U and \\mathsf{w} \\in T_{\\mathsf{x}}U , with \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) , \\begin{split} \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{v}(\\mathsf{x} + t\\mathsf{w})\\\\ &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\sum \\hat{v}_i(\\mathsf{y} + t\\hat{\\mathsf{w}}) \\mathsf{g}_i(\\mathsf{y} + t\\hat{\\mathsf{w}})\\\\ &= \\sum \\left(\\partial_j \\hat{v}_i(\\mathsf{y}) \\hat{w}_j \\mathsf{g}_i(\\mathsf{y}) + \\hat{v}_i(\\mathsf{y}) \\partial_j \\mathsf{g}_i(\\mathsf{y})\\hat{w}_j\\right)\\\\ &= \\sum \\left(\\partial_j \\hat{v}_i(\\mathsf{y}) + \\Gamma^i_{kj}(\\mathsf{y})\\hat{v}_k(\\mathsf{y})\\right)\\hat{w}_j \\mathsf{g}_i(\\mathsf{y}). \\end{split} In the derivation above, \\hat{\\mathsf{w}} = (\\hat{w}_1, \\hat{w}_2, \\hat{w}_3) where \\{\\hat{w}_i\\} are constants such that \\mathsf{w} = \\sum \\hat{w}_i \\mathsf{g}_i(\\mathsf{y}) . The final expression for the covariant derivative \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) just derived can rewritten as follows: \\sum \\left(\\partial_j \\hat{v}_i(\\mathsf{y}) + \\Gamma^i_{kj}(\\mathsf{y})\\hat{v}_k(\\mathsf{y})\\right)\\hat{w}_j \\mathsf{g}_i(\\mathsf{y}) = \\left(\\sum \\left(\\partial_j \\hat{v}_i(\\mathsf{y}) + \\Gamma^i_{kj}(\\mathsf{y})\\hat{v}_k(\\mathsf{y})\\right)\\mathsf{g}_i(\\mathsf{y}) \\otimes \\mathsf{g}^j(\\mathsf{y})\\right) \\cdot \\left(\\sum \\hat{w}_l \\mathsf{g}_l(\\mathsf{y})\\right). It immediately follows from this equation that the gradient of the vector field \\mathsf{v} can be written in curvilinear coordinates as \\nabla \\mathsf{v}(\\mathsf{x}) = \\sum \\left(\\partial_j \\hat{v}_i(\\mathsf{y}) + \\Gamma^i_{kj}(\\mathsf{y})\\hat{v}_k(\\mathsf{y})\\right)\\mathsf{g}_i(\\mathsf{y}) \\otimes \\mathsf{g}^j(\\mathsf{y}). In the expression above, \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) . The divergence of \\mathsf{v} is obtained by contracting the gradient: \\nabla \\cdot \\mathsf{v}(\\mathsf{x}) = \\sum \\left(\\partial_i \\hat{v}_i(\\mathsf{y}) + \\Gamma^i_{ki}(\\mathsf{y})\\hat{v}_k(\\mathsf{y})\\right). Notice how the expressions for the gradient and divergence and divergence derived here reduce to their Cartesian forms when the Christoffel symbols are set to zero. The gradient and divergence of tensor fields over U are considered next. Rather than providing the corresponding expressions for a general tensor field, the special case of a second order tensor field \\mathsf{A}:U \\to \\otimes^2 TU is considered here. The starting point is, as before, the covariant derivative of \\mathsf{A} at \\mathsf{x} \\in U along \\mathsf{w} \\in T_{\\mathsf{x}}U . This is computed as follows: \\begin{split} \\nabla_{\\mathsf{w}}\\mathsf{A}(\\mathsf{x}) &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\hat{A}_{ij}(\\mathsf{y} + t\\hat{\\mathsf{w}}) \\, \\mathsf{g}_i(\\mathsf{y} + t\\hat{\\mathsf{w}}) \\otimes \\mathsf{g}_j(\\mathsf{y} + t\\hat{\\mathsf{w}})\\\\ &= \\sum \\left(\\partial_k \\hat{A}_{ij}(\\mathsf{y}) + \\Gamma^i_{ak}(\\mathsf{y})\\hat{A}_{aj}(\\mathsf{y}) + \\Gamma^j_{bk}(\\mathsf{y})\\hat{A}_{ib}(\\mathsf{y})\\right) \\hat{w}_k \\, \\mathsf{g}_i(\\mathsf{y}) \\otimes \\mathsf{g}_j(\\mathsf{y}). \\end{split} The gradient of \\mathsf{A} can be easily seen from the above calculation as \\nabla \\mathsf{A}(\\mathsf{x}) = \\sum \\left(\\partial_k \\hat{A}_{ij}(\\mathsf{y}) + \\Gamma^i_{ak}(\\mathsf{y})\\hat{A}_{aj}(\\mathsf{y}) + \\Gamma^j_{bk}(\\mathsf{y})\\hat{A}_{ib}(\\mathsf{y})\\right) \\mathsf{g}_i(\\mathsf{y}) \\otimes \\mathsf{g}_j(\\mathsf{y}) \\otimes \\mathsf{g}^k(\\mathsf{y}). The divergence of \\mathsf{A} is computed easily from this expression as \\nabla \\cdot \\mathsf{A}(\\mathsf{x}) = \\sum \\left(\\partial_j \\hat{A}_{ij}(\\mathsf{y}) + \\Gamma^i_{aj}(\\mathsf{y})\\hat{A}_{aj}(\\mathsf{y}) + \\Gamma^j_{bj}(\\mathsf{y})\\hat{A}_{ib}(\\mathsf{y})\\right) \\mathsf{g}_i(\\mathsf{y}) The curvilinear coordinate representation of the gradient and divergence of a tensor field of arbitrary order over U can be computed using a straightforward extension of the ideas presented above.","title":"Gradient and Divergence"},{"location":"curvilinear_coordinates/#curl","text":"The curl of the vector field \\mathsf{v}:U \\to TU can be computed from its divergence using the definition \\mathsf{w} \\cdot \\nabla \\times \\mathsf{v}(\\mathsf{x}) = \\nabla \\cdot (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) , where \\mathsf{x} \\in U and \\mathsf{w}:U \\to TU is a constant vector field. Employing a curvilinear coordinate system (U,\\mathsf\\phi) , the vector \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} \\in T_{\\mathsf{x}}U can be written as \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} = \\left(\\sum \\hat{v}_i(\\mathsf{y}) \\mathsf{g}_i(\\mathsf{y})\\right) \\times \\left(\\sum \\hat{w}_j \\mathsf{g}_j(\\mathsf{y})\\right) = \\sum \\hat{v}_i(\\mathsf{y})\\hat{w}_j \\mathsf{g}_i(\\mathsf{y}) \\times \\mathsf{g}_j(\\mathsf{y}), where \\mathsf{y} = \\mathsf{y}(\\mathsf{x}) . To evaluate \\mathsf{g}_i(\\mathsf{y}) \\times \\mathsf{g}_j(\\mathsf{y}) , it is helpful first to introduce a few useful relations. Introducing the notation J_{ij}(\\mathsf{y}) = \\frac{\\partial x_i(\\mathsf{y})}{\\partial y_j}, for the elements of the Jacobian matrix for the change of coordinate system from (U,\\mathsf\\phi) to the Cartesian coordinate system, it follows from a simple calculation that g_{ij}(\\mathsf{y}) = \\sum \\frac{\\partial x_k(\\mathsf{y})}{\\partial y_i}\\frac{\\partial x_k(\\mathsf{y})}{\\partial y_j} = \\sum J_{ki}(\\mathsf{y})J_{kj}(\\mathsf{y}). Denoting by J(\\mathsf{y}) the determinant of the matrix whose (i,j)^{\\text{th}} entry is J_{ij}(\\mathsf{y}) , and by \\text{det }\\mathsf{g}(\\mathsf{x}) the determinant of the matrix whose (i,j)^{\\text{th}} entry is g_{ij}(\\mathsf{y}) , it follows from the previous equation that \\text{det }\\mathsf{g}(\\mathsf{x}) = J(\\mathsf{y})^2 \\quad\\Rightarrow\\quad J(\\mathsf{y}) = \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})}. Note that in the equation above, \\mathsf{x} = \\mathsf{x}(\\mathsf{y}) . To understand the rationale behind this notation, it is useful to introduce a second order tensor field \\mathsf{g}:U \\to \\otimes^2 U on U called the metric on U , defined as follows: for any \\mathsf{x} \\in U , \\mathsf{g}(\\mathsf{x}) = \\sum \\delta_{ij} \\mathsf{e}_i \\otimes \\mathsf{e}_j = \\sum g_{ij}(\\mathsf{y}) \\, \\mathsf{g}^i(\\mathsf{y}) \\otimes \\mathsf{g}^j(\\mathsf{y}). The idea behind the metric is to provide a smooth extension of the inner product over every tangent space. Indeed, for any \\mathsf{x} \\in U , given any \\mathsf{u},\\mathsf{w} \\in T_{\\mathsf{x}}U , it follows that \\sum u_i w_i = \\mathsf{u} \\cdot \\mathsf{w} = \\mathsf{g}(\\mathsf{x})(\\mathsf{u},\\mathsf{w}) = \\sum g_{ij}(\\mathsf{y}) \\hat{u}_i \\hat{w}_j. Using the ideas just developed, the definition of the Levi-Civita tensor on \\mathbb{R}^3 , and the definition of the determinant, it follows that \\mathsf{g}_i(\\mathsf{y}) \\cdot \\mathsf{g}_j(\\mathsf{y}) \\times \\mathsf{g}_k(\\mathsf{y}) = \\sum \\epsilon_{abc} \\frac{\\partial x_a(\\mathsf{y})}{\\partial y_i}\\frac{\\partial x_b(\\mathsf{y})}{\\partial y_j}\\frac{\\partial x_c(\\mathsf{y})}{\\partial y_k} = \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\, \\epsilon_{ijk}. This shows at once that \\mathsf{g}_j(\\mathsf{y}) \\times \\mathsf{g}_k(\\mathsf{y}) = \\sum \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\, \\epsilon_{ijk} \\, \\mathsf{g}^i(\\mathsf{y}). Returning to the computation of the vector \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} in curvilinear coordinates initiated in the beginning of this section, it follows that \\begin{split} \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} &= \\sum \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{y})} \\, \\epsilon_{ijk} \\hat{v}_i(\\mathsf{y})\\hat{w}_j(\\mathsf{y}) \\, \\mathsf{g}^k(\\mathsf{y})\\\\ &= \\sum \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{y})} \\, \\epsilon_{ijk} \\, g^{ka}(\\mathsf{y}) \\, \\hat{v}_i(\\mathsf{y})\\hat{w}_j(\\mathsf{y}) \\, \\mathsf{g}_a(\\mathsf{y}). \\end{split} The divergence of \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} can now be computed in curvilinear coordinates as \\begin{split} \\nabla \\cdot (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) &= \\sum \\epsilon_{ijk} \\left(\\partial_a \\left(g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\, \\hat{v}_i(\\mathsf{y})\\right) + g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\, \\Gamma^b_{ab}(\\mathsf{y}) \\hat{v}_i(\\mathsf{y})\\right)\\hat{w}_j\\\\ &= \\left(\\sum \\hat{w}_c\\mathsf{g}_c(\\mathsf{y})\\right) \\cdot \\left(\\sum \\epsilon_{ijk} \\left(\\partial_a \\left(g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})}\\right) \\hat{v}_i(\\mathsf{y}) \\right.\\right.\\\\ & \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left. + g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\left(\\partial_a \\hat{v}_i(\\mathsf{y}) + \\Gamma^b_{ab}(\\mathsf{y}) \\hat{v}_i(\\mathsf{y})\\right)\\right) \\mathsf{g}^j(\\mathsf{y})\\right). \\end{split} The curvilinear coordinate expression for the curl of the vector field \\mathsf{v} follows at once from this calculation that \\nabla \\times \\mathsf{v}(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\left(\\partial_a \\left(g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})}\\right) \\hat{v}_i(\\mathsf{y}) + g^{ka}(\\mathsf{y})\\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\left(\\partial_a \\hat{v}_i(\\mathsf{y}) + \\Gamma^b_{ab}(\\mathsf{y}) \\hat{v}_i(\\mathsf{y})\\right)\\right) \\mathsf{g}^j(\\mathsf{y}). The curl of tensor fields of higher order is computed along the same lines. The final expressions are not provided here since they have cumbersome algebraic forms in curvilinear coordinates.","title":"Curl"},{"location":"curvilinear_coordinates/#integration","text":"Suppose that f:U \\to \\mathbb{R} is a scalar field on an open subset U \\subseteq \\mathbb{R}^3 . Given a coordinate system (U,\\mathsf\\phi) on \\mathbb{R}^3 , the integral of f over U is defined as follows: \\begin{split} \\int_U f(\\mathsf{x})\\,dv &= \\int_U f(x_1, x_2, x_3)\\,dx_1 dx_2 dx_3\\\\ &= \\int_{\\mathsf\\phi(U)} (f \\circ \\mathsf\\phi^{-1})(y_1, y_2, y_3) \\, \\text{det}\\left(\\frac{\\partial \\mathsf{x}}{\\partial \\mathsf{y}}\\right) dy_1 dy_2 dy_3\\\\ &= \\int_{\\mathsf\\phi(U)} (f \\circ \\mathsf\\phi^{-1})(y_1, y_2, y_3) \\sqrt{\\text{det }\\mathsf{g}(\\mathsf{x})} \\, dy_1 dy_2 dy_3. \\end{split} In the equation above \\partial \\mathsf{x}/\\partial \\mathsf{y} denotes the matrix whose (i,j)^{\\text{th}} component is \\partial x_i(\\mathsf{y})/\\partial y_j . This result follows from the change of variables formula for multiple integrals.","title":"Integration"},{"location":"curvilinear_coordinates/#special-coordinate-systems","text":"...","title":"Special coordinate systems"},{"location":"inner_product_spaces/","text":"We begin with a discussion of the algebraic properties of vectors , which are defined as elements of a special kind of a set called a vector space . We will then define an additional structure called the inner product that significantly simplifies the mathematical development. We will learn how to represent a vector with respect to a chosen basis , and how this representation changes when the basis changes. Finally, we will study linear maps between vector spaces, and their representation with respect to chosen bases of the vector spaces. To keep the presentation simple, technical proofs for many of the statements given here are omitted. Basic notions from set theory and matrix algebra, reviewed in two appendices, are assumed to be known. Introduction A vector is typically introduced in high school algebra as a quantity with both a magnitude and a direction. A representation of a vector in the familiar three dimensional Euclidean space is shown in the following figure: Representation of a vector in \\mathbb{R}^3 If \\mathsf{i},\\mathsf{j}, \\mathsf{k} represent the unit vectors along the x,y,z axes, respectively, a vector \\mathsf{v} can be expressed uniquely as \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} , where v_x, v_y, v_z are the Cartesian components of the vector \\mathsf{v} (with respect to the basis vectors \\mathsf{i},\\mathsf{j} and \\mathsf{k} ). There are two core operations associated with vectors: Vector addition : Given two vectors \\mathsf{u} = u_x \\mathsf{i} + u_y \\mathsf{j} + u_z \\mathsf{k} and \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} , we can add them to get a new vector \\mathsf{u} + \\mathsf{v} , defined as \\mathsf{u} + \\mathsf{v} = (u_x + v_x) \\mathsf{i} + (u_y + v_y) \\mathsf{j} + (u_z + v_z) \\mathsf{k}. Geometrically, the new vector \\mathsf{u} + \\mathsf{v} is obtained by placing the tail of \\mathsf{v} at the head of \\mathsf{u} . The sum of the two vectors is then the vector which shares it\u2019s tail with \\mathsf{u} and head with \\mathsf{v} , as shown below: Vector addition in \\mathbb{R}^3 Note that we get the same vector independent of the order of addition: \\mathsf{u} + \\mathsf{v} = \\mathsf{v} + \\mathsf{u} . For this reason, vector addition is said to be commutative . It can also be shown easily that if \\mathsf{u}, \\mathsf{v}, \\mathsf{w} are three vectors, then (\\mathsf{u} + \\mathsf{v}) + \\mathsf{w} = \\mathsf{u} + (\\mathsf{v} + \\mathsf{w}) . This property is called associativity . Scalar multiplication of a vector with a real number: Given any vector \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} , we can multiply it by some real number a to get the new vector that is a times as long as \\mathsf{v} : a\\mathsf{v}= av_x \\mathsf{i} + av_y \\mathsf{j} + av_z \\mathsf{k} . This is illustrated in the following figure: Scalar multiplication of a vector in \\mathbb{R}^3 Note that we will need only real vector spaces in what follows. Some vector spaces admit an algebraic operation called the inner product . For instance, in three dimensional space, given two vectors \\mathsf{u} = u_x \\mathsf{i} + u_y \\mathsf{j} + u_z \\mathsf{k} and \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} , we can combine them using the dot product to produce a real number \\mathsf{u} \\cdot \\mathsf{v} = u_x v_x + u_y v_y + u_z v_z . Using the dot product, it is customary to define the length or Euclidean norm of a vector \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} as the (non-negative) real number \\lVert \\mathsf{v} \\rVert = (\\mathsf{v} \\cdot \\mathsf{v})^{\\frac{1}{2}} = (v_x^2 + v_y^2 + v_z^2)^{\\frac{1}{2}} . For vectors in three dimensional space (only), we also an additional important algebraic operation called the cross product : we can combine two vectors \\mathsf{u} = u_x \\mathsf{i} + u_y \\mathsf{j} + u_z \\mathsf{k} and \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} using the cross product to obtain a new vector \\mathsf{u} \\times \\mathsf{v} = (u_y v_z - u_z v_y) \\mathsf{i} + (u_z v_x - u_x v_z) \\mathsf{j} + (u_x v_y - u_y v_x) \\mathsf{k} . In what follows, we will first focus on just vector addition and scalar multiplication. These two operations embody a concept known as linearity , which is fundamental to appreciate what a vector is. We will then study inner product spaces , which are vector spaces with an additional structure known as an inner product , and highlight Euclidean spaces as an important example of inner product spaces. Remark The generalization of the cross product is known as the wedge product . We will not study the wedge product in detail in these notes since it is beyond the scope of these notes. Notice that all the information about the vector \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} is contained in the ordered set of three real numbers (v_x, v_y, v_z) . What this means is that given any vector \\mathsf{v} in three dimensional space, we can uniquely associate with it a triple of real numbers, and vice versa. The set of all such ordered triples is the set \\mathbb{R}^3 , which is the set of all triples of real numbers. If we define addition of two such ordered triples and multiplication of an ordered triple with a real number as, \\begin{gathered} (u_x, u_y, u_z) + (v_x, v_y, v_z) = (u_x + v_x, u_y + v_y, u_z + v_z), \\nonumber\\\\ c(u_x, u_y, u_z) = (cu_x, cu_y, cu_z), \\nonumber\\end{gathered} then the elements of \\mathbb{R}^3 , which are ordered triples of real numbers, behave exactly as the geometric picture of vectors as arrows that we just discussed, as far as the core properties of vector addition and scalar multiplication are concerned. We now have two ways of representing a vector: as an arrow in three dimensional space, and as a set of ordered triple of real numbers. Both of these represent the same object. But the representation in terms of ordered tuples of real numbers immediately admits a generalization to cases where the pictorial representation fails. It is evident from the above discussion that there is nothing special about the number 3 when we considered an ordered triple of real numbers. We can easily generalize this to a set of ordered n -tuple of real numbers (u_1,u_2,\\ldots,u_n) \\in \\mathbb{R}^n , where n \\in \\mathbb{N} could be any arbitrary positive integer. We can define addition and scalar multiplication in \\mathbb{R}^n analogous to the case of the ordered triples, \\begin{gathered} (u_1, u_2, \\ldots, u_n) + (v_1, v_2, \\ldots, v_n) = (u_1 + v_1, u_2 + v_2, \\ldots, u_n + v_n), \\label{eq:linearity_Rn_add}\\\\ a(u_1, u_2, \\ldots, u_n) = (au_1, au_2, \\ldots, au_n),\\label{eq:linearity_Rn_sm}\\end{gathered} where (u_1,u_2,\\ldots,u_n) and (v_1,v_2,\\ldots,v_n) are two elements of \\mathbb{R}^n , and a is a real number. We will call this the standard linear structure on \\mathbb{R}^n , and elements of \\mathbb{R}^n as vectors in \\mathbb{R}^n . Note that there is no obvious way to picture an arrow in the n -dimensional space \\mathbb{R}^n for n > 3 . Thus, by choosing the right representation, we can extend the elementary notion of vectors as quantities with magnitude and direction to more general objects. Linear structure Let us now generalize the previous discussion to define abstract vector spaces . Suppose that V is a set such that it is possible to define two maps \\begin{split} +:V \\times V \\to V; & \\quad +(\\mathsf{u},\\mathsf{v}) \\mapsto \\mathsf{u} + \\mathsf{v},\\\\ \\cdot:V \\times V \\to V; & \\quad \\cdot(a, \\mathsf{u}) \\mapsto a\\mathsf{u}, \\end{split} called vector addition and scalar multiplication , respectively, in V , that satisfy the following properties: for any \\mathsf{u}, \\mathsf{v}, \\mathsf{w} \\in V and a,b \\in \\mathbb{R} , Associativity of addition: \\mathsf{u} + (\\mathsf{v} + \\mathsf{w}) = (\\mathsf{u} + \\mathsf{v}) + \\mathsf{w} , Commutativity of addition: \\mathsf{u} + \\mathsf{v} = \\mathsf{v} + \\mathsf{u} , Existence of additive identity : there exists a unique element \\mathsf{0} \\in V , called the additive identity of V such that \\mathsf{u} + \\mathsf{0} = \\mathsf{u} , Existence of additive inverse : for every \\mathsf{u} \\in V , there exists a unique element -\\mathsf{u} \\in V such that \\mathsf{u} + (-\\mathsf{u}) = \\mathsf{0} , Distributivity of scalar multiplication over vector addition: a(\\mathsf{u} + \\mathsf{v}) = a\\mathsf{u} + a\\mathsf{v} , Distributivity of scalar multiplication over scalar addition: (a + b)\\mathsf{u} = a\\mathsf{u} + b\\mathsf{u} , Compatibility of scalar multiplication with field multiplication: a(b\\mathsf{u}) = (ab)\\mathsf{u} , Scaling property of scalar multiplication: 1\\mathsf{u} = \\mathsf{u} . A set V that has two maps that satisfy these axioms is called a (real) vector space , or a (real) linear space . What we have accomplished through these axioms is to endow a set with a notion of addition that allows us to add two elements of the set to get a third element. We have also provided a mechanism to multiply a member of this set by a real number to get another element of this set. The maps + and \\cdot are said to provide a linear structure on V . Elements of V are called vectors . Remark Some textbooks mention additional closure axioms that indicate that if \\mathsf{u}, \\mathsf{v} \\in V , then \\mathsf{u} + \\mathsf{v} \\in V , and given any \\mathsf{u} \\in V and a \\in \\mathbb{R} , a \\mathsf{u} \\in V . We don\u2019t specify this explicitly since this is already implied by the function definitions +:V \\times V \\to V and \\cdot:\\mathbb{R} \\times V \\to V . As mentioned in the previous discussion on set theory, we will always insist on mentioning the domain and codomain of every map/function we encounter. Hence, the so-called closure axioms are redundant for our purposes. Remark Following standard convention, we will often use the shorthand notation \\mathsf{u} - \\mathsf{v} for \\mathsf{u} + (-\\mathsf{v}) . Example The simplest example of a real vector space is the set of real numbers \\mathbb{R} with addition and multiplication defined in the standard manner. More generally, consider the set \\mathbb{R}^n consisting of all n -tuples of real numbers: \\mathbb{R}^n = \\{(u_1, \\ldots, u_n)\\,|\\,\\forall\\,i=1,\\ldots,n, \\; u_i \\in \\mathbb{R}\\}. Given (u_1, \\ldots, u_n) \\in \\mathbb{R}^n , (v_1, \\ldots, v_n) \\in \\mathbb{R}^n , and a \\in \\mathbb{R} , let us define addition +:\\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}^n and scalar multiplication \\cdot:\\mathbb{R}\\times \\mathbb{R}^n \\to \\mathbb{R}^n as \\begin{split} (u_1, \\ldots, u_n) + (v_1, \\ldots, v_n) &= (u_1 + v_1, \\ldots, u_n + v_n),\\\\ a \\cdot (u_1, \\ldots, u_n) &= (au_1, \\ldots, au_n). \\end{split} It is straightforward to verify that with addition and scalar multiplication thus defined, the triple (\\mathbb{R}^n,+,\\cdot) is a real vector space. Note that the additive identity in \\mathbb{R}^n is the zero vector (0, \\ldots, 0) \\in \\mathbb{R}^n , and the additive inverse of (u_1, \\ldots, u_n) \\in \\mathbb{R}^n is (-u_1, \\ldots, -u_n) \\in \\mathbb{R}^n . Example Consider the set \\mathbb{R}^{m \\times n} of all m \\times n matrices with real entries and defined addition of matrices and scalar mutliplication of a matrix with a real number in the usual sense (see Appendix ). It is easily checked that the set of all m \\times n matrices is a vector space. The zero vector in \\mathbb{R}^{m\\times n} is the matrix with zero in all of its entries, and the additive inverse of a given matrix is just its negative. Example The definition of vector spaces admits more general kinds of objects. As a simple example, consider the set C^0(\\mathbb{R},\\mathbb{R}) consisting of all real-valued and continuous functions of one real variable. Given any f, g \\in C^0(\\mathbb{R},\\mathbb{R}) , and any a \\in \\mathbb{R} , we can define addition and scalar multiplication pointwise as follows: for any x \\in \\mathbb{R} , \\begin{split} (f + g)(x) &= f(x) + g(x),\\\\ (a\\cdot f)(x) &= a \\, f(x). \\end{split} It is not difficult to verify that (C^0(\\mathbb{R},\\mathbb{R}), +, \\cdot) is a real vector space. The additive identity in C^0(\\mathbb{R},\\mathbb{R}) is the zero function 0 \\in C^0(\\mathbb{R},\\mathbb{R}) defined as follows: for any x \\in \\mathbb{R} , 0(x) = 0 . The additive inverse of f \\in C^0(\\mathbb{R},\\mathbb{R}) is the function -f \\in C^0(\\mathbb{R},\\mathbb{R}) defined as follows: for any x \\in \\mathbb{R} , (-f)(x) = -f(x) . Subspaces and linear independence A subset U \\subseteq V of a vector space V is said to be a linear subspace of V if U is also a vector space. Note that it is implicitly assumed that both U and V share the operations of vector addition and scalar multiplication. It can be easily checked that if a subset U \\subseteq V of a real vector space V has the property that for any \\mathsf{u}, \\mathsf{v} \\in U , and any a,b \\in \\mathbb{R} , (a\\mathsf{u} + b\\mathsf{v}) \\in U , then U is a linear subspace of V . This property is often used to check if a given subset of a vector space is a linear subspace. An immediate consequence of this is the fact that every linear subspace of a given vector space must contain the additive identity \\mathsf{0} \\in V . Example Consider the following subsets of \\mathbb{R}^3 : \\begin{split} S_1 &= \\{(x,y,z) \\in \\mathbb{R}^3 \\,|\\, x + y + z = 0\\},\\\\ S_2 &= \\{(x,y,z) \\in \\mathbb{R}^3 \\,|\\, x + y + z = 1\\},\\\\ S_3 &= \\{(x,y,z) \\in \\mathbb{R}^3 \\,|\\, y = z = 0\\}. \\end{split} To see that S_1 is a linear subspace of \\mathbb{R}^3 , note that if (x_1, y_1, z_1) \\in S_1 , (x_2, y_2, z_2) \\in S_1 , and a, b \\in \\mathbb{R} , then a(x_1, y_1, z_1) + b(x_2, y_2, z_2) = ((ax_1 + bx_2), (ay_1 + by_2), (az_1 + bz_2)) \\in \\mathbb{R}^3 . Since (ax_1 + bx_2) + (ay_1 + by_2) + (az_1 + bz_2) = a(x_1 + y_1 + z_1) + b(x_2 + y_2 + z_2) = (0,0,0) , we see that ((ax_1 + bx_2), (ay_1 + by_2), (az_1 + bz_2)) \\in S_1 . This shows that S_1 is indeed a linear subspace of \\mathbb{R}^3 . It is likewise verified that S_3 is also a linear subspace \\mathbb{R}^3 , while S_2 is not. The intersection of two linear subspaces is also a linear subspace. Moreover, any finite intersection of linear subspaces of a vector space V is also a linear subspace of V , as can be easily checked. Example In the previous example, the intersection of the subspaces S_1 = \\{(x,y,z) \\in \\mathbb{R}^3 \\,|\\, x + y + z = 0\\} and S_3 = \\{(x,y,z) \\in \\mathbb{R}^3 \\,|\\, y = z = 0\\} of \\mathbb{R}^3 is easily seen to be S_1 \\cap S_3 = \\{(0,0,0)\\}, which is the trivial subspace of \\mathbb{R}^3 . Two non-zero vectors \\mathsf{u},\\mathsf{v} \\in V in a vector space V are said to be linearly independent iff a\\mathsf{u} + b\\mathsf{v} = 0 , where a,b are real numbers, implies that a=0 and b=0 . Thus \\mathsf{u} and \\mathsf{v} are linearly independent iff the only linear combination of \\mathsf{u} and \\mathsf{v} that yields \\mathsf{0} is the trivial linear combination 0\\mathsf{u} + 0\\mathsf{v} . If this is not true, then \\mathsf{u} and \\mathsf{v} are said to be linearly dependent . This definition can be easily extended to a finite set of non-zero vectors \\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n , where n \\in \\mathbb{N} . Thus, the set of vectors \\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n in V is said to be linearly independent iff the only real numbers a_1, a_2, \\ldots, a_n that satisfy the equation \\sum_{i=1}^n a_i \\mathsf{v}_i = 0, are a_1 = a_2 = \\ldots = a_n = 0 . Example Consider the vectors \\mathsf{v}_1 = (1,2) \\in \\mathbb{R}^2 and \\mathsf{v}_2 = (2,3)\\in\\mathbb{R}^2 in the two dimensional Euclidean space \\mathbb{R}^2 . For real numbers a, b \\in \\mathbb{R} , note that a\\mathsf{v}_1 + b\\mathsf{v}_2 = 0 \\quad\\Rightarrow\\quad (a + 2b, 2a + 3b) = (0,0). Solving the equations a + 2b = 0 and 2a + 3b = 0 , we immediately see that a = b = 0 , which shows that \\mathsf{v}_1 and \\mathsf{v}_2 are linearly independent vectors in \\mathbb{R}^2 . If \\mathsf{v}_3 = (2,4) \\in \\mathbb{R}^2 , then \\mathsf{v}_1 and \\mathsf{v}_3 are linearly dependent since if for a, b \\in \\mathbb{R} , a\\mathsf{v}_1 + b\\mathsf{v}_3 = 0 \\quad\\Rightarrow\\quad (a + 2b, 2a + 4b) = (0,0). These equations do not imply that a = b = 0 . For instance, a = -2, b = 1 satisfies the condition. We thus see that \\mathsf{v}_1 and \\mathsf{v}_3 are linearly dependent. Basis of a vector space We will now introduce a very important tool called the basis of a vector space. The basic idea is that once we identify a basis for a vector space, we can use the linear structure inherent in the space to reduce all computations related to the vector space as a whole, to just computations on the basis set. We will need a few definitions first in order to define the basis. The linear span of a set of vectors \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_m\\} in V , written as \\text{span}(\\{\\mathsf{v}_1,\\ldots,\\mathsf{v}_m\\}) , is defined as the set of all it\u2019s linear combinations : \\text{span}(\\{\\mathsf{v}_1,\\ldots,\\mathsf{v}_m\\}) = \\left\\{\\sum_{i=1}^m a_i \\mathsf{v}_i \\,\\bigg\\vert\\, \\forall\\,1\\le i\\le m,\\,a_i \\in \\mathbb{R}\\right\\}. It is also common to refer to the linear span as just the span . It is straightforward to check that the linear span of any finite collection of vectors in a vector space V is a linear subspace of the vector space V . Example Let us consider the vectors \\mathsf{v}_1 = (1,2,3) \\in \\mathbb{R}^3 and \\mathsf{v}_2 = (2,3,4) \\in \\mathbb{R}^3 in \\mathbb{R}^3 . The span of these two vectors is the following subset of \\mathbb{R}^3 : \\text{span}(\\{\\mathsf{v}_1, \\mathsf{v}_2\\}) = \\{((a + 2b, 2a + 3b, 3a + 4b) \\in \\mathbb{R}^3 \\,|\\, a, b \\in \\mathbb{R}\\}. It is left as an easy exercise to verify that this is indeed a linear subspace of \\mathbb{R}^3 . An ordered subset S \\subseteq V a vector space V is said to constitute a basis of V if S is linearly independent, and \\text{span}(S) = V . In the special case when a vector space V is spanned by a finite ordered set of vectors (\\mathsf{v}_1, \\ldots, \\mathsf{v}_n) \\subseteq V , for some n \\in \\mathbb{N} , the vector space V is said to be finite dimensional , and the number n is called the dimension of the vector space V (written \\text{dim}(V) ). A vector space that is not finite dimensional is said to be infinite dimensional . In what follows, we will only deal with finite dimensional vector spaces. If V is an n -dimensional vector space, and (\\mathsf{v}_1, \\ldots, \\mathsf{v}_n) is a basis for V , then we will often abbreviate the basis as (\\mathsf{v}_i)_{i=1}^n , or just (\\mathsf{v}_i) , when the dimension n is evident from the context. Example Let us revisit the n -dimensional Euclidean space \\mathbb{R}^n , and consider the following vectors: for any 1 \\le i \\le n , \\mathsf{e}_i = (0, \\ldots, \\underbrace{1}_{i^{\\text{th}}\\text{ position}}, \\ldots 0). It is easy to check that the (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) is a basis of \\mathbb{R}^n . In particular, note that any (u_1, \\ldots, u_n) \\in \\mathbb{R}^n can be written as (u_1, \\ldots, u_n) = u_1(1, 0, \\ldots, 0) + \\ldots + u_n(0, \\ldots, 0, 1) = \\sum_{i=1}^n u_i \\mathsf{e}_i. The ordered set (\\mathsf{e}_i)_{i=1}^n is called the standard basis of \\mathbb{R}^n . Note that in the special case of \\mathbb{R}^3 , the set of basis vectors (\\mathsf{e}_1,\\mathsf{e}_2,\\mathsf{e}_3) is identical to the basis set (\\mathsf{i},\\mathsf{j},\\mathsf{k}) introduced in the beginning of this section. Given a vector space V of dimension n , it is possible to choose an infinite number of bases for V . This non-uniqueness in the choice of the basis can be easily understood as follows. Pick any \\mathsf{g}_1 \\in V . Choose \\mathsf{g}_2 from the set V \\setminus \\text{span}({\\mathsf{g}_1}) , \\mathsf{g}_3 from the set V \\setminus \\text{span}({\\mathsf{g}_1, \\mathsf{g}_2}) , and so on. This process will terminate in n steps since the vector space V is of dimension n . The resulting set of vectors (\\mathsf{g}_1, \\ldots, \\mathsf{g}_n) is a basis for V . Inner products and norms We will now introduce a special additional structure on an abstract vector space V called the inner product . There are two reasons for introducing the inner product right at the outset: first, the mathematical development becomes significantly simpler, and second, many important applications in science and engineering can be studied in this setting. Remark There is an elegant theory of abstract linear spaces, both in the finite and infinite dimensional cases, where inner products are not defined. We will however not develop this general theory here. Given a vector space V , an inner product on V is defined as a map of the form g:V \\times V \\to \\mathbb{R} such that, for any \\mathsf{u}, \\mathsf{v}, \\mathsf{w} \\in V and a,b \\in \\mathbb{R} , Symmetry : g(\\mathsf{u},\\mathsf{v}) = g(\\mathsf{v},\\mathsf{u}) , Bilinearity : g(\\mathsf{u},(a \\mathsf{v} + b \\mathsf{w})) = a g(\\mathsf{u},\\mathsf{v}) + b g(\\mathsf{u},\\mathsf{w}) , Positive definiteness : g(\\mathsf{u},\\mathsf{u}) \\ge 0 , and g(\\mathsf{u},\\mathsf{u}) = 0 iff \\mathsf{u} = \\mathsf{0} . A vector space V endowed with a map \\cdot:V \\times V \\to \\mathbb{R} that satisfies the three properties mentioned above is said to be an inner product space . All vector spaces considered henceforth will be assumed to be inner product spaces, unless stated otherwise. Remark Given \\mathsf{u}, \\mathsf{v} \\in V , we will often write g(\\mathsf{u}, \\mathsf{v}) as just \\mathsf{u} \\cdot \\mathsf{v} , with the understanding that the inner product g is evident from the context. Example The simplest, and also the most important, example of an inner product space is the vector space \\mathbb{R}^n defined earlier, with the inner product \\cdot:V \\times V \\to \\mathbb{R} defined as follows: for any (u_1, \\ldots, u_n) \\in \\mathbb{R}^n and (v_1, \\ldots, v_n) \\in \\mathbb{R}^n , (u_1, \\ldots, u_n) \\cdot (v_1, \\ldots, v_n) = \\sum_{i=1}^n u_i v_i. It is easy to check that this is indeed an inner product. The vector space \\mathbb{R}^n with this inner product is called the Euclidean space of dimension n . We will use the same symbol \\mathbb{R}^n to denote the n -dimensional Euclidean space. Example Define the set \\mathcal{P}_n([a,b],\\mathbb{R}) as the set of all real valued polynomials of degree less than or equal to n on the interval [a,b] \\subseteq \\mathbb{R} : \\mathcal{P}_n([a,b],\\mathbb{R}) = \\{f:[a,b] \\subseteq\\mathbb{R} \\to \\mathbb{R} \\,|\\, \\forall\\,x \\in [a,b], \\; f(x) = a_0 + a_1 x + \\ldots + a_n x^n, \\text{ where } a_0, a_1, \\ldots, a_n \\in \\mathbb{R}\\}. Define the function \\cdot:\\mathcal{P}_n([a,b],\\mathbb{R}) \\times \\mathcal{P}_n([a,b],\\mathbb{R}) \\to \\mathbb{R} as follows: for any f,g \\in \\mathcal{P}_n([a,b],\\mathbb{R}) , f \\cdot g = \\int_a^b f(x)g(x) \\, dx. It is left as a simple exercise to verify that this function is actually an inner product on the linear space \\mathcal{P}_n([a,b],\\mathbb{R}) with addition and scalar multiplication defined pointwise. The inner product on a vector space V can be used to define a norm on V . A norm on a vector space V is a function of the form \\lVert \\cdot \\rVert:V \\to \\mathbb{R} such that, for any \\mathsf{u}, \\mathsf{v} \\in V and a \\in \\mathbb{R} , Positive definiteness : \\lVert \\mathsf{u} \\rVert \\ge 0 , and \\lVert \\mathsf{u} \\rVert = 0 iff \\mathsf{u} = \\mathsf{0} , Homogeneity : \\lVert a \\mathsf{u} \\rVert = |a| \\lVert \\mathsf{u} \\rVert , Sub-additivity : \\lVert \\mathsf{u} + \\mathsf{v} \\rVert \\le \\lVert \\mathsf{u} \\rVert + \\lVert \\mathsf{v} \\rVert . Here, |a| refers to the absolute value of a \\in \\mathbb{R} . The property of sub-additivity is also referred to as the triangle inequality . A vector space V equipped with a norm \\lVert \\cdot \\rVert:V \\to \\mathbb{R} that satisfies these properties is called a normed vector space , or a normed linear space . Note that every inner product space is a normed linear space. To see this, note that given an an inner product \\cdot:V \\times V \\to \\mathbb{R} , the norm \\lVert \\cdot \\rVert: V \\to \\mathbb{R} induced by this inner product is defined as follows: for any \\mathsf{v} \\in V , \\lVert \\mathsf{v} \\rVert = \\sqrt{\\mathsf{v} \\cdot \\mathsf{v}}. The norm induced by the Euclidean inner product on \\mathbb{R}^3 is called the standard Euclidean norm on \\mathbb{R}^n . Remark In general, a normed vector space is not an inner product space. If, however, the norm \\lVert \\cdot \\rVert:V \\to \\mathbb{R} on a normed vector space V satisfies the following relation, called the parallelogram identity , \\frac{1}{2}\\left(\\lVert \\mathsf{u} + \\mathsf{v} \\rVert^2 + \\lVert \\mathsf{u} - \\mathsf{v}\\rVert^2\\right) = \\lVert \\mathsf{u} \\rVert^2 + \\lVert \\mathsf{v} \\rVert^2, for any \\mathsf{u}, \\mathsf{v} \\in V , then it is possible to define an inner product \\cdot:V \\times V \\to \\mathbb{R} using the norm as follows: for any \\mathsf{u},\\mathsf{v} \\in V , \\mathsf{u} \\cdot \\mathsf{v} = \\frac{1}{4}\\left(\\lVert \\mathsf{u} + \\mathsf{v} \\rVert^2 - \\lVert \\mathsf{u} - \\mathsf{v}\\rVert^2\\right). This relation is called the polarization identity . Example Let us consider the n -dimensional Euclidean space \\mathbb{R}^n with the standard inner product, defined earlier. The norm of a vector (x_1, \\ldots, x_n) \\in \\mathbb{R}^n is easily computed as \\lVert (x_1, \\ldots, x_n) \\rVert^2 = \\sum_{i=1}^n x_i^2. This norm is called the standard Euclidean norm , or the L_2 -norm on \\mathbb{R}^n . A variety of other norms can be defined on a given inner product space. For instance, the L_p -norm on \\mathbb{R}^n can be defined as follows: for any (x_1, \\ldots, x_n) \\in \\mathbb{R}^n and 1 \\le p < \\infty , \\lVert (x_1, \\ldots, x_n) \\rVert_p = \\left(\\sum_{i=1}^n x_i^p\\right)^{\\frac{1}{p}}. The L_\\infty -norm on \\mathbb{R}^n is defined as \\lVert (x_1, \\ldots, x_n) \\rVert_\\infty = \\text{max } \\{x_1, \\ldots, x_n\\}. Note that the standard Euclidean norm corresponds to \\lVert \\cdot \\lVert_2 . Remark There is an important theorem that states that all norms on a finite dimensional vector space are equivalent . This means the following: given norms \\lVert \\cdot \\rVert_1:V \\to \\mathbb{R} and \\lVert \\cdot \\rVert_2:V \\to \\mathbb{R} on a finite dimensional vector space V , there exists constants c_L, c_U \\in \\mathbb{R} such that, for any \\mathsf{v} \\in V , c_L \\lVert \\mathsf{v} \\rVert_2 \\le \\lVert \\mathsf{v} \\rVert_1 \\le c_U \\lVert \\mathsf{v} \\rVert_2. Without getting into technical details, this roughly means that the conclusions we draw about topological notions in a normed vector space are independent of the specific norm chosen. Cauchy-Schwarz inequality Let V be a real vector space equipped with an inner product \\cdot:V \\times V \\to \\mathbb{R} . An important property of inner products that turns out to be quite useful in practice is discussed now. Given any \\mathsf{u}, \\mathsf{v} \\in V , the Cauchy-Schwarz inequality states that |\\mathsf{u} \\cdot \\mathsf{v}| \\le \\lVert \\mathsf{u} \\rVert \\lVert \\mathsf{v} \\rVert. Furthermore, the equality holds iff \\mathsf{u} and \\mathsf{v} are linearly dependent. The proof of the Cauchy-Schwarz inequality is quite easy: for any \\mathsf{u}, \\mathsf{v} \\in V , and a \\in \\mathbb{R} , \\lVert \\mathsf{u} + a \\mathsf{v} \\rVert^2 \\ge 0 \\Rightarrow \\lVert \\mathsf{u} \\rVert^2 + 2a \\mathsf{u} \\cdot \\mathsf{v} + \\rVert \\mathsf{v} \\rVert^2 \\ge 0. Substituting a = -\\frac{\\mathsf{u} \\cdot \\mathsf{v}}{\\lVert \\mathsf{v} \\rVert^2} in this inequality, we get (\\mathsf{u} \\cdot \\mathsf{v})^2 \\le \\lVert \\mathsf{u} \\rVert^2 \\lVert \\mathsf{v} \\rVert^2, which immediately yields the Cauchy-Schwartz inequality. To prove the second part, note that if \\mathsf{u} and \\mathsf{v} are linearly dependent, then, without loss of generality, \\mathsf{v} = a\\mathsf{u} for some a \\ in \\mathbb{R} . In this case, \\mathsf{u} \\cdot \\mathsf{v} = \\lvert a \\rvert \\lVert \\mathsf{u} \\rVert^2 = \\lVert \\mathsf{u} \\rVert \\lVert \\mathsf{v} \\rVert . On the other hand, if \\mathsf{u} \\cdot \\mathsf{v} = \\lVert \\mathsf{u} \\rVert \\lVert \\mathsf{v} \\rVert , consider the vector \\mathsf{w} \\in V , where, \\mathsf{w} = \\mathsf{u} - \\frac{\\lVert \\mathsf{u} \\rVert}{\\lVert \\mathsf{v} \\rVert}\\mathsf{v}. It is straightforward to show that \\lVert \\mathsf{w} \\rVert = 0 , and hence that \\mathsf{w} = 0 . This shows that \\mathsf{u} and \\mathsf{v} are linearly dependent. The Cauchy-Schwartz inequality is thus proved. The angle \\theta:V \\times V \\to \\in [0,2\\pi) \\subseteq \\mathbb{R} between two vectors \\mathsf{u}, \\mathsf{v} \\in V is defined via the relation \\cos \\theta(\\mathsf{u},\\mathsf{v}) = \\frac{\\mathsf{u} \\cdot \\mathsf{v}}{\\lVert \\mathsf{u} \\rVert\\lVert \\mathsf{v} \\rVert}. Note how the Cauchy-Schwarz inequality implies that this definition is well-defined. If the angle between two vectors \\mathsf{u}, \\mathsf{v} \\in V is \\pi/2 , they are said to be orthogonal . Equivalently, \\mathsf{u}, \\mathsf{v} \\in V are said to be orthogonal if \\mathsf{u} \\cdot \\mathsf{v} = 0 . If \\mathsf{u}, \\mathsf{v} \\in V are orthogonal, and if it is further true that \\lVert \\mathsf{u} \\rVert = \\lVert \\mathsf{v} \\rVert = 1 , then \\mathsf{u} and \\mathsf{v} are said to be orthonormal . Example The vectors \\mathsf{u} = (1, \\sqrt{3}) \\in \\mathbb{R}^2 and \\mathsf{v} = (-\\sqrt{3}, 1) \\in \\mathbb{R}^2 are orthogonal since \\mathsf{u} \\cdot \\mathsf{v} = 0 . They are not orthonormal since \\lVert \\mathsf{u} \\rVert = \\lVert \\mathsf{v} \\rVert = 2 . The vectors \\tilde{\\mathsf{u}} = \\mathsf{u}/\\lVert \\mathsf{u} \\rVert = (1/2, \\sqrt{3}/2) \\in \\mathbb{R}^2 and \\mathsf{v} = \\mathsf{v}/\\lVert \\mathsf{v} \\rVert = (-\\sqrt{3}/2, 1/2) \\in \\mathbb{R}^2 are, however, orthonormal. The following inequality also holds for any \\mathsf{u},\\mathsf{v} \\in V in an inner product space V : \\lVert \\mathsf{u} + \\mathsf{v} \\rVert \\le \\lVert \\mathsf{u} \\rVert + \\lVert \\mathsf{v} \\rVert. Recall that this is the triangle inequality . The triangle inequality is readily proved using the Cauchy-Schwarz inequality: for any \\mathsf{u}, \\mathsf{v} \\in V : \\begin{split} \\lVert \\mathsf{u} + \\mathsf{v} \\rVert^2 &= \\lVert \\mathsf{u} \\rVert^2 + \\lVert \\mathsf{v} \\rVert^2 + 2 \\mathsf{u} \\cdot \\mathsf{v}\\\\ &\\le \\lVert \\mathsf{u} \\rVert^2 + \\lVert \\mathsf{v} \\rVert^2 + 2 \\lVert \\mathsf{u} \\rVert \\lVert \\mathsf{v} \\rVert\\\\ &= (\\lVert \\mathsf{u} \\rVert + \\lVert \\mathsf{v} \\rVert)^2. \\end{split} The triangle inequality follows by taking the square root on both sides. Gram-Schmidt orthogonalization Let us now reconsider the notion of a basis of an n -dimensional vector space V in the special case when the vector space also has an inner product \\cdot:V \\times V \\to \\mathbb{R} defined on it. We say that a basis (\\mathsf{g}_i) of V is orthogonal if \\mathsf{g}_i \\cdot \\mathsf{g}_j = 0 whenever i, j \\in \\{1, \\ldots, n\\} , and i \\neq j . If it is further true that \\mathsf{g}_i \\cdot \\mathsf{g}_i = 1 for every i \\in \\{1, \\ldots, n\\} , we say that the basis (\\mathsf{g}_i) is orthonormal . The fact that a basis (\\mathsf{g}_i) of V is orthonormal can be succinctly expressed by the following equation: for any i,j \\in \\{1,\\ldots,n\\} , \\mathsf{g}_i \\cdot \\mathsf{g}_j = \\delta_{ij}, where \\delta_{ij} is the Kr\u00f6necker delta symbol that is defined as follows: \\delta_{ij} = \\begin{cases} 1, & i = j,\\\\ 0, & i \\neq j. \\end{cases} Example It is straightforward to verify that the standard basis (\\mathsf{e}_i) of \\mathbb{R}^n is an orthonormal basis, since it follows from the definition of the standard basis that \\mathsf{e}_i \\cdot \\mathsf{e}_j = \\delta_{ij} . The Gram-Schmidt orthogonalization procedure is an algorithm that helps us to transform any given basis (\\tilde{\\mathsf{g}}_i) of V into an orthonormal basis (\\mathsf{g}_i) . The algorithm works as follows: Let \\mathsf{g}_1 = \\frac{\\tilde{\\mathsf{g}}_1}{\\lVert \\tilde{\\mathsf{g}}_1 \\rVert}. We now define \\mathsf{g}_2 by removing the component of \\tilde{\\mathsf{g}}_2 along the direction \\mathsf{g}_1 : \\mathsf{g}_2 = \\frac{\\tilde{\\mathsf{g}}_2 - (\\tilde{\\mathsf{g}}_2 \\cdot \\mathsf{g}_1)e_1}{\\lVert \\tilde{\\mathsf{g}}_2 - (\\tilde{\\mathsf{g}}_2 \\cdot \\mathsf{g}_1)\\mathsf{g}_1 \\rVert}. It is easy to check that \\mathsf{g}_2 \\cdot \\mathsf{g}_1 = 0 and \\lVert \\mathsf{g}_2 \\rVert = 1 . We then obtain \\mathsf{g}_3 in a similar manner by removing the components of \\tilde{\\mathsf{g}}_3 along \\mathsf{g}_1 and \\mathsf{g}_2 : \\mathsf{g}_3 = \\frac{\\tilde{\\mathsf{g}}_3 - (\\tilde{\\mathsf{g}}_3 \\cdot \\mathsf{g}_2)\\mathsf{g}_2 - (\\tilde{\\mathsf{g}}_3 \\cdot \\mathsf{g}_1)e_1}{\\lVert \\tilde{\\mathsf{g}}_3 - (\\tilde{\\mathsf{g}}_3 \\cdot \\mathsf{g}_2)\\mathsf{g}_2 - (\\tilde{\\mathsf{g}}_3 \\cdot \\mathsf{g}_1)\\mathsf{g}_1 \\rVert}. It is straightforward to verify that \\mathsf{g}_1 \\cdot \\mathsf{g}_3 = 0 , \\mathsf{g}_2 \\cdot \\mathsf{g}_3 = 0 and \\lVert \\mathsf{g}_3 \\rVert = 1 . Continuing this process, we can construct an orthonormal basis (\\mathsf{g}_i)_{i=1}^n . Example As a simple illustration of the Gram-Schmidt orthogonalization process, let us consider the vectors \\tilde{\\mathsf{g}}_1 = (1,2) \\in \\mathbb{R}^2 and \\tilde{\\mathsf{g}}_2 = (2,3) \\in \\mathbb{R}^2 . We verified earlier that these vectors are linearly independent, and that they form a basis of \\mathbb{R}^2 . They are however not orthogonal since \\tilde{\\mathsf{g}}_1 \\cdot \\tilde{\\mathsf{g}}_2 = 8 \\neq 0 . Let us orthonormalize this basis using the Gram-Schmidt process. To start with, let us normalize \\tilde{\\mathsf{g}}_1 : \\mathsf{g}_1 = \\frac{\\tilde{\\mathsf{g}}_1}{\\lVert \\tilde{\\mathsf{g}}_1 \\rVert} = \\left(\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}\\right) \\in \\mathbb{R}^2. We can now construct \\mathsf{g}_2 \\in \\mathbb{R}^2 by projecting out the component of \\tilde{\\mathsf{g}}_2 along \\mathsf{g}_1 : \\begin{split} \\mathsf{g}_2 &= \\frac{\\tilde{\\mathsf{g}}_2 - (\\tilde{\\mathsf{g}}_2 \\cdot \\mathsf{g}_1)\\mathsf{g}_1}{\\lVert \\tilde{\\mathsf{g}}_2 - (\\tilde{\\mathsf{g}}_2 \\cdot \\mathsf{g}_1)\\mathsf{g}_1 \\rVert}\\\\ &= \\frac{(2,3) - \\left((2,3)\\cdot(1/\\sqrt{5},2/\\sqrt{5})\\right)(1/\\sqrt{5},2/\\sqrt{5})}{\\lVert (2,3) - \\left((2,3)\\cdot(1/\\sqrt{5},2/\\sqrt{5})\\right)(1/\\sqrt{5},2/\\sqrt{5})\\rVert}\\\\ &= \\frac{(2/5, -1/5)}{\\lVert (2/5, -1/5) \\rVert}\\\\ &= \\left(\\frac{2}{\\sqrt{5}}, -\\frac{1}{\\sqrt{5}}\\right) \\in \\mathbb{R}^2. \\end{split} It is easily checked that \\lVert \\mathsf{g}_1 \\rVert = 1 and \\lVert \\mathsf{g}_2 \\rVert = 1 , and that \\mathsf{g}_1 \\cdot \\mathsf{g}_2 = \\left(\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}\\right) \\cdot \\left(\\frac{2}{\\sqrt{5}}, -\\frac{1}{\\sqrt{5}}\\right) = 0. We have thus constructed an orthonormal basis (\\mathsf{g}_1, \\mathsf{g}_2) of \\mathbb{R}^2 starting from the general basis (\\tilde{\\mathsf{g}}_1, \\tilde{\\mathsf{g}}_2) of \\mathbb{R}^2 by following the Gram-Schmidt algorithm. Note that the orientation of the basis (\\mathsf{g}_1, \\mathsf{g}_2) obtained here is the opposite of the orientation of the standard basis (\\mathsf{e}_1, \\mathsf{e}_2) . To see this, embed these vectors in \\mathbb{R}^3 to get the vectors \\mathsf{e}_1 = (1,0,0) , \\mathsf{e}_2 = (0,1,0) , \\mathsf{g}_1 = (1/\\sqrt{5}, 2/\\sqrt{5}, 0) , \\mathsf{g}_2 = (2/\\sqrt{5}, -1/\\sqrt{5}, 0) , and note that \\mathsf{e}_1 \\times \\mathsf{e}_2 = \\mathsf{e}_3 , whereas \\mathsf{g}_1 \\times \\mathsf{g}_2 = -\\sqrt{5} \\hat{e}_3 - they have opposite signs! This doesn\u2019t really affect the Gram-Schmidt algorithm because if (\\mathsf{g}_1, \\mathsf{g}_2) is a basis of \\mathbb{R}^2 , then so is (\\mathsf{g}_2, \\mathsf{g}_1) . Remark It turns out that the choice of an orthonormal basis is sufficient for most applications. In the discussion below, we will first study various concepts with respect to the choice of an orthonormal basis, since the calculations are much simpler in this case. The general case of arbitrary bases will be discussed after this to give an idea of how some calculations can be more involved with respect to general bases. Basis representation of vectors We will now study the representation of a vector \\mathsf{v} \\in V in an n -dimensional inner product space V with respect to an orthonormal basis (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) of V . It is worth reiterating that we will only deal with orthonormal bases unless otherwise stated. The fact that (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) is a basis of V implies that every \\mathsf{v} \\in V can be written as \\mathsf{v} = \\sum_{i=1}^n v_i \\mathsf{e}_i where v_i \\in \\mathbb{R} for every i \\in \\{1, \\ldots, n\\} . This is called the representation of v with respect to the basis (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) . The real numbers v_1, \\ldots v_n are called the components of \\mathsf{v} with respect to the basis (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) . To compute the components v_i , we can exploit the fact that (\\mathsf{e}_i) is an orthonormal basis: \\mathsf{v} \\cdot \\mathsf{e}_j = v_j \\quad\\Rightarrow\\quad \\mathsf{v} = \\sum (\\mathsf{v} \\cdot \\mathsf{e}_i) \\mathsf{e}_i. The components v_i thus computed are unique since we have explicitly constructed each component v_i as \\mathsf{v} \\cdot \\mathsf{e}_i . We can alternatively show the uniqueness of the components based on the fact that the basis vectors are linearly independent by definition. Remark Notice how we have represented the sum on the right without representing the summation index, and the range of summation. We will write \\sum_i , or just \\sum in place of \\sum_{i=1}^n , whenever the range under consideration is obvious from the context. If no index is associated with the summation symbol, as in \\sum , it will be assumed that the sum is with respect to all repeating indices. In addition, we will assume that the range of the all the indices involved is known from the context. While on this, it is worth noting that many authors employ the Einstein summation convention , according to which, a sum of the form \\sum u_i \\mathsf{e}_i is written simply as u_i \\mathsf{e}_i , with the summation over i being implicitly understood as long as the indices repeat twice. For pedagogical reasons, we will not follow the Einstein summation convention in these notes. Example As a trivial example of the basis representation of a vector, consider any (x_1, \\ldots, x_n) \\in \\mathbb{R}^n . This vector can be written with respect to the standard basis (\\mathsf{e}_i) of \\mathbb{R}^n as (x_1, \\ldots, x_n) = \\sum x_i \\mathsf{e}_i. Notice that x_i = (x_1, \\ldots, x_n) \\cdot \\mathsf{e}_i . Example As a non-trivial, yet simple, example of basis representation of a vector, consider the orthonormal basis (\\mathsf{g}_1, \\mathsf{g}_2) of \\mathbb{R}^2 , where \\mathsf{g}_1 = (2/\\sqrt{5}, -1/\\sqrt{5}) and \\mathsf{g}_2 = (1/\\sqrt{5}, 2/\\sqrt{5}) - notice that we have swapped the order of the basis constructed earlier in the context of the Gram-Schmidt procedure to maintain orientation. Consider any (x_1, x_2) \\in \\mathbb{R}^2 , we can express this in terms of the basis (\\mathsf{g}_1, \\mathsf{g}_2) as (x_1, x_2) = \\sum \\bar{x}_i \\mathsf{g}_i, for some real constants (\\bar{x}_i) . To compute this, note that we can write (x_1, x_2) = \\sum x_i \\mathsf{e}_i using the standard basis of \\mathbb{R}^2 . The constants (\\bar{x}_i) are easily computed by taking the appropriate dot products: \\begin{split} \\bar{x}_i &= \\left(\\sum x_j \\mathsf{e}_j\\right)\\cdot \\mathsf{g}_i\\\\ &= (\\mathsf{g}_i \\cdot \\mathsf{e}_1) x_1 + (\\mathsf{g}_i \\cdot \\mathsf{e}_2) x_2. \\end{split} For instance, the vector (1,2) \\in \\mathbb{R}^2 can be expressed in terms of the (\\mathsf{g}_1, \\mathsf{g}_2) basis as (1,2) = \\sum \\bar{x}_i \\mathsf{g}_i , where \\begin{split} \\bar{x}_1 &= \\frac{2}{\\sqrt{5}}1 - \\frac{1}{\\sqrt{5}}2 = 0,\\\\ \\bar{x}_2 &= \\frac{1}{\\sqrt{5}}1 + \\frac{2}{\\sqrt{5}}2 = \\sqrt{5}. \\end{split} We thus see that (1,2) = \\sqrt{5}\\mathsf{g}_2 , a fact that can be easily checked directly. We will now introduce a useful notion called component maps to collect the components v_i of any \\mathsf{v} \\in V with respect to the (not necessarily orthonormal) basis B = (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) of V . Define the component map \\mathsf\\phi_{V,B}:V \\to \\mathbb{R}^n as follows: \\mathsf\\phi_{V,B}(\\mathsf{v}) = [v_1, \\ldots, v_n]^T. Notice how we have collected together the components of \\mathsf{v} as a column vector of size n using the component map. It is useful at this point to introduce the following notation: _{(\\mathsf{e}_i)} = [v_1, \\ldots, v_n]^T When the choice of basis is evident from the context, we will often write [\\mathsf{v}]_{(\\mathsf{e}_i)} as just [\\mathsf{v}] . We can thus alternatively the basis representation of any \\mathsf{v} \\in V with respect to a general basis (\\mathsf{e}_i) of V as follows: \\mathsf{v} = \\sum v_i \\mathsf{e}_i = \\sum \\left(\\mathsf\\phi_{V,B}(\\mathsf{v})\\right)_i \\mathsf{e}_i = \\sum [\\mathsf{v}]_i \\mathsf{e}_i. We will however use the simpler notation \\mathsf{v} = \\sum v_i \\mathsf{e}_i , and use the [\\mathsf{v}] notation only when we want to refer to the components alone as a column vector. We will see shortly that the component map is an example of an isomorphism between the vector spaces V and \\mathbb{R}^n . Remark We will often write the component map \\mathsf\\phi_{V,B} as \\mathsf\\phi_V , or just \\mathsf\\phi , when the vector space and its basis are evident from the context. At times, we will omit \\mathsf\\phi altogether and refer to the component map using the following notation: \\mathsf{v} \\in V \\mapsto [\\mathsf{v}] \\in \\mathbb{R}^n . Example As a quick illustration of this, notice that the vector (1,2) \\in \\mathbb{R}^2 has the following matrix representation with respect to the orthonormal basis (e_1, e_2) , where e_1 = (2/\\sqrt{5}, -1/\\sqrt{5}) and e_2 = (1/\\sqrt{5}, 2/\\sqrt{5}) of \\mathbb{R}^2 is [0,\\sqrt{5}]^T . Change of basis rules for vectors Given an n -dimensional inner product space V , let us first consider the case where (\\mathsf{e}_i) and (\\mathsf{g}_i) are two general bases of V , not necessarily orthonormal. Then, any \\mathsf{v} \\in V can be written as \\mathsf{v} = \\sum v_i \\mathsf{e}_i = \\sum \\tilde{v}_i \\mathsf{g}_i, where (v_i) and (\\tilde{v}_i) are the components of \\mathsf{v} with respect to the bases (\\mathsf{e}_i) and (\\mathsf{g}_i) , respectively. The fact that (\\mathsf{e}_i) is a basis of V implies that \\mathsf{g}_i = \\sum A_{ji} \\mathsf{e}_j, where A_{ij} \\in \\mathbb{R} for every 1 \\le i,j \\le n . Similarly, we have \\mathsf{e}_i = \\sum B_{ji} \\mathsf{g}_j, where B_{ij} \\in \\mathbb{R} for every 1 \\le i,j \\le n . Notice how the first index of the transformation coefficients pairs with the corresponding basis vector. The reason for this specific choice will become clear shortly. Combining these two transformation relations, we see that \\mathsf{g}_i = \\sum A_{ji}B_{kj} \\mathsf{g}_k \\quad\\Rightarrow\\quad \\sum B_{kj}A_{ji} = \\delta_{ki}, and \\mathsf{e}_i = \\sum B_{ji}A_{kj}\\mathsf{e}_k \\quad\\Rightarrow\\quad \\sum A_{kj}B_{ji} = \\delta_{ki}. It is convenient to collect together the constants \\{A_{ij}\\} as a matrix \\mathsf{A} whose (i,j)^{\\text{th}} entry is A_{ij} . We will similarly collect the constants \\{B_{ij}\\} in a matrix \\mathsf{B} . In matrix notation, we can write the foregoing equations succinctly as \\mathsf{A}\\mathsf{B} = \\mathsf{I}, \\quad \\mathsf{B}\\mathsf{A} = \\mathsf{I}, \\quad\\Rightarrow\\quad \\mathsf{A} = \\mathsf{B}^{-1}, where \\mathsf{I} is the identity matrix of order n . We thus see that matrices \\mathsf{A} and \\mathsf{B} are inverses of each other. Given the transformation relations between the two bases, we can use the identity \\sum v_i \\mathsf{e}_i = \\sum \\tilde{v}_i \\mathsf{g}_i to see that \\sum \\tilde{v}_i \\mathsf{g}_i = \\sum v_i B_{ji} \\mathsf{g}_j \\quad\\Rightarrow\\quad \\tilde{v}_i = \\sum B_{ij} v_j = \\sum A^{-1}_{ij} v_j. In matrix notation, we can summarize the foregoing result as follows: \\mathsf{g}_i = \\sum A_{ji} \\mathsf{e}_j \\quad\\Rightarrow\\quad [\\mathsf{v}]_{(\\mathsf{g}_i)} = \\mathsf{A}^{-1}[\\mathsf{v}]_{(\\mathsf{e}_i)}. !!! info \"Remark\" In a more general treatment of linear algebra, the fact that the components of a vector \\mathsf{v} \\in V transform in a manner contrary to that of the manner in which the basis vectors transform is used to call elements of V as contravariant vectors. We will not develop the general theory here, but make a few elementary remarks occasionally regarding this. Let us now consider the special case when both (\\mathsf{e}_i) and (\\mathsf{g}_i) are orthonormal bases of V . In this case, we can use the fact that \\mathsf{e}_i \\cdot \\mathsf{e}_j = \\delta_{ij}, \\qquad \\mathsf{g}_i \\cdot \\mathsf{g}_j = \\delta_{ij}, to simplify the calculations. Suppose that \\mathsf{g}_i = \\sum Q_{ji} \\mathsf{e}_j. It follows immediately that Q_{ji} = \\mathsf{g}_i \\cdot \\mathsf{e}_j. Let us now see how the components of any \\mathsf{v} \\in V transform upon this change of basis: \\mathsf{v} = \\sum \\tilde{v}_i \\mathsf{g}_i = \\sum v_i \\mathsf{e}_i \\quad\\Rightarrow\\quad \\tilde{v}_i = \\sum \\mathsf{e}_j \\cdot \\mathsf{g}_i v_j = \\sum Q_{ji} v_j. In matrix notation, this is written as _{(\\mathsf{g}_i)} = \\mathsf{Q}^T[\\mathsf{v}]_{(\\mathsf{e}_i)}, where \\mathsf{Q} is the matrix whose (i,j)^{\\text{th}} entry is Q_{ij} . But, based on the calculation we carried out earlier in the context of general bases, we see that \\mathsf{g}_i = \\sum Q_{ji} \\mathsf{e}_j \\quad\\Rightarrow\\quad [\\mathsf{v}]_{(\\mathsf{g}_i)} = \\mathsf{Q}^{-1}[\\mathsf{v}]_{(\\mathsf{e}_i)}. Comparing these two expressions, we are led to the following conclusion: \\mathsf{Q}^T = \\mathsf{Q}^{-1}. Recall that matrices that satisfy this condition are called orthogonal matrices. It is an easy consequence of orthogonality that the determinant of an orthogonal matrix is \\pm 1 , as the following calculation shows: if \\mathsf{Q} is an orthogonal matrix (\\text{det}(\\mathsf{Q}))^2 = \\text{det}(\\mathsf{Q}^T\\mathsf{Q}) = \\text{det}(\\mathsf{I}) = 1 \\quad\\Rightarrow\\quad \\text{det}(\\mathsf{Q}) = \\pm 1. If \\text{det}(\\mathsf{Q}) = 1 , then the orthogonal matrix \\mathsf{Q} is called proper orthogonal , or special orthogonal . Remark It is important to note that the foregoing conclusion that the matrix involved in the change of basis is orthogonal is true only in the special case when both bases are orthonormal. Example Consider the orthonormal bases (\\mathsf{e}_1, \\mathsf{e}_2) and (\\mathsf{g}_1, \\mathsf{g}_2) of \\mathbb{R}^2 , where (\\mathsf{e}_1, \\mathsf{e}_2) is the standard basis of \\mathbb{R}^2 and \\mathsf{g}_1 = (2/\\sqrt{5}, -1/\\sqrt{5}) , \\mathsf{g}_2 = (1/\\sqrt{5}, 2/\\sqrt{5}) . The transformation matrix \\mathsf{Q} from (\\mathsf{e}_1,\\mathsf{e}_2) to (\\mathsf{g}_1,\\mathsf{g}_2) is computed using the relation Q_{ij} = \\mathsf{g}_j \\cdot \\mathsf{e}_i as \\mathsf{Q} = \\begin{bmatrix} \\mathsf{g}_1 \\cdot \\mathsf{e}_1 & \\mathsf{g}_2 \\cdot \\mathsf{e}_1\\\\ \\mathsf{g}_1 \\cdot \\mathsf{e}_2 & \\mathsf{g}_2 \\cdot \\mathsf{e}_2 \\end{bmatrix} = \\frac{1}{\\sqrt{5}}\\begin{bmatrix} 2 & 1\\\\ -1 & 2 \\end{bmatrix}. It is easily checked that \\mathsf{Q} is orthogonal: \\mathsf{Q}^T\\mathsf{Q} = \\frac{1}{5}\\begin{bmatrix}2 & -1\\\\ 1 & 2\\end{bmatrix}\\begin{bmatrix}2 & 1\\\\ -1 & 2\\end{bmatrix} = \\begin{bmatrix} 1 & 0\\\\ 0 & 1\\end{bmatrix}. It can be similarly checked that \\mathsf{Q}\\mathsf{Q}^T = \\mathsf{I} . As a quick check, note that also the determinant of the transformation map \\mathsf{Q} in the previous example is 1 : \\text{det}\\left( \\frac{1}{\\sqrt{5}}\\begin{bmatrix} 2 & 1\\\\ -1 & 2 \\end{bmatrix} \\right) = 1. This informs us that the transformation matrix \\mathsf{Q} is in fact special orthogonal. General basis Most of the discussion thus far regarding the representation of a vector in a finite dimensional inner product space has been restricted to the special case of orthonormal bases. Let us briefly consider the general case when a general basis, which is not necessarily orthonormal, is chosen. In what follows, V denotes an inner product space of dimension n , and (\\mathsf{g}_i) is a general basis of V . Representation of vectors Any \\mathsf{v} \\in V can be written in terms of the basis (\\mathsf{g}_1, \\ldots, \\mathsf{g}_n) of V as \\mathsf{v} = \\sum v_i \\mathsf{g}_i, where (v_1, \\ldots, v_n) are the components of \\mathsf{v} with respect to this basis. To compute these components, start with taking the inner product of this equation with the basis vector g_i ; this yields \\mathsf{v} \\cdot \\mathsf{g}_i = \\sum \\mathsf{g}_i \\cdot \\mathsf{g}_j \\, v_j. This equation can be written in the form of a matrix equation, as follows: \\begin{bmatrix} \\mathsf{g}_1 \\cdot \\mathsf{g}_1 & \\ldots & \\mathsf{g}_1 \\cdot \\mathsf{g}_n\\\\ \\vdots & \\ddots & \\vdots\\\\ \\mathsf{g}_n \\cdot \\mathsf{g}_1 & \\ldots & \\mathsf{g}_n \\cdot \\mathsf{g}_n \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} \\mathsf{v} \\cdot \\mathsf{g}_1\\\\ \\vdots\\\\ \\mathsf{v} \\cdot \\mathsf{g}_n \\end{bmatrix}. The fact that (\\mathsf{g}_i) is a basis of V implies that the components v_1,\\ldots,v_n exist and are unique. This implies that the matrix introduced above, whose (i,j)^{\\text{th}} entry is g_{ij} = \\mathsf{g}_i\\cdot\\mathsf{g}_j , is invertible. It is conventional, and convenient, to represent the inverse of this matrix as the matrix with entries g^{ij} ; thus, \\begin{bmatrix} g_{11} & \\ldots & g_{1n}\\\\ \\vdots & \\ddots & \\vdots\\\\ g_{n1} & \\ldots & g_{nn} \\end{bmatrix}^{-1} = \\begin{bmatrix} g^{11} & \\ldots & g^{1n}\\\\ \\vdots & \\ddots & \\vdots\\\\ g^{n1} & \\ldots & g^{nn} \\end{bmatrix}. A proper justification for this choice of notation will be given shortly when we study reciprocal bases . The fact that these two matrices are inverses of each other can be written succinctly as follows: \\sum g^{ik}g_{kj} = \\delta_{ij} = \\sum g_{ik}g^{kj}. Using this result, a trite calculation yields the following result: for any \\mathsf{v} \\in V , \\mathsf{v} = \\sum v_i \\mathsf{g}_i \\quad\\Rightarrow\\quad v_i = \\sum g^{ij}\\mathsf{g}_j \\cdot \\mathsf{v}. The components of any vector with respect to a general basis can thus be computed explicitly. Example Consider the basis (\\mathsf{g}_1,\\mathsf{g}_2) of \\mathbb{R}^2 , where \\mathsf{g}_1 = (1,2) and \\mathsf{g}_2 = (2,3) . Let us now compute the components of \\mathsf{v} = (2,5) \\in \\mathbb{R}^2 with respect to this basis. The first step in to compute the matrix whose entries are g_{ij} : \\begin{bmatrix} \\mathsf{g}_1 \\cdot \\mathsf{g}_1 & \\mathsf{g}_1 \\cdot \\mathsf{g}_2\\\\ \\mathsf{g}_2 \\cdot \\mathsf{g}_1 & \\mathsf{g}_2 \\cdot \\mathsf{g}_2 \\end{bmatrix} = \\begin{bmatrix} 5 & 8\\\\ 8 & 13 \\end{bmatrix}. Notice that this matrix is symmetric, as expected, since g_{ij} = g_{ji} , in general. The inverse of this matrix gives the scalars (g^{ij}) as follows: \\begin{bmatrix} g^{11} & g^{12}\\\\ g^{21} & g^{22} \\end{bmatrix} = \\begin{bmatrix} 5 & 8\\\\ 8 & 13 \\end{bmatrix}^{-1} = \\begin{bmatrix} 13 & -8\\\\ -8 & 5 \\end{bmatrix}. The components of \\mathsf{v} with respect to the basis (\\mathsf{g}_i) are now easily computed using the result v_i = \\sum g^{ij} \\mathsf{g}_j \\cdot \\mathsf{v} as, \\begin{split} v_1 &= \\sum g^{1j} \\mathsf{g}_j \\cdot \\mathsf{v} = g^{11} \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 5\\end{bmatrix} + g^{12} \\begin{bmatrix} 2 \\\\ 3\\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 5\\end{bmatrix} = 4,\\\\ v_2 &= \\sum g^{2j} \\mathsf{g}_j \\cdot \\mathsf{v} = g^{21} \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 5\\end{bmatrix} + g^{22} \\begin{bmatrix} 2 \\\\ 3\\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 5\\end{bmatrix} = -1. \\end{split} We thus see that \\mathsf{v} = 4\\mathsf{g}_1 - \\mathsf{g}_2 . As a consistency check, substitute the representations of \\mathsf{v}, \\mathsf{g}_1,\\mathsf{g}_2 with respect to the standard basis of \\mathbb{R}^2 and verify that this is correct. Reciprocal basis The computations presented in the previous section can be greatly simplified by introducing the reciprocal basis corresponding to a given basis. Given a basis (\\mathsf{g}_1, \\ldots, \\mathsf{g}_n) of V , its reciprocal basis is defined as the basis (\\mathsf{g}^1, \\ldots, \\mathsf{g}^n) such that \\mathsf{g}^i \\cdot \\mathsf{g}_j = \\delta_{ij}, where 1 \\le i,j \\le n . Based on the preceding development, it can be seen that the reciprocal basis is explicitly given by the following equations: \\mathsf{g}^i = \\sum g^{ij}\\mathsf{g}_j. This can be readily inverted to yield the following equation: \\mathsf{g}_i = \\sum g_{ij} \\mathsf{g}^j. Note that in the special case of the standard basis (\\mathsf{e}_i) of \\mathbb{R}^3 , \\mathsf{e}^i = \\mathsf{e}_i . More generally, if (\\mathsf{g}_i) is an orthonormal basis of an inner product space V , then \\mathsf{g}^i = \\mathsf{g}_i . This is one of the reasons why many calculations are much simpler when using orthonormal bases. It follows from the definition of the reciprocal basis (\\mathsf{g}^i) of V that \\begin{split} \\mathsf{g}^i \\cdot \\mathsf{g}^j &= \\left(\\sum g^{ik}\\mathsf{g}_k\\right)\\left(\\sum g^{jl}\\mathsf{g}_l\\right)\\\\ &= \\sum g^{ik}g^{jl}g_{kl} = \\sum \\delta_{il}g^{jl}\\\\ &= g^{ij}. \\end{split} Thus, the following useful formulate are obtained: if (\\mathsf{g}_i) is a general basis of V and (\\mathsf{g}^i) is its reciprocal basis, then \\mathsf{g}_i \\cdot \\mathsf{g}_j = g_{ij} and \\mathsf{g}^i \\cdot \\mathsf{g}^j = g^{ij} . Remark The use of superscripts here is done purely for notational convenience. It is however possible to justify such a notation when considering a more detailed treatment of this subject, as will be briefly noted later. Example Consider the previous example involving the basis (\\mathsf{g}_1,\\mathsf{g}_2) of \\mathbb{R}^2 , where \\mathsf{g}_1 = (1,2) and \\mathsf{g}_2 = (2,3) . In this case, the reciprocal basis (\\mathsf{g}^1, \\mathsf{g}^2) is computed as follows: \\begin{split} \\mathsf{g}^1 &= \\sum g^{1j}\\mathsf{g}_j = 13\\cdot(1,2) - 8\\cdot(2,3) = (-3,2),\\\\ \\mathsf{g}^2 &= \\sum g^{2j}\\mathsf{g}_j = -8\\cdot(1,2) + 5\\cdot(2,3) = (2,-1). \\end{split} It can be checked with a simple calculation that \\mathsf{g}^i \\cdot \\mathsf{g}_j = \\delta_{ij} , as expected. Further more, the matrix whose (i,j)^{\\text{th}} entry is \\mathsf{g}^i \\cdot \\mathsf{g}^j is computed as \\begin{bmatrix} \\mathsf{g}^1 \\cdot \\mathsf{g}^1 & \\mathsf{g}^1 \\cdot \\mathsf{g}^2\\\\ \\mathsf{g}^2 \\cdot \\mathsf{g}^1 & \\mathsf{g}^2 \\cdot \\mathsf{g}^2 \\end{bmatrix} = \\begin{bmatrix} 13 & -8\\\\ -8 & 5 \\end{bmatrix} = \\begin{bmatrix} 5 & 8\\\\ 8 & 13 \\end{bmatrix}^{-1} = \\begin{bmatrix} \\mathsf{g}_1 \\cdot \\mathsf{g}_1 & \\mathsf{g}_1 \\cdot \\mathsf{g}_2\\\\ \\mathsf{g}_2 \\cdot \\mathsf{g}_1 & \\mathsf{g}_2 \\cdot \\mathsf{g}_2 \\end{bmatrix}^{-1}. This confirms that the matrix whose (i,j)^{\\text{th}} entry is \\mathsf{g}_i \\cdot \\mathsf{g}_j is indeed the inverse of the matrix whose (i,j)^{\\text{th}} entry is \\mathsf{g}^i \\cdot \\mathsf{g}^j . The expressions for the coefficients of a vector with respect to a given basis simple form when expressed in terms of the reciprocal basis. Given any \\mathsf{v} \\in V and a basis (\\mathsf{g}_i) of V , \\mathsf{v} = \\sum \\tilde{v}_i \\mathsf{g}_i \\quad\\Rightarrow\\quad \\tilde{v}_i = \\mathsf{v} \\cdot \\mathsf{g}^i. Thus, any \\mathsf{v} \\in V has the compact representation \\mathsf{v} = \\sum (\\mathsf{v} \\cdot \\mathsf{g}^i) \\mathsf{g}_i. Compare this with the representation \\mathsf{v} = \\sum (\\mathsf{v} \\cdot \\mathsf{e}_i) \\mathsf{e}_i of \\mathsf{v} with respect to an orthonormal basis (\\mathsf{e}_i) of V . Remark Given any \\mathsf{v} \\in V and a basis (\\mathsf{g}_i) of V , the components of \\mathsf{v} with respect to the (\\mathsf{g}_i) and its reciprocal basis (\\mathsf{g}^i) are written as follows: \\mathsf{v} = \\sum v_i \\mathsf{g}_i = \\sum v^*_i \\mathsf{g}^i. The components (v_i) and (v^*_i) are called the contravariant and covariant components of \\mathsf{v} , respectively. In many textbooks, the following alternative notation is used: \\mathsf{v} = \\sum v^i \\mathsf{g}_i = \\sum v_i \\mathsf{g}^i. The components v_i and v^i are related as follows: v^i = g^{ij}v_j and v_i = g_{ij}v^j . For this reason, g^{ij} and g_{ij} are said to raise and lower, respectively, indices. Since we will largely restrict ourselves to the case of orthonormal bases and rarely represent a vector in terms of the reciprocal basis to a given basis, we will not adopt this more nuanced notation here. Change of basis rules The ideas presented so far can be used to express a given vector \\mathsf{v} \\in V with respect to different bases. Suppose that \\mathsf{v} has the following representations, with respect to two different bases (\\mathsf{f}_i) and (\\mathsf{g}_i) of V : \\mathsf{v} = \\sum \\bar{v}_i\\mathsf{f}_i = \\sum \\tilde{v}_i \\mathsf{g}_i. Taking the inner product of these representations with respect to the appropriate reciprocal basis vectors, it is evident that \\mathsf{v} = \\sum \\bar{v}_i \\mathsf{f}_i = \\sum \\tilde{v}_i \\mathsf{g}_i \\quad\\Rightarrow\\quad \\bar{v}_i = \\sum \\mathsf{f}^i\\cdot\\mathsf{g}_j \\tilde{v}_j, and a similar formula expressing (\\tilde{v}_i) in terms of (\\bar{v}_i) . Notice how the use of the reciprocal basis significantly simplifies the computations. Example Consider the example considered earlier where the vector (2,5) \\in \\mathbb{R}^2 was expressed in terms of the basis (\\mathsf{g}_1,\\mathsf{g}_2) of \\mathbb{R}^2 , where \\mathsf{g}_1 = (1,2) and \\mathsf{g}_2 = (2,3) . We saw earlier that \\mathsf{v} = 4\\mathsf{g}_1 - \\mathsf{g}_2. Let us now consider another basis (\\mathsf{f}_1,\\mathsf{f}_2) of \\mathbb{R}^2 , where \\mathsf{f}_1 = (2,1) , \\mathsf{f}_2 = (1,3) . To compute the representation of \\mathsf{v} with respect to the basis (\\mathsf{f}_1,\\mathsf{f}_2) , we first need to compute its reciprocal basis (\\mathsf{f}^1,\\mathsf{f}^2) . This is easily accomplished as follows: \\begin{bmatrix} \\mathsf{f}_1 \\cdot \\mathsf{f}_1 & \\mathsf{f}_1 \\cdot \\mathsf{f}_2\\\\ \\mathsf{f}_2 \\cdot \\mathsf{f}_1 & \\mathsf{f}_2 \\cdot \\mathsf{f}_2 \\end{bmatrix} = \\begin{bmatrix} 5 & 8\\\\ 8 & 13 \\end{bmatrix} \\quad\\Rightarrow\\quad \\begin{bmatrix} f^{11} & f^{12}\\\\ f^{21} & f^{22} \\end{bmatrix} = \\frac{1}{5} \\begin{bmatrix} 2 & -1\\\\ -1 & 1 \\end{bmatrix} The reciprocal basis is computed using the relations \\mathsf{f}^i = \\sum f^{ij}\\mathsf{f}_j : \\begin{split} \\mathsf{f}^1 &= \\sum f^{1j}\\mathsf{f}_j = \\frac{2}{5}(2,1) - \\frac{1}{5}(1,3) = \\frac{1}{5}(3,-1),\\\\ \\mathsf{f}^2 &= \\sum f^{2j}\\mathsf{f}_j = -\\frac{1}{5}(2,1) + \\frac{1}{5}(1,3) = \\frac{1}{5}(-1,2). \\end{split} It is left as an easy exercise to verify that \\mathsf{f}^i \\cdot \\mathsf{f}_j = \\delta_{ij} . Using these relations the components (\\bar{v}_1,\\bar{v}_2) of \\mathsf{v} with respect to the basis (\\mathsf{f}_1,\\mathsf{f}_2) can be computed using the relations \\bar{v}_i = \\mathsf{v} \\cdot \\mathsf{f}^i as follows: \\begin{split} \\bar{v}_1 &= \\mathsf{v} \\cdot \\mathsf{f}^1 = (2,5) \\cdot \\frac{1}{5}(3,-1) = \\frac{1}{5},\\\\ \\bar{v}_2 &= \\mathsf{v} \\cdot \\mathsf{f}^2 = (2,5) \\cdot \\frac{1}{5}(-1,2) = \\frac{8}{5}. \\end{split} We thus see get the representation of \\mathsf{v} in the basis (\\mathsf{f}_1,\\mathsf{f}_2) as \\mathsf{v} = \\frac{1}{5}(\\mathsf{f}_1 + 8\\mathsf{f}_2). It is left as a simple exercise to verify by direct substitution that this is true. Finally, note that the components (\\bar{v}_1,\\bar{v}_2) of \\mathsf{v} with respect to the basis (\\mathsf{f}_1,\\mathsf{f}_2) can be directly obtained from its components (\\tilde{v}_1,\\tilde{v}_2) with respect to the basis (\\mathsf{g}_1,\\mathsf{g}_2) using the relations \\bar{v}_i = \\sum \\mathsf{f}^i \\cdot \\mathsf{g}_j \\tilde{v}_j as follows: \\begin{split} \\bar{v}_1 &= \\sum \\mathsf{f}^1\\cdot\\mathsf{g}_j \\tilde{v}_j = \\frac{4}{5}(3,-1)\\cdot(1,2) - \\frac{1}{5}(3,-1)\\cdot(2,3) = \\frac{1}{5},\\\\ \\bar{v}_2 &= \\sum \\mathsf{f}^2\\cdot\\mathsf{g}_j \\tilde{v}_j = \\frac{4}{5}(-1,2)\\cdot(1,2) - \\frac{1}{5}(-1,2)\\cdot(2,3) = \\frac{8}{5}. \\end{split} We thus see that all the different ways to compute the components of \\mathsf{v} with respect to two different choices of bases are consistent with each other.","title":"Inner Product Spaces"},{"location":"inner_product_spaces/#introduction","text":"A vector is typically introduced in high school algebra as a quantity with both a magnitude and a direction. A representation of a vector in the familiar three dimensional Euclidean space is shown in the following figure: Representation of a vector in \\mathbb{R}^3 If \\mathsf{i},\\mathsf{j}, \\mathsf{k} represent the unit vectors along the x,y,z axes, respectively, a vector \\mathsf{v} can be expressed uniquely as \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} , where v_x, v_y, v_z are the Cartesian components of the vector \\mathsf{v} (with respect to the basis vectors \\mathsf{i},\\mathsf{j} and \\mathsf{k} ). There are two core operations associated with vectors: Vector addition : Given two vectors \\mathsf{u} = u_x \\mathsf{i} + u_y \\mathsf{j} + u_z \\mathsf{k} and \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} , we can add them to get a new vector \\mathsf{u} + \\mathsf{v} , defined as \\mathsf{u} + \\mathsf{v} = (u_x + v_x) \\mathsf{i} + (u_y + v_y) \\mathsf{j} + (u_z + v_z) \\mathsf{k}. Geometrically, the new vector \\mathsf{u} + \\mathsf{v} is obtained by placing the tail of \\mathsf{v} at the head of \\mathsf{u} . The sum of the two vectors is then the vector which shares it\u2019s tail with \\mathsf{u} and head with \\mathsf{v} , as shown below: Vector addition in \\mathbb{R}^3 Note that we get the same vector independent of the order of addition: \\mathsf{u} + \\mathsf{v} = \\mathsf{v} + \\mathsf{u} . For this reason, vector addition is said to be commutative . It can also be shown easily that if \\mathsf{u}, \\mathsf{v}, \\mathsf{w} are three vectors, then (\\mathsf{u} + \\mathsf{v}) + \\mathsf{w} = \\mathsf{u} + (\\mathsf{v} + \\mathsf{w}) . This property is called associativity . Scalar multiplication of a vector with a real number: Given any vector \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} , we can multiply it by some real number a to get the new vector that is a times as long as \\mathsf{v} : a\\mathsf{v}= av_x \\mathsf{i} + av_y \\mathsf{j} + av_z \\mathsf{k} . This is illustrated in the following figure: Scalar multiplication of a vector in \\mathbb{R}^3 Note that we will need only real vector spaces in what follows. Some vector spaces admit an algebraic operation called the inner product . For instance, in three dimensional space, given two vectors \\mathsf{u} = u_x \\mathsf{i} + u_y \\mathsf{j} + u_z \\mathsf{k} and \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} , we can combine them using the dot product to produce a real number \\mathsf{u} \\cdot \\mathsf{v} = u_x v_x + u_y v_y + u_z v_z . Using the dot product, it is customary to define the length or Euclidean norm of a vector \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} as the (non-negative) real number \\lVert \\mathsf{v} \\rVert = (\\mathsf{v} \\cdot \\mathsf{v})^{\\frac{1}{2}} = (v_x^2 + v_y^2 + v_z^2)^{\\frac{1}{2}} . For vectors in three dimensional space (only), we also an additional important algebraic operation called the cross product : we can combine two vectors \\mathsf{u} = u_x \\mathsf{i} + u_y \\mathsf{j} + u_z \\mathsf{k} and \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} using the cross product to obtain a new vector \\mathsf{u} \\times \\mathsf{v} = (u_y v_z - u_z v_y) \\mathsf{i} + (u_z v_x - u_x v_z) \\mathsf{j} + (u_x v_y - u_y v_x) \\mathsf{k} . In what follows, we will first focus on just vector addition and scalar multiplication. These two operations embody a concept known as linearity , which is fundamental to appreciate what a vector is. We will then study inner product spaces , which are vector spaces with an additional structure known as an inner product , and highlight Euclidean spaces as an important example of inner product spaces. Remark The generalization of the cross product is known as the wedge product . We will not study the wedge product in detail in these notes since it is beyond the scope of these notes. Notice that all the information about the vector \\mathsf{v} = v_x \\mathsf{i} + v_y \\mathsf{j} + v_z \\mathsf{k} is contained in the ordered set of three real numbers (v_x, v_y, v_z) . What this means is that given any vector \\mathsf{v} in three dimensional space, we can uniquely associate with it a triple of real numbers, and vice versa. The set of all such ordered triples is the set \\mathbb{R}^3 , which is the set of all triples of real numbers. If we define addition of two such ordered triples and multiplication of an ordered triple with a real number as, \\begin{gathered} (u_x, u_y, u_z) + (v_x, v_y, v_z) = (u_x + v_x, u_y + v_y, u_z + v_z), \\nonumber\\\\ c(u_x, u_y, u_z) = (cu_x, cu_y, cu_z), \\nonumber\\end{gathered} then the elements of \\mathbb{R}^3 , which are ordered triples of real numbers, behave exactly as the geometric picture of vectors as arrows that we just discussed, as far as the core properties of vector addition and scalar multiplication are concerned. We now have two ways of representing a vector: as an arrow in three dimensional space, and as a set of ordered triple of real numbers. Both of these represent the same object. But the representation in terms of ordered tuples of real numbers immediately admits a generalization to cases where the pictorial representation fails. It is evident from the above discussion that there is nothing special about the number 3 when we considered an ordered triple of real numbers. We can easily generalize this to a set of ordered n -tuple of real numbers (u_1,u_2,\\ldots,u_n) \\in \\mathbb{R}^n , where n \\in \\mathbb{N} could be any arbitrary positive integer. We can define addition and scalar multiplication in \\mathbb{R}^n analogous to the case of the ordered triples, \\begin{gathered} (u_1, u_2, \\ldots, u_n) + (v_1, v_2, \\ldots, v_n) = (u_1 + v_1, u_2 + v_2, \\ldots, u_n + v_n), \\label{eq:linearity_Rn_add}\\\\ a(u_1, u_2, \\ldots, u_n) = (au_1, au_2, \\ldots, au_n),\\label{eq:linearity_Rn_sm}\\end{gathered} where (u_1,u_2,\\ldots,u_n) and (v_1,v_2,\\ldots,v_n) are two elements of \\mathbb{R}^n , and a is a real number. We will call this the standard linear structure on \\mathbb{R}^n , and elements of \\mathbb{R}^n as vectors in \\mathbb{R}^n . Note that there is no obvious way to picture an arrow in the n -dimensional space \\mathbb{R}^n for n > 3 . Thus, by choosing the right representation, we can extend the elementary notion of vectors as quantities with magnitude and direction to more general objects.","title":"Introduction"},{"location":"inner_product_spaces/#linear-structure","text":"Let us now generalize the previous discussion to define abstract vector spaces . Suppose that V is a set such that it is possible to define two maps \\begin{split} +:V \\times V \\to V; & \\quad +(\\mathsf{u},\\mathsf{v}) \\mapsto \\mathsf{u} + \\mathsf{v},\\\\ \\cdot:V \\times V \\to V; & \\quad \\cdot(a, \\mathsf{u}) \\mapsto a\\mathsf{u}, \\end{split} called vector addition and scalar multiplication , respectively, in V , that satisfy the following properties: for any \\mathsf{u}, \\mathsf{v}, \\mathsf{w} \\in V and a,b \\in \\mathbb{R} , Associativity of addition: \\mathsf{u} + (\\mathsf{v} + \\mathsf{w}) = (\\mathsf{u} + \\mathsf{v}) + \\mathsf{w} , Commutativity of addition: \\mathsf{u} + \\mathsf{v} = \\mathsf{v} + \\mathsf{u} , Existence of additive identity : there exists a unique element \\mathsf{0} \\in V , called the additive identity of V such that \\mathsf{u} + \\mathsf{0} = \\mathsf{u} , Existence of additive inverse : for every \\mathsf{u} \\in V , there exists a unique element -\\mathsf{u} \\in V such that \\mathsf{u} + (-\\mathsf{u}) = \\mathsf{0} , Distributivity of scalar multiplication over vector addition: a(\\mathsf{u} + \\mathsf{v}) = a\\mathsf{u} + a\\mathsf{v} , Distributivity of scalar multiplication over scalar addition: (a + b)\\mathsf{u} = a\\mathsf{u} + b\\mathsf{u} , Compatibility of scalar multiplication with field multiplication: a(b\\mathsf{u}) = (ab)\\mathsf{u} , Scaling property of scalar multiplication: 1\\mathsf{u} = \\mathsf{u} . A set V that has two maps that satisfy these axioms is called a (real) vector space , or a (real) linear space . What we have accomplished through these axioms is to endow a set with a notion of addition that allows us to add two elements of the set to get a third element. We have also provided a mechanism to multiply a member of this set by a real number to get another element of this set. The maps + and \\cdot are said to provide a linear structure on V . Elements of V are called vectors . Remark Some textbooks mention additional closure axioms that indicate that if \\mathsf{u}, \\mathsf{v} \\in V , then \\mathsf{u} + \\mathsf{v} \\in V , and given any \\mathsf{u} \\in V and a \\in \\mathbb{R} , a \\mathsf{u} \\in V . We don\u2019t specify this explicitly since this is already implied by the function definitions +:V \\times V \\to V and \\cdot:\\mathbb{R} \\times V \\to V . As mentioned in the previous discussion on set theory, we will always insist on mentioning the domain and codomain of every map/function we encounter. Hence, the so-called closure axioms are redundant for our purposes. Remark Following standard convention, we will often use the shorthand notation \\mathsf{u} - \\mathsf{v} for \\mathsf{u} + (-\\mathsf{v}) . Example The simplest example of a real vector space is the set of real numbers \\mathbb{R} with addition and multiplication defined in the standard manner. More generally, consider the set \\mathbb{R}^n consisting of all n -tuples of real numbers: \\mathbb{R}^n = \\{(u_1, \\ldots, u_n)\\,|\\,\\forall\\,i=1,\\ldots,n, \\; u_i \\in \\mathbb{R}\\}. Given (u_1, \\ldots, u_n) \\in \\mathbb{R}^n , (v_1, \\ldots, v_n) \\in \\mathbb{R}^n , and a \\in \\mathbb{R} , let us define addition +:\\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}^n and scalar multiplication \\cdot:\\mathbb{R}\\times \\mathbb{R}^n \\to \\mathbb{R}^n as \\begin{split} (u_1, \\ldots, u_n) + (v_1, \\ldots, v_n) &= (u_1 + v_1, \\ldots, u_n + v_n),\\\\ a \\cdot (u_1, \\ldots, u_n) &= (au_1, \\ldots, au_n). \\end{split} It is straightforward to verify that with addition and scalar multiplication thus defined, the triple (\\mathbb{R}^n,+,\\cdot) is a real vector space. Note that the additive identity in \\mathbb{R}^n is the zero vector (0, \\ldots, 0) \\in \\mathbb{R}^n , and the additive inverse of (u_1, \\ldots, u_n) \\in \\mathbb{R}^n is (-u_1, \\ldots, -u_n) \\in \\mathbb{R}^n . Example Consider the set \\mathbb{R}^{m \\times n} of all m \\times n matrices with real entries and defined addition of matrices and scalar mutliplication of a matrix with a real number in the usual sense (see Appendix ). It is easily checked that the set of all m \\times n matrices is a vector space. The zero vector in \\mathbb{R}^{m\\times n} is the matrix with zero in all of its entries, and the additive inverse of a given matrix is just its negative. Example The definition of vector spaces admits more general kinds of objects. As a simple example, consider the set C^0(\\mathbb{R},\\mathbb{R}) consisting of all real-valued and continuous functions of one real variable. Given any f, g \\in C^0(\\mathbb{R},\\mathbb{R}) , and any a \\in \\mathbb{R} , we can define addition and scalar multiplication pointwise as follows: for any x \\in \\mathbb{R} , \\begin{split} (f + g)(x) &= f(x) + g(x),\\\\ (a\\cdot f)(x) &= a \\, f(x). \\end{split} It is not difficult to verify that (C^0(\\mathbb{R},\\mathbb{R}), +, \\cdot) is a real vector space. The additive identity in C^0(\\mathbb{R},\\mathbb{R}) is the zero function 0 \\in C^0(\\mathbb{R},\\mathbb{R}) defined as follows: for any x \\in \\mathbb{R} , 0(x) = 0 . The additive inverse of f \\in C^0(\\mathbb{R},\\mathbb{R}) is the function -f \\in C^0(\\mathbb{R},\\mathbb{R}) defined as follows: for any x \\in \\mathbb{R} , (-f)(x) = -f(x) .","title":"Linear structure"},{"location":"inner_product_spaces/#subspaces-and-linear-independence","text":"A subset U \\subseteq V of a vector space V is said to be a linear subspace of V if U is also a vector space. Note that it is implicitly assumed that both U and V share the operations of vector addition and scalar multiplication. It can be easily checked that if a subset U \\subseteq V of a real vector space V has the property that for any \\mathsf{u}, \\mathsf{v} \\in U , and any a,b \\in \\mathbb{R} , (a\\mathsf{u} + b\\mathsf{v}) \\in U , then U is a linear subspace of V . This property is often used to check if a given subset of a vector space is a linear subspace. An immediate consequence of this is the fact that every linear subspace of a given vector space must contain the additive identity \\mathsf{0} \\in V . Example Consider the following subsets of \\mathbb{R}^3 : \\begin{split} S_1 &= \\{(x,y,z) \\in \\mathbb{R}^3 \\,|\\, x + y + z = 0\\},\\\\ S_2 &= \\{(x,y,z) \\in \\mathbb{R}^3 \\,|\\, x + y + z = 1\\},\\\\ S_3 &= \\{(x,y,z) \\in \\mathbb{R}^3 \\,|\\, y = z = 0\\}. \\end{split} To see that S_1 is a linear subspace of \\mathbb{R}^3 , note that if (x_1, y_1, z_1) \\in S_1 , (x_2, y_2, z_2) \\in S_1 , and a, b \\in \\mathbb{R} , then a(x_1, y_1, z_1) + b(x_2, y_2, z_2) = ((ax_1 + bx_2), (ay_1 + by_2), (az_1 + bz_2)) \\in \\mathbb{R}^3 . Since (ax_1 + bx_2) + (ay_1 + by_2) + (az_1 + bz_2) = a(x_1 + y_1 + z_1) + b(x_2 + y_2 + z_2) = (0,0,0) , we see that ((ax_1 + bx_2), (ay_1 + by_2), (az_1 + bz_2)) \\in S_1 . This shows that S_1 is indeed a linear subspace of \\mathbb{R}^3 . It is likewise verified that S_3 is also a linear subspace \\mathbb{R}^3 , while S_2 is not. The intersection of two linear subspaces is also a linear subspace. Moreover, any finite intersection of linear subspaces of a vector space V is also a linear subspace of V , as can be easily checked. Example In the previous example, the intersection of the subspaces S_1 = \\{(x,y,z) \\in \\mathbb{R}^3 \\,|\\, x + y + z = 0\\} and S_3 = \\{(x,y,z) \\in \\mathbb{R}^3 \\,|\\, y = z = 0\\} of \\mathbb{R}^3 is easily seen to be S_1 \\cap S_3 = \\{(0,0,0)\\}, which is the trivial subspace of \\mathbb{R}^3 . Two non-zero vectors \\mathsf{u},\\mathsf{v} \\in V in a vector space V are said to be linearly independent iff a\\mathsf{u} + b\\mathsf{v} = 0 , where a,b are real numbers, implies that a=0 and b=0 . Thus \\mathsf{u} and \\mathsf{v} are linearly independent iff the only linear combination of \\mathsf{u} and \\mathsf{v} that yields \\mathsf{0} is the trivial linear combination 0\\mathsf{u} + 0\\mathsf{v} . If this is not true, then \\mathsf{u} and \\mathsf{v} are said to be linearly dependent . This definition can be easily extended to a finite set of non-zero vectors \\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n , where n \\in \\mathbb{N} . Thus, the set of vectors \\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n in V is said to be linearly independent iff the only real numbers a_1, a_2, \\ldots, a_n that satisfy the equation \\sum_{i=1}^n a_i \\mathsf{v}_i = 0, are a_1 = a_2 = \\ldots = a_n = 0 . Example Consider the vectors \\mathsf{v}_1 = (1,2) \\in \\mathbb{R}^2 and \\mathsf{v}_2 = (2,3)\\in\\mathbb{R}^2 in the two dimensional Euclidean space \\mathbb{R}^2 . For real numbers a, b \\in \\mathbb{R} , note that a\\mathsf{v}_1 + b\\mathsf{v}_2 = 0 \\quad\\Rightarrow\\quad (a + 2b, 2a + 3b) = (0,0). Solving the equations a + 2b = 0 and 2a + 3b = 0 , we immediately see that a = b = 0 , which shows that \\mathsf{v}_1 and \\mathsf{v}_2 are linearly independent vectors in \\mathbb{R}^2 . If \\mathsf{v}_3 = (2,4) \\in \\mathbb{R}^2 , then \\mathsf{v}_1 and \\mathsf{v}_3 are linearly dependent since if for a, b \\in \\mathbb{R} , a\\mathsf{v}_1 + b\\mathsf{v}_3 = 0 \\quad\\Rightarrow\\quad (a + 2b, 2a + 4b) = (0,0). These equations do not imply that a = b = 0 . For instance, a = -2, b = 1 satisfies the condition. We thus see that \\mathsf{v}_1 and \\mathsf{v}_3 are linearly dependent.","title":"Subspaces and linear independence"},{"location":"inner_product_spaces/#basis-of-a-vector-space","text":"We will now introduce a very important tool called the basis of a vector space. The basic idea is that once we identify a basis for a vector space, we can use the linear structure inherent in the space to reduce all computations related to the vector space as a whole, to just computations on the basis set. We will need a few definitions first in order to define the basis. The linear span of a set of vectors \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_m\\} in V , written as \\text{span}(\\{\\mathsf{v}_1,\\ldots,\\mathsf{v}_m\\}) , is defined as the set of all it\u2019s linear combinations : \\text{span}(\\{\\mathsf{v}_1,\\ldots,\\mathsf{v}_m\\}) = \\left\\{\\sum_{i=1}^m a_i \\mathsf{v}_i \\,\\bigg\\vert\\, \\forall\\,1\\le i\\le m,\\,a_i \\in \\mathbb{R}\\right\\}. It is also common to refer to the linear span as just the span . It is straightforward to check that the linear span of any finite collection of vectors in a vector space V is a linear subspace of the vector space V . Example Let us consider the vectors \\mathsf{v}_1 = (1,2,3) \\in \\mathbb{R}^3 and \\mathsf{v}_2 = (2,3,4) \\in \\mathbb{R}^3 in \\mathbb{R}^3 . The span of these two vectors is the following subset of \\mathbb{R}^3 : \\text{span}(\\{\\mathsf{v}_1, \\mathsf{v}_2\\}) = \\{((a + 2b, 2a + 3b, 3a + 4b) \\in \\mathbb{R}^3 \\,|\\, a, b \\in \\mathbb{R}\\}. It is left as an easy exercise to verify that this is indeed a linear subspace of \\mathbb{R}^3 . An ordered subset S \\subseteq V a vector space V is said to constitute a basis of V if S is linearly independent, and \\text{span}(S) = V . In the special case when a vector space V is spanned by a finite ordered set of vectors (\\mathsf{v}_1, \\ldots, \\mathsf{v}_n) \\subseteq V , for some n \\in \\mathbb{N} , the vector space V is said to be finite dimensional , and the number n is called the dimension of the vector space V (written \\text{dim}(V) ). A vector space that is not finite dimensional is said to be infinite dimensional . In what follows, we will only deal with finite dimensional vector spaces. If V is an n -dimensional vector space, and (\\mathsf{v}_1, \\ldots, \\mathsf{v}_n) is a basis for V , then we will often abbreviate the basis as (\\mathsf{v}_i)_{i=1}^n , or just (\\mathsf{v}_i) , when the dimension n is evident from the context. Example Let us revisit the n -dimensional Euclidean space \\mathbb{R}^n , and consider the following vectors: for any 1 \\le i \\le n , \\mathsf{e}_i = (0, \\ldots, \\underbrace{1}_{i^{\\text{th}}\\text{ position}}, \\ldots 0). It is easy to check that the (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) is a basis of \\mathbb{R}^n . In particular, note that any (u_1, \\ldots, u_n) \\in \\mathbb{R}^n can be written as (u_1, \\ldots, u_n) = u_1(1, 0, \\ldots, 0) + \\ldots + u_n(0, \\ldots, 0, 1) = \\sum_{i=1}^n u_i \\mathsf{e}_i. The ordered set (\\mathsf{e}_i)_{i=1}^n is called the standard basis of \\mathbb{R}^n . Note that in the special case of \\mathbb{R}^3 , the set of basis vectors (\\mathsf{e}_1,\\mathsf{e}_2,\\mathsf{e}_3) is identical to the basis set (\\mathsf{i},\\mathsf{j},\\mathsf{k}) introduced in the beginning of this section. Given a vector space V of dimension n , it is possible to choose an infinite number of bases for V . This non-uniqueness in the choice of the basis can be easily understood as follows. Pick any \\mathsf{g}_1 \\in V . Choose \\mathsf{g}_2 from the set V \\setminus \\text{span}({\\mathsf{g}_1}) , \\mathsf{g}_3 from the set V \\setminus \\text{span}({\\mathsf{g}_1, \\mathsf{g}_2}) , and so on. This process will terminate in n steps since the vector space V is of dimension n . The resulting set of vectors (\\mathsf{g}_1, \\ldots, \\mathsf{g}_n) is a basis for V .","title":"Basis of a vector space"},{"location":"inner_product_spaces/#inner-products-and-norms","text":"We will now introduce a special additional structure on an abstract vector space V called the inner product . There are two reasons for introducing the inner product right at the outset: first, the mathematical development becomes significantly simpler, and second, many important applications in science and engineering can be studied in this setting. Remark There is an elegant theory of abstract linear spaces, both in the finite and infinite dimensional cases, where inner products are not defined. We will however not develop this general theory here. Given a vector space V , an inner product on V is defined as a map of the form g:V \\times V \\to \\mathbb{R} such that, for any \\mathsf{u}, \\mathsf{v}, \\mathsf{w} \\in V and a,b \\in \\mathbb{R} , Symmetry : g(\\mathsf{u},\\mathsf{v}) = g(\\mathsf{v},\\mathsf{u}) , Bilinearity : g(\\mathsf{u},(a \\mathsf{v} + b \\mathsf{w})) = a g(\\mathsf{u},\\mathsf{v}) + b g(\\mathsf{u},\\mathsf{w}) , Positive definiteness : g(\\mathsf{u},\\mathsf{u}) \\ge 0 , and g(\\mathsf{u},\\mathsf{u}) = 0 iff \\mathsf{u} = \\mathsf{0} . A vector space V endowed with a map \\cdot:V \\times V \\to \\mathbb{R} that satisfies the three properties mentioned above is said to be an inner product space . All vector spaces considered henceforth will be assumed to be inner product spaces, unless stated otherwise. Remark Given \\mathsf{u}, \\mathsf{v} \\in V , we will often write g(\\mathsf{u}, \\mathsf{v}) as just \\mathsf{u} \\cdot \\mathsf{v} , with the understanding that the inner product g is evident from the context. Example The simplest, and also the most important, example of an inner product space is the vector space \\mathbb{R}^n defined earlier, with the inner product \\cdot:V \\times V \\to \\mathbb{R} defined as follows: for any (u_1, \\ldots, u_n) \\in \\mathbb{R}^n and (v_1, \\ldots, v_n) \\in \\mathbb{R}^n , (u_1, \\ldots, u_n) \\cdot (v_1, \\ldots, v_n) = \\sum_{i=1}^n u_i v_i. It is easy to check that this is indeed an inner product. The vector space \\mathbb{R}^n with this inner product is called the Euclidean space of dimension n . We will use the same symbol \\mathbb{R}^n to denote the n -dimensional Euclidean space. Example Define the set \\mathcal{P}_n([a,b],\\mathbb{R}) as the set of all real valued polynomials of degree less than or equal to n on the interval [a,b] \\subseteq \\mathbb{R} : \\mathcal{P}_n([a,b],\\mathbb{R}) = \\{f:[a,b] \\subseteq\\mathbb{R} \\to \\mathbb{R} \\,|\\, \\forall\\,x \\in [a,b], \\; f(x) = a_0 + a_1 x + \\ldots + a_n x^n, \\text{ where } a_0, a_1, \\ldots, a_n \\in \\mathbb{R}\\}. Define the function \\cdot:\\mathcal{P}_n([a,b],\\mathbb{R}) \\times \\mathcal{P}_n([a,b],\\mathbb{R}) \\to \\mathbb{R} as follows: for any f,g \\in \\mathcal{P}_n([a,b],\\mathbb{R}) , f \\cdot g = \\int_a^b f(x)g(x) \\, dx. It is left as a simple exercise to verify that this function is actually an inner product on the linear space \\mathcal{P}_n([a,b],\\mathbb{R}) with addition and scalar multiplication defined pointwise. The inner product on a vector space V can be used to define a norm on V . A norm on a vector space V is a function of the form \\lVert \\cdot \\rVert:V \\to \\mathbb{R} such that, for any \\mathsf{u}, \\mathsf{v} \\in V and a \\in \\mathbb{R} , Positive definiteness : \\lVert \\mathsf{u} \\rVert \\ge 0 , and \\lVert \\mathsf{u} \\rVert = 0 iff \\mathsf{u} = \\mathsf{0} , Homogeneity : \\lVert a \\mathsf{u} \\rVert = |a| \\lVert \\mathsf{u} \\rVert , Sub-additivity : \\lVert \\mathsf{u} + \\mathsf{v} \\rVert \\le \\lVert \\mathsf{u} \\rVert + \\lVert \\mathsf{v} \\rVert . Here, |a| refers to the absolute value of a \\in \\mathbb{R} . The property of sub-additivity is also referred to as the triangle inequality . A vector space V equipped with a norm \\lVert \\cdot \\rVert:V \\to \\mathbb{R} that satisfies these properties is called a normed vector space , or a normed linear space . Note that every inner product space is a normed linear space. To see this, note that given an an inner product \\cdot:V \\times V \\to \\mathbb{R} , the norm \\lVert \\cdot \\rVert: V \\to \\mathbb{R} induced by this inner product is defined as follows: for any \\mathsf{v} \\in V , \\lVert \\mathsf{v} \\rVert = \\sqrt{\\mathsf{v} \\cdot \\mathsf{v}}. The norm induced by the Euclidean inner product on \\mathbb{R}^3 is called the standard Euclidean norm on \\mathbb{R}^n . Remark In general, a normed vector space is not an inner product space. If, however, the norm \\lVert \\cdot \\rVert:V \\to \\mathbb{R} on a normed vector space V satisfies the following relation, called the parallelogram identity , \\frac{1}{2}\\left(\\lVert \\mathsf{u} + \\mathsf{v} \\rVert^2 + \\lVert \\mathsf{u} - \\mathsf{v}\\rVert^2\\right) = \\lVert \\mathsf{u} \\rVert^2 + \\lVert \\mathsf{v} \\rVert^2, for any \\mathsf{u}, \\mathsf{v} \\in V , then it is possible to define an inner product \\cdot:V \\times V \\to \\mathbb{R} using the norm as follows: for any \\mathsf{u},\\mathsf{v} \\in V , \\mathsf{u} \\cdot \\mathsf{v} = \\frac{1}{4}\\left(\\lVert \\mathsf{u} + \\mathsf{v} \\rVert^2 - \\lVert \\mathsf{u} - \\mathsf{v}\\rVert^2\\right). This relation is called the polarization identity . Example Let us consider the n -dimensional Euclidean space \\mathbb{R}^n with the standard inner product, defined earlier. The norm of a vector (x_1, \\ldots, x_n) \\in \\mathbb{R}^n is easily computed as \\lVert (x_1, \\ldots, x_n) \\rVert^2 = \\sum_{i=1}^n x_i^2. This norm is called the standard Euclidean norm , or the L_2 -norm on \\mathbb{R}^n . A variety of other norms can be defined on a given inner product space. For instance, the L_p -norm on \\mathbb{R}^n can be defined as follows: for any (x_1, \\ldots, x_n) \\in \\mathbb{R}^n and 1 \\le p < \\infty , \\lVert (x_1, \\ldots, x_n) \\rVert_p = \\left(\\sum_{i=1}^n x_i^p\\right)^{\\frac{1}{p}}. The L_\\infty -norm on \\mathbb{R}^n is defined as \\lVert (x_1, \\ldots, x_n) \\rVert_\\infty = \\text{max } \\{x_1, \\ldots, x_n\\}. Note that the standard Euclidean norm corresponds to \\lVert \\cdot \\lVert_2 . Remark There is an important theorem that states that all norms on a finite dimensional vector space are equivalent . This means the following: given norms \\lVert \\cdot \\rVert_1:V \\to \\mathbb{R} and \\lVert \\cdot \\rVert_2:V \\to \\mathbb{R} on a finite dimensional vector space V , there exists constants c_L, c_U \\in \\mathbb{R} such that, for any \\mathsf{v} \\in V , c_L \\lVert \\mathsf{v} \\rVert_2 \\le \\lVert \\mathsf{v} \\rVert_1 \\le c_U \\lVert \\mathsf{v} \\rVert_2. Without getting into technical details, this roughly means that the conclusions we draw about topological notions in a normed vector space are independent of the specific norm chosen.","title":"Inner products and norms"},{"location":"inner_product_spaces/#cauchy-schwarz-inequality","text":"Let V be a real vector space equipped with an inner product \\cdot:V \\times V \\to \\mathbb{R} . An important property of inner products that turns out to be quite useful in practice is discussed now. Given any \\mathsf{u}, \\mathsf{v} \\in V , the Cauchy-Schwarz inequality states that |\\mathsf{u} \\cdot \\mathsf{v}| \\le \\lVert \\mathsf{u} \\rVert \\lVert \\mathsf{v} \\rVert. Furthermore, the equality holds iff \\mathsf{u} and \\mathsf{v} are linearly dependent. The proof of the Cauchy-Schwarz inequality is quite easy: for any \\mathsf{u}, \\mathsf{v} \\in V , and a \\in \\mathbb{R} , \\lVert \\mathsf{u} + a \\mathsf{v} \\rVert^2 \\ge 0 \\Rightarrow \\lVert \\mathsf{u} \\rVert^2 + 2a \\mathsf{u} \\cdot \\mathsf{v} + \\rVert \\mathsf{v} \\rVert^2 \\ge 0. Substituting a = -\\frac{\\mathsf{u} \\cdot \\mathsf{v}}{\\lVert \\mathsf{v} \\rVert^2} in this inequality, we get (\\mathsf{u} \\cdot \\mathsf{v})^2 \\le \\lVert \\mathsf{u} \\rVert^2 \\lVert \\mathsf{v} \\rVert^2, which immediately yields the Cauchy-Schwartz inequality. To prove the second part, note that if \\mathsf{u} and \\mathsf{v} are linearly dependent, then, without loss of generality, \\mathsf{v} = a\\mathsf{u} for some a \\ in \\mathbb{R} . In this case, \\mathsf{u} \\cdot \\mathsf{v} = \\lvert a \\rvert \\lVert \\mathsf{u} \\rVert^2 = \\lVert \\mathsf{u} \\rVert \\lVert \\mathsf{v} \\rVert . On the other hand, if \\mathsf{u} \\cdot \\mathsf{v} = \\lVert \\mathsf{u} \\rVert \\lVert \\mathsf{v} \\rVert , consider the vector \\mathsf{w} \\in V , where, \\mathsf{w} = \\mathsf{u} - \\frac{\\lVert \\mathsf{u} \\rVert}{\\lVert \\mathsf{v} \\rVert}\\mathsf{v}. It is straightforward to show that \\lVert \\mathsf{w} \\rVert = 0 , and hence that \\mathsf{w} = 0 . This shows that \\mathsf{u} and \\mathsf{v} are linearly dependent. The Cauchy-Schwartz inequality is thus proved. The angle \\theta:V \\times V \\to \\in [0,2\\pi) \\subseteq \\mathbb{R} between two vectors \\mathsf{u}, \\mathsf{v} \\in V is defined via the relation \\cos \\theta(\\mathsf{u},\\mathsf{v}) = \\frac{\\mathsf{u} \\cdot \\mathsf{v}}{\\lVert \\mathsf{u} \\rVert\\lVert \\mathsf{v} \\rVert}. Note how the Cauchy-Schwarz inequality implies that this definition is well-defined. If the angle between two vectors \\mathsf{u}, \\mathsf{v} \\in V is \\pi/2 , they are said to be orthogonal . Equivalently, \\mathsf{u}, \\mathsf{v} \\in V are said to be orthogonal if \\mathsf{u} \\cdot \\mathsf{v} = 0 . If \\mathsf{u}, \\mathsf{v} \\in V are orthogonal, and if it is further true that \\lVert \\mathsf{u} \\rVert = \\lVert \\mathsf{v} \\rVert = 1 , then \\mathsf{u} and \\mathsf{v} are said to be orthonormal . Example The vectors \\mathsf{u} = (1, \\sqrt{3}) \\in \\mathbb{R}^2 and \\mathsf{v} = (-\\sqrt{3}, 1) \\in \\mathbb{R}^2 are orthogonal since \\mathsf{u} \\cdot \\mathsf{v} = 0 . They are not orthonormal since \\lVert \\mathsf{u} \\rVert = \\lVert \\mathsf{v} \\rVert = 2 . The vectors \\tilde{\\mathsf{u}} = \\mathsf{u}/\\lVert \\mathsf{u} \\rVert = (1/2, \\sqrt{3}/2) \\in \\mathbb{R}^2 and \\mathsf{v} = \\mathsf{v}/\\lVert \\mathsf{v} \\rVert = (-\\sqrt{3}/2, 1/2) \\in \\mathbb{R}^2 are, however, orthonormal. The following inequality also holds for any \\mathsf{u},\\mathsf{v} \\in V in an inner product space V : \\lVert \\mathsf{u} + \\mathsf{v} \\rVert \\le \\lVert \\mathsf{u} \\rVert + \\lVert \\mathsf{v} \\rVert. Recall that this is the triangle inequality . The triangle inequality is readily proved using the Cauchy-Schwarz inequality: for any \\mathsf{u}, \\mathsf{v} \\in V : \\begin{split} \\lVert \\mathsf{u} + \\mathsf{v} \\rVert^2 &= \\lVert \\mathsf{u} \\rVert^2 + \\lVert \\mathsf{v} \\rVert^2 + 2 \\mathsf{u} \\cdot \\mathsf{v}\\\\ &\\le \\lVert \\mathsf{u} \\rVert^2 + \\lVert \\mathsf{v} \\rVert^2 + 2 \\lVert \\mathsf{u} \\rVert \\lVert \\mathsf{v} \\rVert\\\\ &= (\\lVert \\mathsf{u} \\rVert + \\lVert \\mathsf{v} \\rVert)^2. \\end{split} The triangle inequality follows by taking the square root on both sides.","title":"Cauchy-Schwarz inequality"},{"location":"inner_product_spaces/#gram-schmidt-orthogonalization","text":"Let us now reconsider the notion of a basis of an n -dimensional vector space V in the special case when the vector space also has an inner product \\cdot:V \\times V \\to \\mathbb{R} defined on it. We say that a basis (\\mathsf{g}_i) of V is orthogonal if \\mathsf{g}_i \\cdot \\mathsf{g}_j = 0 whenever i, j \\in \\{1, \\ldots, n\\} , and i \\neq j . If it is further true that \\mathsf{g}_i \\cdot \\mathsf{g}_i = 1 for every i \\in \\{1, \\ldots, n\\} , we say that the basis (\\mathsf{g}_i) is orthonormal . The fact that a basis (\\mathsf{g}_i) of V is orthonormal can be succinctly expressed by the following equation: for any i,j \\in \\{1,\\ldots,n\\} , \\mathsf{g}_i \\cdot \\mathsf{g}_j = \\delta_{ij}, where \\delta_{ij} is the Kr\u00f6necker delta symbol that is defined as follows: \\delta_{ij} = \\begin{cases} 1, & i = j,\\\\ 0, & i \\neq j. \\end{cases} Example It is straightforward to verify that the standard basis (\\mathsf{e}_i) of \\mathbb{R}^n is an orthonormal basis, since it follows from the definition of the standard basis that \\mathsf{e}_i \\cdot \\mathsf{e}_j = \\delta_{ij} . The Gram-Schmidt orthogonalization procedure is an algorithm that helps us to transform any given basis (\\tilde{\\mathsf{g}}_i) of V into an orthonormal basis (\\mathsf{g}_i) . The algorithm works as follows: Let \\mathsf{g}_1 = \\frac{\\tilde{\\mathsf{g}}_1}{\\lVert \\tilde{\\mathsf{g}}_1 \\rVert}. We now define \\mathsf{g}_2 by removing the component of \\tilde{\\mathsf{g}}_2 along the direction \\mathsf{g}_1 : \\mathsf{g}_2 = \\frac{\\tilde{\\mathsf{g}}_2 - (\\tilde{\\mathsf{g}}_2 \\cdot \\mathsf{g}_1)e_1}{\\lVert \\tilde{\\mathsf{g}}_2 - (\\tilde{\\mathsf{g}}_2 \\cdot \\mathsf{g}_1)\\mathsf{g}_1 \\rVert}. It is easy to check that \\mathsf{g}_2 \\cdot \\mathsf{g}_1 = 0 and \\lVert \\mathsf{g}_2 \\rVert = 1 . We then obtain \\mathsf{g}_3 in a similar manner by removing the components of \\tilde{\\mathsf{g}}_3 along \\mathsf{g}_1 and \\mathsf{g}_2 : \\mathsf{g}_3 = \\frac{\\tilde{\\mathsf{g}}_3 - (\\tilde{\\mathsf{g}}_3 \\cdot \\mathsf{g}_2)\\mathsf{g}_2 - (\\tilde{\\mathsf{g}}_3 \\cdot \\mathsf{g}_1)e_1}{\\lVert \\tilde{\\mathsf{g}}_3 - (\\tilde{\\mathsf{g}}_3 \\cdot \\mathsf{g}_2)\\mathsf{g}_2 - (\\tilde{\\mathsf{g}}_3 \\cdot \\mathsf{g}_1)\\mathsf{g}_1 \\rVert}. It is straightforward to verify that \\mathsf{g}_1 \\cdot \\mathsf{g}_3 = 0 , \\mathsf{g}_2 \\cdot \\mathsf{g}_3 = 0 and \\lVert \\mathsf{g}_3 \\rVert = 1 . Continuing this process, we can construct an orthonormal basis (\\mathsf{g}_i)_{i=1}^n . Example As a simple illustration of the Gram-Schmidt orthogonalization process, let us consider the vectors \\tilde{\\mathsf{g}}_1 = (1,2) \\in \\mathbb{R}^2 and \\tilde{\\mathsf{g}}_2 = (2,3) \\in \\mathbb{R}^2 . We verified earlier that these vectors are linearly independent, and that they form a basis of \\mathbb{R}^2 . They are however not orthogonal since \\tilde{\\mathsf{g}}_1 \\cdot \\tilde{\\mathsf{g}}_2 = 8 \\neq 0 . Let us orthonormalize this basis using the Gram-Schmidt process. To start with, let us normalize \\tilde{\\mathsf{g}}_1 : \\mathsf{g}_1 = \\frac{\\tilde{\\mathsf{g}}_1}{\\lVert \\tilde{\\mathsf{g}}_1 \\rVert} = \\left(\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}\\right) \\in \\mathbb{R}^2. We can now construct \\mathsf{g}_2 \\in \\mathbb{R}^2 by projecting out the component of \\tilde{\\mathsf{g}}_2 along \\mathsf{g}_1 : \\begin{split} \\mathsf{g}_2 &= \\frac{\\tilde{\\mathsf{g}}_2 - (\\tilde{\\mathsf{g}}_2 \\cdot \\mathsf{g}_1)\\mathsf{g}_1}{\\lVert \\tilde{\\mathsf{g}}_2 - (\\tilde{\\mathsf{g}}_2 \\cdot \\mathsf{g}_1)\\mathsf{g}_1 \\rVert}\\\\ &= \\frac{(2,3) - \\left((2,3)\\cdot(1/\\sqrt{5},2/\\sqrt{5})\\right)(1/\\sqrt{5},2/\\sqrt{5})}{\\lVert (2,3) - \\left((2,3)\\cdot(1/\\sqrt{5},2/\\sqrt{5})\\right)(1/\\sqrt{5},2/\\sqrt{5})\\rVert}\\\\ &= \\frac{(2/5, -1/5)}{\\lVert (2/5, -1/5) \\rVert}\\\\ &= \\left(\\frac{2}{\\sqrt{5}}, -\\frac{1}{\\sqrt{5}}\\right) \\in \\mathbb{R}^2. \\end{split} It is easily checked that \\lVert \\mathsf{g}_1 \\rVert = 1 and \\lVert \\mathsf{g}_2 \\rVert = 1 , and that \\mathsf{g}_1 \\cdot \\mathsf{g}_2 = \\left(\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}\\right) \\cdot \\left(\\frac{2}{\\sqrt{5}}, -\\frac{1}{\\sqrt{5}}\\right) = 0. We have thus constructed an orthonormal basis (\\mathsf{g}_1, \\mathsf{g}_2) of \\mathbb{R}^2 starting from the general basis (\\tilde{\\mathsf{g}}_1, \\tilde{\\mathsf{g}}_2) of \\mathbb{R}^2 by following the Gram-Schmidt algorithm. Note that the orientation of the basis (\\mathsf{g}_1, \\mathsf{g}_2) obtained here is the opposite of the orientation of the standard basis (\\mathsf{e}_1, \\mathsf{e}_2) . To see this, embed these vectors in \\mathbb{R}^3 to get the vectors \\mathsf{e}_1 = (1,0,0) , \\mathsf{e}_2 = (0,1,0) , \\mathsf{g}_1 = (1/\\sqrt{5}, 2/\\sqrt{5}, 0) , \\mathsf{g}_2 = (2/\\sqrt{5}, -1/\\sqrt{5}, 0) , and note that \\mathsf{e}_1 \\times \\mathsf{e}_2 = \\mathsf{e}_3 , whereas \\mathsf{g}_1 \\times \\mathsf{g}_2 = -\\sqrt{5} \\hat{e}_3 - they have opposite signs! This doesn\u2019t really affect the Gram-Schmidt algorithm because if (\\mathsf{g}_1, \\mathsf{g}_2) is a basis of \\mathbb{R}^2 , then so is (\\mathsf{g}_2, \\mathsf{g}_1) . Remark It turns out that the choice of an orthonormal basis is sufficient for most applications. In the discussion below, we will first study various concepts with respect to the choice of an orthonormal basis, since the calculations are much simpler in this case. The general case of arbitrary bases will be discussed after this to give an idea of how some calculations can be more involved with respect to general bases.","title":"Gram-Schmidt orthogonalization"},{"location":"inner_product_spaces/#basis-representation-of-vectors","text":"We will now study the representation of a vector \\mathsf{v} \\in V in an n -dimensional inner product space V with respect to an orthonormal basis (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) of V . It is worth reiterating that we will only deal with orthonormal bases unless otherwise stated. The fact that (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) is a basis of V implies that every \\mathsf{v} \\in V can be written as \\mathsf{v} = \\sum_{i=1}^n v_i \\mathsf{e}_i where v_i \\in \\mathbb{R} for every i \\in \\{1, \\ldots, n\\} . This is called the representation of v with respect to the basis (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) . The real numbers v_1, \\ldots v_n are called the components of \\mathsf{v} with respect to the basis (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) . To compute the components v_i , we can exploit the fact that (\\mathsf{e}_i) is an orthonormal basis: \\mathsf{v} \\cdot \\mathsf{e}_j = v_j \\quad\\Rightarrow\\quad \\mathsf{v} = \\sum (\\mathsf{v} \\cdot \\mathsf{e}_i) \\mathsf{e}_i. The components v_i thus computed are unique since we have explicitly constructed each component v_i as \\mathsf{v} \\cdot \\mathsf{e}_i . We can alternatively show the uniqueness of the components based on the fact that the basis vectors are linearly independent by definition. Remark Notice how we have represented the sum on the right without representing the summation index, and the range of summation. We will write \\sum_i , or just \\sum in place of \\sum_{i=1}^n , whenever the range under consideration is obvious from the context. If no index is associated with the summation symbol, as in \\sum , it will be assumed that the sum is with respect to all repeating indices. In addition, we will assume that the range of the all the indices involved is known from the context. While on this, it is worth noting that many authors employ the Einstein summation convention , according to which, a sum of the form \\sum u_i \\mathsf{e}_i is written simply as u_i \\mathsf{e}_i , with the summation over i being implicitly understood as long as the indices repeat twice. For pedagogical reasons, we will not follow the Einstein summation convention in these notes. Example As a trivial example of the basis representation of a vector, consider any (x_1, \\ldots, x_n) \\in \\mathbb{R}^n . This vector can be written with respect to the standard basis (\\mathsf{e}_i) of \\mathbb{R}^n as (x_1, \\ldots, x_n) = \\sum x_i \\mathsf{e}_i. Notice that x_i = (x_1, \\ldots, x_n) \\cdot \\mathsf{e}_i . Example As a non-trivial, yet simple, example of basis representation of a vector, consider the orthonormal basis (\\mathsf{g}_1, \\mathsf{g}_2) of \\mathbb{R}^2 , where \\mathsf{g}_1 = (2/\\sqrt{5}, -1/\\sqrt{5}) and \\mathsf{g}_2 = (1/\\sqrt{5}, 2/\\sqrt{5}) - notice that we have swapped the order of the basis constructed earlier in the context of the Gram-Schmidt procedure to maintain orientation. Consider any (x_1, x_2) \\in \\mathbb{R}^2 , we can express this in terms of the basis (\\mathsf{g}_1, \\mathsf{g}_2) as (x_1, x_2) = \\sum \\bar{x}_i \\mathsf{g}_i, for some real constants (\\bar{x}_i) . To compute this, note that we can write (x_1, x_2) = \\sum x_i \\mathsf{e}_i using the standard basis of \\mathbb{R}^2 . The constants (\\bar{x}_i) are easily computed by taking the appropriate dot products: \\begin{split} \\bar{x}_i &= \\left(\\sum x_j \\mathsf{e}_j\\right)\\cdot \\mathsf{g}_i\\\\ &= (\\mathsf{g}_i \\cdot \\mathsf{e}_1) x_1 + (\\mathsf{g}_i \\cdot \\mathsf{e}_2) x_2. \\end{split} For instance, the vector (1,2) \\in \\mathbb{R}^2 can be expressed in terms of the (\\mathsf{g}_1, \\mathsf{g}_2) basis as (1,2) = \\sum \\bar{x}_i \\mathsf{g}_i , where \\begin{split} \\bar{x}_1 &= \\frac{2}{\\sqrt{5}}1 - \\frac{1}{\\sqrt{5}}2 = 0,\\\\ \\bar{x}_2 &= \\frac{1}{\\sqrt{5}}1 + \\frac{2}{\\sqrt{5}}2 = \\sqrt{5}. \\end{split} We thus see that (1,2) = \\sqrt{5}\\mathsf{g}_2 , a fact that can be easily checked directly. We will now introduce a useful notion called component maps to collect the components v_i of any \\mathsf{v} \\in V with respect to the (not necessarily orthonormal) basis B = (\\mathsf{e}_1, \\ldots, \\mathsf{e}_n) of V . Define the component map \\mathsf\\phi_{V,B}:V \\to \\mathbb{R}^n as follows: \\mathsf\\phi_{V,B}(\\mathsf{v}) = [v_1, \\ldots, v_n]^T. Notice how we have collected together the components of \\mathsf{v} as a column vector of size n using the component map. It is useful at this point to introduce the following notation: _{(\\mathsf{e}_i)} = [v_1, \\ldots, v_n]^T When the choice of basis is evident from the context, we will often write [\\mathsf{v}]_{(\\mathsf{e}_i)} as just [\\mathsf{v}] . We can thus alternatively the basis representation of any \\mathsf{v} \\in V with respect to a general basis (\\mathsf{e}_i) of V as follows: \\mathsf{v} = \\sum v_i \\mathsf{e}_i = \\sum \\left(\\mathsf\\phi_{V,B}(\\mathsf{v})\\right)_i \\mathsf{e}_i = \\sum [\\mathsf{v}]_i \\mathsf{e}_i. We will however use the simpler notation \\mathsf{v} = \\sum v_i \\mathsf{e}_i , and use the [\\mathsf{v}] notation only when we want to refer to the components alone as a column vector. We will see shortly that the component map is an example of an isomorphism between the vector spaces V and \\mathbb{R}^n . Remark We will often write the component map \\mathsf\\phi_{V,B} as \\mathsf\\phi_V , or just \\mathsf\\phi , when the vector space and its basis are evident from the context. At times, we will omit \\mathsf\\phi altogether and refer to the component map using the following notation: \\mathsf{v} \\in V \\mapsto [\\mathsf{v}] \\in \\mathbb{R}^n . Example As a quick illustration of this, notice that the vector (1,2) \\in \\mathbb{R}^2 has the following matrix representation with respect to the orthonormal basis (e_1, e_2) , where e_1 = (2/\\sqrt{5}, -1/\\sqrt{5}) and e_2 = (1/\\sqrt{5}, 2/\\sqrt{5}) of \\mathbb{R}^2 is [0,\\sqrt{5}]^T .","title":"Basis representation of vectors"},{"location":"inner_product_spaces/#change-of-basis-rules-for-vectors","text":"Given an n -dimensional inner product space V , let us first consider the case where (\\mathsf{e}_i) and (\\mathsf{g}_i) are two general bases of V , not necessarily orthonormal. Then, any \\mathsf{v} \\in V can be written as \\mathsf{v} = \\sum v_i \\mathsf{e}_i = \\sum \\tilde{v}_i \\mathsf{g}_i, where (v_i) and (\\tilde{v}_i) are the components of \\mathsf{v} with respect to the bases (\\mathsf{e}_i) and (\\mathsf{g}_i) , respectively. The fact that (\\mathsf{e}_i) is a basis of V implies that \\mathsf{g}_i = \\sum A_{ji} \\mathsf{e}_j, where A_{ij} \\in \\mathbb{R} for every 1 \\le i,j \\le n . Similarly, we have \\mathsf{e}_i = \\sum B_{ji} \\mathsf{g}_j, where B_{ij} \\in \\mathbb{R} for every 1 \\le i,j \\le n . Notice how the first index of the transformation coefficients pairs with the corresponding basis vector. The reason for this specific choice will become clear shortly. Combining these two transformation relations, we see that \\mathsf{g}_i = \\sum A_{ji}B_{kj} \\mathsf{g}_k \\quad\\Rightarrow\\quad \\sum B_{kj}A_{ji} = \\delta_{ki}, and \\mathsf{e}_i = \\sum B_{ji}A_{kj}\\mathsf{e}_k \\quad\\Rightarrow\\quad \\sum A_{kj}B_{ji} = \\delta_{ki}. It is convenient to collect together the constants \\{A_{ij}\\} as a matrix \\mathsf{A} whose (i,j)^{\\text{th}} entry is A_{ij} . We will similarly collect the constants \\{B_{ij}\\} in a matrix \\mathsf{B} . In matrix notation, we can write the foregoing equations succinctly as \\mathsf{A}\\mathsf{B} = \\mathsf{I}, \\quad \\mathsf{B}\\mathsf{A} = \\mathsf{I}, \\quad\\Rightarrow\\quad \\mathsf{A} = \\mathsf{B}^{-1}, where \\mathsf{I} is the identity matrix of order n . We thus see that matrices \\mathsf{A} and \\mathsf{B} are inverses of each other. Given the transformation relations between the two bases, we can use the identity \\sum v_i \\mathsf{e}_i = \\sum \\tilde{v}_i \\mathsf{g}_i to see that \\sum \\tilde{v}_i \\mathsf{g}_i = \\sum v_i B_{ji} \\mathsf{g}_j \\quad\\Rightarrow\\quad \\tilde{v}_i = \\sum B_{ij} v_j = \\sum A^{-1}_{ij} v_j. In matrix notation, we can summarize the foregoing result as follows: \\mathsf{g}_i = \\sum A_{ji} \\mathsf{e}_j \\quad\\Rightarrow\\quad [\\mathsf{v}]_{(\\mathsf{g}_i)} = \\mathsf{A}^{-1}[\\mathsf{v}]_{(\\mathsf{e}_i)}. !!! info \"Remark\" In a more general treatment of linear algebra, the fact that the components of a vector \\mathsf{v} \\in V transform in a manner contrary to that of the manner in which the basis vectors transform is used to call elements of V as contravariant vectors. We will not develop the general theory here, but make a few elementary remarks occasionally regarding this. Let us now consider the special case when both (\\mathsf{e}_i) and (\\mathsf{g}_i) are orthonormal bases of V . In this case, we can use the fact that \\mathsf{e}_i \\cdot \\mathsf{e}_j = \\delta_{ij}, \\qquad \\mathsf{g}_i \\cdot \\mathsf{g}_j = \\delta_{ij}, to simplify the calculations. Suppose that \\mathsf{g}_i = \\sum Q_{ji} \\mathsf{e}_j. It follows immediately that Q_{ji} = \\mathsf{g}_i \\cdot \\mathsf{e}_j. Let us now see how the components of any \\mathsf{v} \\in V transform upon this change of basis: \\mathsf{v} = \\sum \\tilde{v}_i \\mathsf{g}_i = \\sum v_i \\mathsf{e}_i \\quad\\Rightarrow\\quad \\tilde{v}_i = \\sum \\mathsf{e}_j \\cdot \\mathsf{g}_i v_j = \\sum Q_{ji} v_j. In matrix notation, this is written as _{(\\mathsf{g}_i)} = \\mathsf{Q}^T[\\mathsf{v}]_{(\\mathsf{e}_i)}, where \\mathsf{Q} is the matrix whose (i,j)^{\\text{th}} entry is Q_{ij} . But, based on the calculation we carried out earlier in the context of general bases, we see that \\mathsf{g}_i = \\sum Q_{ji} \\mathsf{e}_j \\quad\\Rightarrow\\quad [\\mathsf{v}]_{(\\mathsf{g}_i)} = \\mathsf{Q}^{-1}[\\mathsf{v}]_{(\\mathsf{e}_i)}. Comparing these two expressions, we are led to the following conclusion: \\mathsf{Q}^T = \\mathsf{Q}^{-1}. Recall that matrices that satisfy this condition are called orthogonal matrices. It is an easy consequence of orthogonality that the determinant of an orthogonal matrix is \\pm 1 , as the following calculation shows: if \\mathsf{Q} is an orthogonal matrix (\\text{det}(\\mathsf{Q}))^2 = \\text{det}(\\mathsf{Q}^T\\mathsf{Q}) = \\text{det}(\\mathsf{I}) = 1 \\quad\\Rightarrow\\quad \\text{det}(\\mathsf{Q}) = \\pm 1. If \\text{det}(\\mathsf{Q}) = 1 , then the orthogonal matrix \\mathsf{Q} is called proper orthogonal , or special orthogonal . Remark It is important to note that the foregoing conclusion that the matrix involved in the change of basis is orthogonal is true only in the special case when both bases are orthonormal. Example Consider the orthonormal bases (\\mathsf{e}_1, \\mathsf{e}_2) and (\\mathsf{g}_1, \\mathsf{g}_2) of \\mathbb{R}^2 , where (\\mathsf{e}_1, \\mathsf{e}_2) is the standard basis of \\mathbb{R}^2 and \\mathsf{g}_1 = (2/\\sqrt{5}, -1/\\sqrt{5}) , \\mathsf{g}_2 = (1/\\sqrt{5}, 2/\\sqrt{5}) . The transformation matrix \\mathsf{Q} from (\\mathsf{e}_1,\\mathsf{e}_2) to (\\mathsf{g}_1,\\mathsf{g}_2) is computed using the relation Q_{ij} = \\mathsf{g}_j \\cdot \\mathsf{e}_i as \\mathsf{Q} = \\begin{bmatrix} \\mathsf{g}_1 \\cdot \\mathsf{e}_1 & \\mathsf{g}_2 \\cdot \\mathsf{e}_1\\\\ \\mathsf{g}_1 \\cdot \\mathsf{e}_2 & \\mathsf{g}_2 \\cdot \\mathsf{e}_2 \\end{bmatrix} = \\frac{1}{\\sqrt{5}}\\begin{bmatrix} 2 & 1\\\\ -1 & 2 \\end{bmatrix}. It is easily checked that \\mathsf{Q} is orthogonal: \\mathsf{Q}^T\\mathsf{Q} = \\frac{1}{5}\\begin{bmatrix}2 & -1\\\\ 1 & 2\\end{bmatrix}\\begin{bmatrix}2 & 1\\\\ -1 & 2\\end{bmatrix} = \\begin{bmatrix} 1 & 0\\\\ 0 & 1\\end{bmatrix}. It can be similarly checked that \\mathsf{Q}\\mathsf{Q}^T = \\mathsf{I} . As a quick check, note that also the determinant of the transformation map \\mathsf{Q} in the previous example is 1 : \\text{det}\\left( \\frac{1}{\\sqrt{5}}\\begin{bmatrix} 2 & 1\\\\ -1 & 2 \\end{bmatrix} \\right) = 1. This informs us that the transformation matrix \\mathsf{Q} is in fact special orthogonal.","title":"Change of basis rules for vectors"},{"location":"inner_product_spaces/#general-basis","text":"Most of the discussion thus far regarding the representation of a vector in a finite dimensional inner product space has been restricted to the special case of orthonormal bases. Let us briefly consider the general case when a general basis, which is not necessarily orthonormal, is chosen. In what follows, V denotes an inner product space of dimension n , and (\\mathsf{g}_i) is a general basis of V .","title":"General basis"},{"location":"inner_product_spaces/#representation-of-vectors","text":"Any \\mathsf{v} \\in V can be written in terms of the basis (\\mathsf{g}_1, \\ldots, \\mathsf{g}_n) of V as \\mathsf{v} = \\sum v_i \\mathsf{g}_i, where (v_1, \\ldots, v_n) are the components of \\mathsf{v} with respect to this basis. To compute these components, start with taking the inner product of this equation with the basis vector g_i ; this yields \\mathsf{v} \\cdot \\mathsf{g}_i = \\sum \\mathsf{g}_i \\cdot \\mathsf{g}_j \\, v_j. This equation can be written in the form of a matrix equation, as follows: \\begin{bmatrix} \\mathsf{g}_1 \\cdot \\mathsf{g}_1 & \\ldots & \\mathsf{g}_1 \\cdot \\mathsf{g}_n\\\\ \\vdots & \\ddots & \\vdots\\\\ \\mathsf{g}_n \\cdot \\mathsf{g}_1 & \\ldots & \\mathsf{g}_n \\cdot \\mathsf{g}_n \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} \\mathsf{v} \\cdot \\mathsf{g}_1\\\\ \\vdots\\\\ \\mathsf{v} \\cdot \\mathsf{g}_n \\end{bmatrix}. The fact that (\\mathsf{g}_i) is a basis of V implies that the components v_1,\\ldots,v_n exist and are unique. This implies that the matrix introduced above, whose (i,j)^{\\text{th}} entry is g_{ij} = \\mathsf{g}_i\\cdot\\mathsf{g}_j , is invertible. It is conventional, and convenient, to represent the inverse of this matrix as the matrix with entries g^{ij} ; thus, \\begin{bmatrix} g_{11} & \\ldots & g_{1n}\\\\ \\vdots & \\ddots & \\vdots\\\\ g_{n1} & \\ldots & g_{nn} \\end{bmatrix}^{-1} = \\begin{bmatrix} g^{11} & \\ldots & g^{1n}\\\\ \\vdots & \\ddots & \\vdots\\\\ g^{n1} & \\ldots & g^{nn} \\end{bmatrix}. A proper justification for this choice of notation will be given shortly when we study reciprocal bases . The fact that these two matrices are inverses of each other can be written succinctly as follows: \\sum g^{ik}g_{kj} = \\delta_{ij} = \\sum g_{ik}g^{kj}. Using this result, a trite calculation yields the following result: for any \\mathsf{v} \\in V , \\mathsf{v} = \\sum v_i \\mathsf{g}_i \\quad\\Rightarrow\\quad v_i = \\sum g^{ij}\\mathsf{g}_j \\cdot \\mathsf{v}. The components of any vector with respect to a general basis can thus be computed explicitly. Example Consider the basis (\\mathsf{g}_1,\\mathsf{g}_2) of \\mathbb{R}^2 , where \\mathsf{g}_1 = (1,2) and \\mathsf{g}_2 = (2,3) . Let us now compute the components of \\mathsf{v} = (2,5) \\in \\mathbb{R}^2 with respect to this basis. The first step in to compute the matrix whose entries are g_{ij} : \\begin{bmatrix} \\mathsf{g}_1 \\cdot \\mathsf{g}_1 & \\mathsf{g}_1 \\cdot \\mathsf{g}_2\\\\ \\mathsf{g}_2 \\cdot \\mathsf{g}_1 & \\mathsf{g}_2 \\cdot \\mathsf{g}_2 \\end{bmatrix} = \\begin{bmatrix} 5 & 8\\\\ 8 & 13 \\end{bmatrix}. Notice that this matrix is symmetric, as expected, since g_{ij} = g_{ji} , in general. The inverse of this matrix gives the scalars (g^{ij}) as follows: \\begin{bmatrix} g^{11} & g^{12}\\\\ g^{21} & g^{22} \\end{bmatrix} = \\begin{bmatrix} 5 & 8\\\\ 8 & 13 \\end{bmatrix}^{-1} = \\begin{bmatrix} 13 & -8\\\\ -8 & 5 \\end{bmatrix}. The components of \\mathsf{v} with respect to the basis (\\mathsf{g}_i) are now easily computed using the result v_i = \\sum g^{ij} \\mathsf{g}_j \\cdot \\mathsf{v} as, \\begin{split} v_1 &= \\sum g^{1j} \\mathsf{g}_j \\cdot \\mathsf{v} = g^{11} \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 5\\end{bmatrix} + g^{12} \\begin{bmatrix} 2 \\\\ 3\\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 5\\end{bmatrix} = 4,\\\\ v_2 &= \\sum g^{2j} \\mathsf{g}_j \\cdot \\mathsf{v} = g^{21} \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 5\\end{bmatrix} + g^{22} \\begin{bmatrix} 2 \\\\ 3\\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 5\\end{bmatrix} = -1. \\end{split} We thus see that \\mathsf{v} = 4\\mathsf{g}_1 - \\mathsf{g}_2 . As a consistency check, substitute the representations of \\mathsf{v}, \\mathsf{g}_1,\\mathsf{g}_2 with respect to the standard basis of \\mathbb{R}^2 and verify that this is correct.","title":"Representation of vectors"},{"location":"inner_product_spaces/#reciprocal-basis","text":"The computations presented in the previous section can be greatly simplified by introducing the reciprocal basis corresponding to a given basis. Given a basis (\\mathsf{g}_1, \\ldots, \\mathsf{g}_n) of V , its reciprocal basis is defined as the basis (\\mathsf{g}^1, \\ldots, \\mathsf{g}^n) such that \\mathsf{g}^i \\cdot \\mathsf{g}_j = \\delta_{ij}, where 1 \\le i,j \\le n . Based on the preceding development, it can be seen that the reciprocal basis is explicitly given by the following equations: \\mathsf{g}^i = \\sum g^{ij}\\mathsf{g}_j. This can be readily inverted to yield the following equation: \\mathsf{g}_i = \\sum g_{ij} \\mathsf{g}^j. Note that in the special case of the standard basis (\\mathsf{e}_i) of \\mathbb{R}^3 , \\mathsf{e}^i = \\mathsf{e}_i . More generally, if (\\mathsf{g}_i) is an orthonormal basis of an inner product space V , then \\mathsf{g}^i = \\mathsf{g}_i . This is one of the reasons why many calculations are much simpler when using orthonormal bases. It follows from the definition of the reciprocal basis (\\mathsf{g}^i) of V that \\begin{split} \\mathsf{g}^i \\cdot \\mathsf{g}^j &= \\left(\\sum g^{ik}\\mathsf{g}_k\\right)\\left(\\sum g^{jl}\\mathsf{g}_l\\right)\\\\ &= \\sum g^{ik}g^{jl}g_{kl} = \\sum \\delta_{il}g^{jl}\\\\ &= g^{ij}. \\end{split} Thus, the following useful formulate are obtained: if (\\mathsf{g}_i) is a general basis of V and (\\mathsf{g}^i) is its reciprocal basis, then \\mathsf{g}_i \\cdot \\mathsf{g}_j = g_{ij} and \\mathsf{g}^i \\cdot \\mathsf{g}^j = g^{ij} . Remark The use of superscripts here is done purely for notational convenience. It is however possible to justify such a notation when considering a more detailed treatment of this subject, as will be briefly noted later. Example Consider the previous example involving the basis (\\mathsf{g}_1,\\mathsf{g}_2) of \\mathbb{R}^2 , where \\mathsf{g}_1 = (1,2) and \\mathsf{g}_2 = (2,3) . In this case, the reciprocal basis (\\mathsf{g}^1, \\mathsf{g}^2) is computed as follows: \\begin{split} \\mathsf{g}^1 &= \\sum g^{1j}\\mathsf{g}_j = 13\\cdot(1,2) - 8\\cdot(2,3) = (-3,2),\\\\ \\mathsf{g}^2 &= \\sum g^{2j}\\mathsf{g}_j = -8\\cdot(1,2) + 5\\cdot(2,3) = (2,-1). \\end{split} It can be checked with a simple calculation that \\mathsf{g}^i \\cdot \\mathsf{g}_j = \\delta_{ij} , as expected. Further more, the matrix whose (i,j)^{\\text{th}} entry is \\mathsf{g}^i \\cdot \\mathsf{g}^j is computed as \\begin{bmatrix} \\mathsf{g}^1 \\cdot \\mathsf{g}^1 & \\mathsf{g}^1 \\cdot \\mathsf{g}^2\\\\ \\mathsf{g}^2 \\cdot \\mathsf{g}^1 & \\mathsf{g}^2 \\cdot \\mathsf{g}^2 \\end{bmatrix} = \\begin{bmatrix} 13 & -8\\\\ -8 & 5 \\end{bmatrix} = \\begin{bmatrix} 5 & 8\\\\ 8 & 13 \\end{bmatrix}^{-1} = \\begin{bmatrix} \\mathsf{g}_1 \\cdot \\mathsf{g}_1 & \\mathsf{g}_1 \\cdot \\mathsf{g}_2\\\\ \\mathsf{g}_2 \\cdot \\mathsf{g}_1 & \\mathsf{g}_2 \\cdot \\mathsf{g}_2 \\end{bmatrix}^{-1}. This confirms that the matrix whose (i,j)^{\\text{th}} entry is \\mathsf{g}_i \\cdot \\mathsf{g}_j is indeed the inverse of the matrix whose (i,j)^{\\text{th}} entry is \\mathsf{g}^i \\cdot \\mathsf{g}^j . The expressions for the coefficients of a vector with respect to a given basis simple form when expressed in terms of the reciprocal basis. Given any \\mathsf{v} \\in V and a basis (\\mathsf{g}_i) of V , \\mathsf{v} = \\sum \\tilde{v}_i \\mathsf{g}_i \\quad\\Rightarrow\\quad \\tilde{v}_i = \\mathsf{v} \\cdot \\mathsf{g}^i. Thus, any \\mathsf{v} \\in V has the compact representation \\mathsf{v} = \\sum (\\mathsf{v} \\cdot \\mathsf{g}^i) \\mathsf{g}_i. Compare this with the representation \\mathsf{v} = \\sum (\\mathsf{v} \\cdot \\mathsf{e}_i) \\mathsf{e}_i of \\mathsf{v} with respect to an orthonormal basis (\\mathsf{e}_i) of V . Remark Given any \\mathsf{v} \\in V and a basis (\\mathsf{g}_i) of V , the components of \\mathsf{v} with respect to the (\\mathsf{g}_i) and its reciprocal basis (\\mathsf{g}^i) are written as follows: \\mathsf{v} = \\sum v_i \\mathsf{g}_i = \\sum v^*_i \\mathsf{g}^i. The components (v_i) and (v^*_i) are called the contravariant and covariant components of \\mathsf{v} , respectively. In many textbooks, the following alternative notation is used: \\mathsf{v} = \\sum v^i \\mathsf{g}_i = \\sum v_i \\mathsf{g}^i. The components v_i and v^i are related as follows: v^i = g^{ij}v_j and v_i = g_{ij}v^j . For this reason, g^{ij} and g_{ij} are said to raise and lower, respectively, indices. Since we will largely restrict ourselves to the case of orthonormal bases and rarely represent a vector in terms of the reciprocal basis to a given basis, we will not adopt this more nuanced notation here.","title":"Reciprocal basis"},{"location":"inner_product_spaces/#change-of-basis-rules","text":"The ideas presented so far can be used to express a given vector \\mathsf{v} \\in V with respect to different bases. Suppose that \\mathsf{v} has the following representations, with respect to two different bases (\\mathsf{f}_i) and (\\mathsf{g}_i) of V : \\mathsf{v} = \\sum \\bar{v}_i\\mathsf{f}_i = \\sum \\tilde{v}_i \\mathsf{g}_i. Taking the inner product of these representations with respect to the appropriate reciprocal basis vectors, it is evident that \\mathsf{v} = \\sum \\bar{v}_i \\mathsf{f}_i = \\sum \\tilde{v}_i \\mathsf{g}_i \\quad\\Rightarrow\\quad \\bar{v}_i = \\sum \\mathsf{f}^i\\cdot\\mathsf{g}_j \\tilde{v}_j, and a similar formula expressing (\\tilde{v}_i) in terms of (\\bar{v}_i) . Notice how the use of the reciprocal basis significantly simplifies the computations. Example Consider the example considered earlier where the vector (2,5) \\in \\mathbb{R}^2 was expressed in terms of the basis (\\mathsf{g}_1,\\mathsf{g}_2) of \\mathbb{R}^2 , where \\mathsf{g}_1 = (1,2) and \\mathsf{g}_2 = (2,3) . We saw earlier that \\mathsf{v} = 4\\mathsf{g}_1 - \\mathsf{g}_2. Let us now consider another basis (\\mathsf{f}_1,\\mathsf{f}_2) of \\mathbb{R}^2 , where \\mathsf{f}_1 = (2,1) , \\mathsf{f}_2 = (1,3) . To compute the representation of \\mathsf{v} with respect to the basis (\\mathsf{f}_1,\\mathsf{f}_2) , we first need to compute its reciprocal basis (\\mathsf{f}^1,\\mathsf{f}^2) . This is easily accomplished as follows: \\begin{bmatrix} \\mathsf{f}_1 \\cdot \\mathsf{f}_1 & \\mathsf{f}_1 \\cdot \\mathsf{f}_2\\\\ \\mathsf{f}_2 \\cdot \\mathsf{f}_1 & \\mathsf{f}_2 \\cdot \\mathsf{f}_2 \\end{bmatrix} = \\begin{bmatrix} 5 & 8\\\\ 8 & 13 \\end{bmatrix} \\quad\\Rightarrow\\quad \\begin{bmatrix} f^{11} & f^{12}\\\\ f^{21} & f^{22} \\end{bmatrix} = \\frac{1}{5} \\begin{bmatrix} 2 & -1\\\\ -1 & 1 \\end{bmatrix} The reciprocal basis is computed using the relations \\mathsf{f}^i = \\sum f^{ij}\\mathsf{f}_j : \\begin{split} \\mathsf{f}^1 &= \\sum f^{1j}\\mathsf{f}_j = \\frac{2}{5}(2,1) - \\frac{1}{5}(1,3) = \\frac{1}{5}(3,-1),\\\\ \\mathsf{f}^2 &= \\sum f^{2j}\\mathsf{f}_j = -\\frac{1}{5}(2,1) + \\frac{1}{5}(1,3) = \\frac{1}{5}(-1,2). \\end{split} It is left as an easy exercise to verify that \\mathsf{f}^i \\cdot \\mathsf{f}_j = \\delta_{ij} . Using these relations the components (\\bar{v}_1,\\bar{v}_2) of \\mathsf{v} with respect to the basis (\\mathsf{f}_1,\\mathsf{f}_2) can be computed using the relations \\bar{v}_i = \\mathsf{v} \\cdot \\mathsf{f}^i as follows: \\begin{split} \\bar{v}_1 &= \\mathsf{v} \\cdot \\mathsf{f}^1 = (2,5) \\cdot \\frac{1}{5}(3,-1) = \\frac{1}{5},\\\\ \\bar{v}_2 &= \\mathsf{v} \\cdot \\mathsf{f}^2 = (2,5) \\cdot \\frac{1}{5}(-1,2) = \\frac{8}{5}. \\end{split} We thus see get the representation of \\mathsf{v} in the basis (\\mathsf{f}_1,\\mathsf{f}_2) as \\mathsf{v} = \\frac{1}{5}(\\mathsf{f}_1 + 8\\mathsf{f}_2). It is left as a simple exercise to verify by direct substitution that this is true. Finally, note that the components (\\bar{v}_1,\\bar{v}_2) of \\mathsf{v} with respect to the basis (\\mathsf{f}_1,\\mathsf{f}_2) can be directly obtained from its components (\\tilde{v}_1,\\tilde{v}_2) with respect to the basis (\\mathsf{g}_1,\\mathsf{g}_2) using the relations \\bar{v}_i = \\sum \\mathsf{f}^i \\cdot \\mathsf{g}_j \\tilde{v}_j as follows: \\begin{split} \\bar{v}_1 &= \\sum \\mathsf{f}^1\\cdot\\mathsf{g}_j \\tilde{v}_j = \\frac{4}{5}(3,-1)\\cdot(1,2) - \\frac{1}{5}(3,-1)\\cdot(2,3) = \\frac{1}{5},\\\\ \\bar{v}_2 &= \\sum \\mathsf{f}^2\\cdot\\mathsf{g}_j \\tilde{v}_j = \\frac{4}{5}(-1,2)\\cdot(1,2) - \\frac{1}{5}(-1,2)\\cdot(2,3) = \\frac{8}{5}. \\end{split} We thus see that all the different ways to compute the components of \\mathsf{v} with respect to two different choices of bases are consistent with each other.","title":"Change of basis rules"},{"location":"linear_maps_1/","text":"An important technique to study structured sets is to study functions between such sets that preserve their structure. In the current context, the structure inherent to vector spaces is linearity. Maps between vector spaces that preserve this linear structure, called linear maps are studied now. Throughout this section, V and W represent finite dimensional inner product spaces of dimension n and m , respectively. Basic definitions Let us first consider the case when V and W are real vector spaces, not necessarily endowed with an inner product. We call a map of the form \\mathsf{T}:V \\to W a vector space homomorphism , or more simply a linear map , if, for any \\mathsf{u}, \\mathsf{v} \\in V , and for any a, b \\in \\mathbb{R} , \\mathsf{T}(a\\mathsf{u} + b\\mathsf{v}) = a\\mathsf{T}(\\mathsf{u}) + b \\mathsf{T}(\\mathsf{v}). It is conventional to write \\mathsf{T}(\\mathsf{v}) as just \\mathsf{T}\\mathsf{v} when \\mathsf{T} is a linear map. Notice how linear maps preserve the linear structure: the action of a linear map on a linear combination of vectors is the linear combination of the action of the linear map on the individual vectors. Remark Note that, in the definition above, the vector addition and scalar multiplication in the term (a\\mathsf{u} + b\\mathsf{v}) are those defined in V , while the ones in the term a\\mathsf{T}(\\mathsf{u}) + b\\mathsf{T}(\\mathsf{v}) are those in W . The same notation is used just to keep the notation simple. Example Consider the map \\mathsf\\pi:\\mathbb{R}^3 \\to \\mathbb{R}^2 defined as follows: for any (x_1, x_2, x_3) \\in \\mathbb{R}^3 , \\mathsf\\pi(x_1, x_2, x_3) = (x_1, x_2). It is easy to verify that \\mathsf\\pi:\\mathbb{R}^3 \\to \\mathbb{R}^2 is a linear map. We will now present a few definitions associated with linear maps that are useful in practice. The kernel of the linear map \\mathsf{T}:V \\to W is the set of all elements in V that are mapped to \\mathsf{0} \\in W by \\mathsf{T} : \\text{ker}(\\mathsf{T}) = \\{\\mathsf{v}\\in V \\,|\\, \\mathsf{T}\\mathsf{v} = \\mathsf{0}\\}. \\text{ker}(\\mathsf{T}) is also called the null space of \\mathsf{T} , and is a linear subspace of V . Note that the kernel \\mathsf{T} is always non-empty since \\mathsf{0} \\in \\text{ker}(T) . The image of the linear map \\mathsf{T}:V \\to W between two vector spaces V and W , written as \\text{img}(\\mathsf{T}) is defined as the set of all those elements in W which are obtained by the action of \\mathsf{T} on some element of V : \\text{img}(\\mathsf{T}) = \\{\\mathsf{w} \\in W \\,|\\,\\text{for some }\\mathsf{v} \\in V,\\,\\mathsf{T}\\mathsf{v} = \\mathsf{w}\\}. The image of a linear map is a linear subspace of its codomain. The dimension of the image of the linear map \\mathsf{T}:V \\to W is called the rank of \\mathsf{T} . The following result holds for all linear maps \\mathsf{T}:V \\to W between finite dimensional vector spaces: \\text{dim}(V) = \\text{dim}(\\text{ker}(\\mathsf{T})) + \\text{dim}(\\text{img}(\\mathsf{T})). This is known as the rank-nullity theorem . Example Consider the map \\mathsf\\pi:\\mathbb{R}^3 \\to \\mathbb{R}^2 that was defined earlier as \\mathsf\\pi(x_1, x_2, x_3) = (x_1, x_2) for any (x_1, x_2, x_3) \\in \\mathbb{R}^3 . In this case the null space of \\pi is seen to be \\text{ker}(\\mathsf\\pi) = \\{(0,0,a)\\,|\\,a \\in \\mathbb{R}\\}. Note that \\text{dim}(\\text{ker}(\\mathsf\\pi)) = 1 . The range of \\mathsf\\pi is easily seen to be the whole of \\mathbb{R}^2 . We therefore obtain the relation \\text{dim}(\\text{img}(\\mathsf\\pi)) = 2 . From these results, we see that \\text{dim}(\\mathbb{R}^3) = \\text{dim}(\\text{ker}(\\mathsf\\pi)) + \\text{dim}(\\text{ker}(\\mathsf\\pi)) = 3, thereby verifying the rank-nullity theorem. If a linear map \\mathsf{T}:V \\to W is a bijection, then \\mathsf{T} is called a vector space isomorphism , or just an isomorphism . In this case V and W are said to be isomorphic - written as V \\cong W . Example The map \\mathsf{S}:\\mathbb{R}^2 \\to \\mathbb{R}^2 defined as \\mathsf{S}(x_1, x_2) = (x_2, x_1), for any (x_1, x_2) \\in \\mathbb{R}^2 , is easily seen to be an isomorphism on \\mathbb{R}^2 . Example Consider the set \\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) of all real-valued polynomials of one real variable of degree less than or equal to two: \\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) = \\{p:\\mathbb{R}\\to\\mathbb{R}\\,|\\,\\text{for any }x \\in \\mathbb{R},\\;p(x) = a_0 + a_1 x + a_2 x^2 \\text{ for some }a_0, a_1, a_2 \\in \\mathbb{R}\\}. It is easy to check that \\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) is a real vector space with addition and scalar multiplication defined as follows: given p(x) = a_0 + a_1x + a_2x^2 , q(x) = b_0 + b_1x + b_2x^2 , and c \\in \\mathbb{R} , (p + c\\cdot q) \\in P_2(\\mathbb{R},\\mathbb{R}) is defined as (p + cq)(x) = p(x) + c\\cdot q(x) = (a_0 + b_0) + (a_1 + b_1)x + (a_2 + b_2)x^2. Consider now the following map \\iota:\\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) \\to \\mathbb{R}^3 defined as follows: for any p \\in \\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) such that for any x \\in \\mathbb{R} , p(x) = a_0 + a_1x + a_2x^2 , \\iota(p) = (a_0, a_1, a_2). It is easy to check that \\iota:\\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) \\to \\mathbb{R}^3 is indeed an isomorphism. Finally, if \\mathsf{T}:V \\to W is a vector space isomorphism, then \\mathsf{T}^{-1}:W \\to V is also a linear map. To show this note that for any \\mathsf{u}, \\mathsf{v} \\in W , and a, b \\in \\mathbb{R} , \\begin{split} \\mathsf{T}^{-1}(a\\mathsf{u} + b\\mathsf{v}) &= \\mathsf{T}^{-1}(a\\mathsf{T}\\tilde{\\mathsf{u}} + b\\mathsf{T}\\tilde{\\mathsf{v}})\\\\ &= \\mathsf{T}^{-1}(\\mathsf{T}(a\\tilde{\\mathsf{u}} + b\\tilde{\\mathsf{v}}))\\\\ &= a\\tilde{\\mathsf{u}} + b\\tilde{\\mathsf{v}}\\\\ &= a \\mathsf{T}^{-1}\\mathsf{u} + b \\mathsf{T}^{-1}\\mathsf{v}. \\end{split} Here, \\tilde{\\mathsf{u}}, \\tilde{\\mathsf{v}} \\in V are the unique elements of V such that \\mathsf{T}\\tilde{\\mathsf{u}} = \\mathsf{u} and \\mathsf{T}\\tilde{\\mathsf{v}} = \\mathsf{v} . Note that the existence and uniqueness of these vectors follows from the fact that \\mathsf{T} is invertible. Representation of linear maps Let us now specialize the discussion to the case when V and W are finite dimensional inner product spaces of dimension m and n , respectively. For notational simplicity, we will denote the inner products in both V and W with the same symbol \\cdot , with the meaning assumed to be evident from the context. Let \\mathsf{T}:V \\to W be a linear map from V into W , and let (\\mathsf{e}_1, \\ldots, \\mathsf{e}_m) and (\\mathsf{f}_1, \\ldots, \\mathsf{f}_n) be orthonormal bases of V and W , respectively. Then, for any \\mathsf{v} \\in V , we see from the linearity of \\mathsf{T} that \\mathsf{T}\\mathsf{v} = \\mathsf{T}\\left(\\sum_{i=1}^m v_i \\mathsf{e}_i\\right) = \\sum_{i=1}^m v_i \\,\\mathsf{T}\\mathsf{e}_i. Since \\mathsf{T}\\mathsf{e}_i \\in W , we can express it in terms of the basis (\\mathsf{f}_1, \\ldots, \\mathsf{f}_n) of W as \\mathsf{T}\\mathsf{e}_i = \\sum_{j=1}^n T_{ji} \\mathsf{f}_j, for some constants T_{ji} \\in \\mathbb{R} for every 1 \\le i \\le m and 1 \\le j \\le n . Note the order in which the indices are placed! We can now exploit the availability of an inner product in both V and W to see that T_{ij} = \\mathsf{f}_i \\cdot \\mathsf{T}\\mathsf{e}_j, \\quad 1 \\le i \\le n, \\; 1 \\le j \\le m. We now collect together all the constants T_{ij} as an n \\times m matrix [\\mathsf{T}] whose (i,j)^{\\text{th}} entry is [\\mathsf{T}]_{ij} = T_{ij} . The elements T_{ij} are called the components of \\mathsf{T} with respect to the chosen bases. The matrix [\\mathsf{T}] is called the matrix representation of \\mathsf{T}:V \\to W with respect to the bases (\\mathsf{e}_i) of V and (\\mathsf{f}_i) of W . Remark Occasionally, the notation [\\mathsf{T}]_{(\\mathsf{e}_i)}^{(\\mathsf{f}_i)} will be used to denote the matrix corresponding to a given linear map \\mathsf{T}:V \\to W with respect to bases (\\mathsf{e}_i) of V and (\\mathsf{f}_i) of W . When the choice of bases is evident from the context, the simpler notation [\\mathsf{T}] will also be used. Example Let us revisit the linear map \\mathsf\\pi:\\mathbb{R}^3 \\to \\mathbb{R}^2 . With respect to the standard basis of \\mathbb{R}^3 and \\mathbb{R}^2 , \\mathsf\\pi has the following matrix representation: = \\begin{bmatrix} 1 & 0 & 0\\\\ 0 & 1 & 0 \\end{bmatrix}, as can be checked easily with a simple calculation. To see the advantage of representing a linear map as a matrix, let \\mathsf{w} = \\mathsf{T}\\mathsf{v} \\in W be the effect of the action of \\mathsf{T}:V \\to W on \\mathsf{v} \\in V . We can write this equation in component form , with respect to the orthonormal bases (\\mathsf{e}_1, \\ldots, \\mathsf{e}_m) and (\\mathsf{f}_1, \\ldots, \\mathsf{f}_n) of V and W , respectively, as \\sum w_i \\mathsf{f}_i = \\mathsf{w} = \\mathsf{T}\\mathsf{v} = \\sum v_j \\mathsf{T}\\mathsf{e}_j = \\sum T_{ij} v_j \\mathsf{f}_i \\quad\\Rightarrow\\quad w_i = \\sum T_{ij}v_j. In matrix notation, the component form of the equation \\mathsf{w} = \\mathsf{T}\\mathsf{v} reads \\begin{bmatrix} w_1\\\\ \\vdots\\\\ w_n \\end{bmatrix} = \\begin{bmatrix} T_{11} & \\ldots & T_{1m}\\\\ \\vdots & \\ddots & \\vdots\\\\ T_{n1} & \\ldots & T_{nm} \\end{bmatrix} \\begin{bmatrix} v_1\\\\ \\vdots\\\\ v_m \\end{bmatrix}. We thus see that \\mathsf{w} = \\mathsf{T}\\mathsf{v} \\quad\\Rightarrow\\quad [\\mathsf{w}] = [\\mathsf{T}][\\mathsf{v}]. The special choice of the placements of indices for T_{ij} is done so as to ensure a neat matrix equation of the form [\\mathsf{w}] = [\\mathsf{T}][\\mathsf{v}] for the components of the various quantities with respect to chosen bases. Remark The foregoing discussion can be summarized through the following commutative diagram , Commutative diagram illustrating the basis representation of linear maps Notice how we have interpreted the matrix [\\mathsf{T}] as a map of the form [\\mathsf{T}]:\\mathbb{R}^m \\to \\mathbb{R}^n that acts on a column vector of size m to produce a column vector of size n . This map is called the representation of \\mathsf{T} with respect to the chosen bases. Let us now consider the case when V and W are equipped with general bases. Given two general bases (\\mathsf{f}_i) and (\\mathsf{g}_i) of V and W , respectively, the linear map \\mathsf{T}:V \\to W can be represented as follows: fo any \\mathsf{u} \\in V , note that \\mathsf{T}\\mathsf{u} = \\mathsf{T}\\left(\\sum_{i=1}^n u_i \\mathsf{f}_i\\right) = \\sum u_i \\mathsf{T}\\mathsf{f}_i. Since \\mathsf{T}\\mathsf{f}_i \\in W , it can be expressed in terms of the basis (\\mathsf{g}_i) of W as \\mathsf{T}\\mathsf{f}_j = \\sum_{j=1}^m T_{ij} \\mathsf{g}_i \\quad\\Rightarrow\\quad T_{ij} = \\mathsf{g}^i \\cdot \\mathsf{T}\\mathsf{f}_j. Defining \\mathsf{v} = \\mathsf{T}\\mathsf{u} , it follows that \\mathsf{v} = \\mathsf{T}\\mathsf{u} = \\sum T_{ij} u_j \\mathsf{g}_i \\qquad\\Rightarrow\\quad v_i = \\sum T_{ij} u_j, where \\mathsf{v} = \\sum v_i \\mathsf{g}_i . Notice the similarity and difference with the corresponding expression for the components of [\\mathsf{T}] with respect to orthonormal bases in V and W . Example Let \\mathcal{P}_2([-1,1],\\mathbb{R}) and \\mathcal{P}_1([-1,1],\\mathbb{R}) represent real-valued polynomials of orders 2 and 1 , respectively, defined on the closed interval [-1,1]\\subseteq\\mathbb{R} . Given any p, q \\in \\mathcal{P}_2([-1,1],\\mathbb{R}) , define their inner product as p \\cdot q = \\int_{-1}^1 p(x)q(x) \\,dx. The inner product on \\mathcal{P}_1([-1,1],\\mathbb{R}) is similarly defined. Consider the bases (e_1, e_2, e_3) and (f_1, f_2) of \\mathcal{P}_2([-1,1],\\mathbb{R}) and \\mathcal{P}_1([-1,1],\\mathbb{R}) , respectively, defined as follows: for any x \\in [-1,1] , e_1(x) = f_1(x) = \\frac{1}{\\sqrt{2}}, \\quad e_2(x) = f_2(x) = \\sqrt{\\frac{3}{2}}x, \\quad e_3(x) = \\frac{\\sqrt{5}}{2\\sqrt{2}}(3x^2 - 1). It is not hard to check that (e_i) and (f_i) are orthonormal bases of \\mathcal{P}_2([-1,1],\\mathbb{R}) and \\mathcal{P}_1([-1,1],\\mathbb{R}) , respectively. Remark The basis (e_1, e_2, e_3) is obtained by applying the Gram-Schmidt orthogonalization procedure to the basis (\\tilde{e}_1, \\tilde{e}_2, \\tilde{e}_3) defined as follows: for any x \\in [-1,1] , \\tilde{e}_1(x) = 1 , \\tilde{e}_2(x) = x , and \\tilde{e}_3(x) = x^2 . The reader is invited to verify this. Let us now consider the map D:\\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) \\to \\mathcal{P}_1(\\mathbb{R},\\mathbb{R}) as follows: for any p \\in \\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) defined as x \\in \\mathbb{R} \\mapsto p(x) = a_0 + a_1x + a_2x^2 , Dp(x) = \\frac{dp(x)}{dx} = a_1 + 2a_2x. It is left as an easy exercise to verify that D:\\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) \\to \\mathcal{P}_1(\\mathbb{R},\\mathbb{R}) is a linear map. We can work out the representation [D] of D with respect to the orthonormal bases defined earlier as follows: [D]_{ij} = f_i \\cdot De_j , for i = 1,2 and j = 1,2,3 . Doing the computation, we see that = \\begin{bmatrix} 0 & \\sqrt{3} & 0\\\\ 0 & 0 & \\sqrt{15} \\end{bmatrix} As an illustration, the computation of [D]_{22} and [D]_{23} are shown below: \\begin{split} [D]_{22} &= \\int_{-1}^1 f_2(x) De_2(x)\\,dx = \\int_{-1}^1 \\frac{3}{2}x \\, dx = 0,\\\\ [D]_{23} &= \\int_{-1}^1 f_2(x) De_3(x)\\,dx = \\int_{-1}^1 \\frac{3}{2}\\sqrt{15}\\,x^2\\,dx = \\sqrt{15}. \\end{split} The other components are computed similarly. To check that this representation is valid, let us verify that if p \\in \\mathcal{P}_2([-1,1],\\mathbb{R}) and q = Dp \\in \\mathcal{P}_1([-1,1],\\mathbb{R}) , then [q] = [D][p] . Suppose, without loss of generality, that for any x \\in [-1,1] , p(x) = a_0 + a_1 x + a_2 x^2 , for some a_0, a_1, a_2 \\in \\mathbb{R} . We can compute the representation of p with respect to the orthonormal basis (e_i) of \\mathcal{P}_2([-1,1],\\mathbb{R}) as follows: p = \\sum b_i e_i, \\quad b_i = p \\cdot e_i. A straightforward calculation shows that p = \\left(\\sqrt{2}a_0 + \\frac{2}{3\\sqrt{2}}a_2\\right) e_1 + \\sqrt{\\frac{2}{3}}a_1 e_2+ \\frac{4}{3\\sqrt{10}}a_2 e_3, \\quad\\Rightarrow\\quad [p] = \\begin{bmatrix}\\left(\\sqrt{2}a_0 + \\frac{2}{3\\sqrt{2}}a_2\\right)\\\\ \\sqrt{\\frac{2}{3}}a_1\\\\ \\frac{4}{3\\sqrt{10}}a_2\\end{bmatrix}. It is likewise easy to compute the representation of Dp with respect to the basis (f_1, f_2) of \\mathcal{P}_1([-1,1],\\mathbb{R}) as Dp = \\sum c_i f_i, \\;\\; c_i = Dp \\cdot f_i, \\quad\\Rightarrow\\quad [Dp] = \\begin{bmatrix}\\sqrt{2}a_1\\\\ \\frac{2\\sqrt{2}}{\\sqrt{3}}a_2 \\end{bmatrix}. We therefore verify by simple matrix multiplication that [Dp] = [D][p] : \\begin{bmatrix}\\sqrt{2}a_1\\\\ \\frac{2\\sqrt{2}}{\\sqrt{3}}a_2 \\end{bmatrix} = \\begin{bmatrix} 0 & \\sqrt{3} & 0\\\\ 0 & 0 & \\sqrt{15} \\end{bmatrix} \\begin{bmatrix}\\left(\\sqrt{2}a_0 + \\frac{2}{3\\sqrt{2}}a_2\\right)\\\\ \\sqrt{\\frac{2}{3}}a_1\\\\ \\frac{4}{3\\sqrt{10}}a_2\\end{bmatrix}. Remark When learning linear algebra for the first time, it is strongly recommended to work out all the details of this example step by step since it covers many of the concepts introduced earlier. Given three vector spaces U, V, W , let us consider the successive action of two linear maps \\mathsf{T}:U \\to V and \\mathsf{S}:V \\to W on a vector \\mathsf{u} \\in U . We define the product map \\mathsf{ST}:U \\to W as (\\mathsf{ST})(\\mathsf{u}) = \\mathsf{S}(\\mathsf{T}\\mathsf{u}) . It can be shown that this translates to = [\\mathsf{S}][\\mathsf{T}] in matrix notation, with respect to any choice of bases in U, V, W . To see this, let (\\mathsf{e}_i), (\\mathsf{f}_i), (\\mathsf{g}_i) be general bases of U, V, W , respectively. Then, for any \\mathsf{u} \\in U , we see that \\begin{split} \\mathsf{ST}\\mathsf{u} &= \\mathsf{S}\\left(\\mathsf{T}\\left(\\sum u_i \\mathsf{e}_i\\right)\\right)\\\\ &= \\mathsf{S}\\left(\\sum T_{ij} u_j \\mathsf{f}_i\\right)\\\\ &= \\sum S_{ij} T_{jk} u_k \\mathsf{g}_i. \\end{split} The matrix product in the right hand side of this equation thus corresponds to the familiar matrix multiplication. Combining the expression just derived with the definition [\\mathsf{ST}]_{ij} = \\mathsf{g}_i \\cdot \\mathsf{ST}\\mathsf{e}_j , we see that _{ij} = \\sum S_{ik}T_{kj} = ([\\mathsf{S}][\\mathsf{T}])_{ij} \\quad\\Rightarrow\\quad [\\mathsf{ST}] = [\\mathsf{S}][\\mathsf{T}]. In fact, the rationale for defining matrix multiplication in the specific way it is defined is to ensure that the matrix representation of the product map is the product of the matrix representations of the individual maps. Remark Some authors write \\mathsf{S} \\cdot \\mathsf{T} for the product map which we denote as \\mathsf{ST} \\equiv \\mathsf{S} \\circ \\mathsf{T} . We will reserve the \\cdot symbol almost exclusively for the inner product, and hence will write the product of \\mathsf{S} and \\mathsf{T} as \\mathsf{ST} . It\u2019s a good idea to be conscious of the specific notational choices whenever you consult other references. Change of basis for linear maps The representation of a linear map \\mathsf{T}:V \\to W between finite dimensional inner product spaces V and W , of dimension m and n , respectively, as a map [\\mathsf{T}]:\\mathbb{R}^m \\to \\mathbb{R}^n depends on the choice of bases for both V and W . Specifically, recall that if (\\mathsf{f}_i) and (\\mathsf{g}_i) are general bases of V and W , respectively, then the matrix representation of \\mathsf{T} is computed using the relation T_{ij} = \\mathsf{g}^i \\cdot \\mathsf{T} \\mathsf{f}_j. Suppose now that (\\tilde{\\mathsf{f}}_i) and (\\tilde{\\mathsf{g}}_i) are another choice of general bases for V and W , respectively. Then the components of the matrix representation of \\mathsf{T} , written \\tilde{T}_{ij} , are computed as \\tilde{T}_{ij} = \\tilde{\\mathsf{g}}^i \\cdot \\mathsf{T}\\tilde{\\mathsf{f}}_j. The equations relating \\tilde{T}_ij and T_{ij} are now worked. Though these calculations take a much simpler form with respect to a choice of orthonormal bases for both V and W , the slightly more involved case involving general bases is presented below as a good algebraic exercise. Let the new bases (\\tilde{\\mathsf{f}}_i) of V and (\\tilde{\\mathsf{g}}_i) of W depend on the old bases (\\mathsf{f}_i) of V and (\\mathsf{g}_i) of W as \\tilde{\\mathsf{f}}_i = \\sum A_{ji}\\mathsf{f}_j, \\qquad \\tilde{\\mathsf{g}}_i = \\sum B_{ji} \\mathsf{g}_j, where A_{ij} = \\mathsf{f}^i \\cdot \\tilde{\\mathsf{f}}_j and B_{ij} = \\mathsf{g}^i \\cdot \\tilde{\\mathsf{g}}_j . It follows then from an easy computation that \\begin{split} \\tilde{T}_{ij} &= \\tilde{\\mathsf{g}}^i \\cdot \\mathsf{T}\\tilde{\\mathsf{f}}_j\\\\ &= \\sum \\tilde{g}^{ik}\\tilde{\\mathsf{g}}_k \\cdot \\mathsf{T}\\tilde{\\mathsf{f}}_j\\\\ &= \\sum \\tilde{g}^{ik}B_{ck}\\,\\mathsf{g}_c \\cdot \\mathsf{T}\\mathsf{f}_b \\,A_{bj}\\\\ &= \\sum \\tilde{g}^{ik}B_{ck}g_{ca}\\,\\mathsf{g}^a \\cdot \\mathsf{T}\\mathsf{f}_b \\,A_{bj}\\\\ &= \\sum \\tilde{g}^{ik}B_{ck}g_{ca}T_{ab}A_{bj}, \\end{split} where g_{ij} = \\mathsf{g}_i \\cdot \\mathsf{g}_j and \\tilde{g}^{ij} = \\tilde{\\mathsf{g}}^i \\cdot \\tilde{\\mathsf{g}}^j . Noting that \\begin{split} \\sum \\tilde{g}^{ik}B_{ck}g_{ca} &= \\sum g_{ac}B_{ck}\\tilde{g}^{ki}\\\\ &= \\sum g_{ac} \\mathsf{g}^c \\cdot \\tilde{\\mathsf{g}}_k \\tilde{g}^{ki}\\\\ &= \\mathsf{g}_a \\cdot \\tilde{\\mathsf{g}}^i, \\end{split} and that \\tilde{\\mathsf{g}}_i = \\sum B_{ji}\\mathsf{g}_j \\quad\\Rightarrow\\quad \\mathsf{g}_i = \\sum B^{-1}_{ji}\\tilde{\\mathsf{g}}_j \\quad\\Rightarrow\\quad B^{-1}_{ij} = \\mathsf{g}_j \\cdot \\tilde{\\mathsf{g}}^i, it follows at once that \\tilde{T}_{ij} = \\sum B^{-1}_{ia} T_{ab} A_{bj}. Using matrix notation, we can write the foregoing equation in matrix form as _{(\\tilde{\\mathsf{f}}_i)}^{(\\tilde{\\mathsf{g}}_i)} = \\mathsf{B}^{-1}\\;[\\mathsf{T}]_{(\\mathsf{f}_i)}^{(\\mathsf{g}_i)}\\;\\mathsf{A}, where \\mathsf{A} and \\mathsf{B} are the matrices whose (i,j)^{\\text{th}} component is A_{ij} and B_{ij} , respectively. Remark The foregoing discussion is summarized in the following commutative diagram: Commutative diagram illustrating change of basis rules for linear maps Notice how the commutative diagram neatly summarizes the change in the representation of a linear map upon change of bases in the domain and codomain vector spaces. In the special case when all the bases are orthonormal, the transformation matrices are orthogonal. In this case, we can write the transformation rule as follows: [\\mathsf{T}]_{(\\tilde{\\mathsf{f}}_i)}^{(\\tilde{\\mathsf{g}}_i)} = \\mathsf{B}^T\\;[\\mathsf{T}]_{(\\mathsf{f}_i)}^{(\\mathsf{g}_i)} \\; \\mathsf{A}. This special case will turn out to be very useful in later applications. Example TO DO Change of basis for linear maps. WS 1. Problem 2. Tensor product basis for L(V,W) Suppose that V and W are inner product spaces of dimension m and n , respectively. Let us focus on the set L(V,W) , L(V,W) = \\{\\mathsf{T}:V \\to W \\,|\\, \\mathsf{T} \\text{ is linear }\\}, of all linear maps from V into W . We will now show that the set of all linear maps L(V,W) from V into W is also a vector space, and study a particularly useful basis called the tensor product basis for L(V,W) . Defining addition +:L(V,W) \\times L(V,W) \\to L(V,W) and scalar multiplication \\cdot:\\mathbb{R} \\times L(V,W) \\to L(V,W) as (\\mathsf{S} + \\mathsf{T})(\\mathsf{u}) = \\mathsf{S}\\mathsf{u} + \\mathsf{T}\\mathsf{u}, \\quad (a\\mathsf{T})(\\mathsf{u}) = a \\, \\mathsf{T}\\mathsf{u}, for any \\mathsf{S}, \\mathsf{T} \\in L(V,W) and \\mathsf{u} \\in V , it is easy to check that the set L(V,W) has the structure of a real vector space - the two operations introduced above satisfy all the axioms of a real vector space listed earlier. Remark It is possible to define a norm on L(V,W) as \\lVert \\mathsf{T} \\rVert = \\text{sup}_{\\lVert \\mathsf{v} \\rVert = 1} \\lVert \\mathsf{T}\\mathsf{v} \\rVert , for any \\mathsf{T} \\in L(V,W) and \\mathsf{v} \\in V . This is also called the sup norm . It is easy to show that \\lVert \\mathsf{T}\\mathsf{v} \\rVert \\le \\lVert \\mathsf{T} \\rVert \\lVert \\mathsf{v} \\rVert for any \\mathsf{T} \\in L(V,W) and \\mathsf{v} \\in V . An inner product for L(V,W) will be introduced later using the trace of a linear map. What is the dimension of L(V,W) ? To answer this question, it is helpful to introduce the notion of a tensor product map . Given vectors \\mathsf{v} \\in V and \\mathsf{w} \\in W , the tensor product map \\mathsf{w} \\otimes \\mathsf{v} \\in L(V,W) is defined as follows: for any \\mathsf{u} \\in V , (\\mathsf{w} \\otimes \\mathsf{v})(\\mathsf{u}) = (\\mathsf{v} \\cdot \\mathsf{u}) \\mathsf{w}. It is easily checked that this is in fact a linear map. It is convenient to consider first the special case when V and W are equipped with orthonormal bases. Let (\\mathsf{f}_i)_{i=1}^m and (\\mathsf{g}_i)_{i=1}^n be orthonormal bases of V and W , respectively. Let us study the mn linear maps \\mathsf{g}_i \\otimes \\mathsf{f}_j:V \\to W, for every 1 \\le i \\le n and 1 \\le j \\le m . Note that for any \\mathsf{v} = \\sum v_i \\mathsf{f}_i \\in V , (\\mathsf{g}_i \\otimes \\mathsf{f}_j)(\\mathsf{v}) = (\\mathsf{v} \\cdot \\mathsf{f}_j) \\mathsf{g}_i = v_j \\mathsf{g}_i. It is easily checked that the mn maps \\mathsf{g}_i \\otimes \\mathsf{f}_j \\in L(V,W) are linearly independent. Indeed, if for real numbers \\{a_{ij}\\} , where 1 \\le i \\le n and 1 \\le j \\le m , it is the case that \\sum a_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}_j = 0, then, for every 1 \\le k \\le m , \\sum a_{ij} (\\mathsf{g}_i \\otimes \\mathsf{f}_j)(\\mathsf{f}_k) = 0 \\quad\\Rightarrow\\quad \\sum a_{ik} \\mathsf{g}_i = 0 \\quad\\Rightarrow\\quad a_{ik} = 0. This shows that the mn linear maps \\{\\mathsf{g}_i \\otimes \\mathsf{f}_j\\} are linearly independent. Notice how the orthonormality of the basis (\\mathsf{f}_i) of V and the linear independence of the basis (\\mathsf{g}_i) of W is used in proving this. Suppose now that \\mathsf{T} \\in L(V,W) is any linear map. Then for any \\mathsf{v} = \\sum v_i \\mathsf{f}_i \\in V , we have \\mathsf{T}\\mathsf{v} = \\sum T_{ij} v_j \\mathsf{g}_i = \\sum T_{ij} (\\mathsf{g}_i \\otimes \\mathsf{f}_j)(\\mathsf{v}) = \\left(\\sum T_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}_j\\right)\\mathsf{v}. Since this is true for any \\mathsf{v} \\in V , we get the following identity: \\mathsf{T} = \\sum T_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}_j. This informs us that \\text{span}(\\{\\mathsf{g}_i \\otimes \\mathsf{f}_j\\}) = L(V,W) . The preceding two facts show that the mn maps (\\mathsf{g}_i \\otimes \\mathsf{f}_j) indeed form a basis of L(V,W) , called the tensor product basis of L(V,W) . Since there are mn such maps, we see that the dimension of L(V,W) is mn : \\text{dim}(L(V,W)) = \\text{dim}(V) \\text{dim}(W). Thus, L(V,W) is a finite dimensional vector space with dimension equal to the product of the dimensions of V and W . Remark The change of basis rule derived earlier for the components of a linear map can also be derived using the tensor product representation. It is a simple exercise to verify this. Let us briefly look at the representation of any \\mathsf{T} \\in L(V,W) with respect to general bases (\\mathsf{f}_i) of V and (\\mathsf{g}_i) . Given any any \\mathsf{v} \\in V , \\begin{split} \\mathsf{T}\\mathsf{v} &= \\sum T_{ij} v_j \\mathsf{g}_i\\\\ &= \\sum T_{ij} (\\mathsf{f}^j \\cdot \\mathsf{v}) \\mathsf{g}_i\\\\ &= \\sum T_{ij} (\\mathsf{g}_i \\otimes \\mathsf{f}^j)(\\mathsf{v}). \\end{split} Since this is true for any \\mathsf{v} \\in V , it is evident that \\mathsf{T} = \\sum T_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}^j. Notice how the reciprocal basis shows up when using general bases. Remark Note that the linear map \\mathsf{T}:V \\to W can be represented in a number of equivalent ways with respect to general bases (\\mathsf{f}_i) of V and (\\mathsf{g}_i) of W as follows: \\mathsf{T} = \\sum T_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}^j = \\sum T_{ij}f^{jk} \\mathsf{g}_i \\otimes \\mathsf{f}_k = \\sum g_{ki}T_{ij}\\mathsf{g}^k \\otimes \\mathsf{f}^j = \\sum g_{ki}T_{ij}f^{jl} \\mathsf{g}^k \\otimes \\mathsf{f}_l. The default representation will be chosen in these notes as \\mathsf{T} = \\sum T_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}^j , but this is merely a matter of convention. Transpose of a linear map Given a linear map \\mathsf{T}:V \\to W between finite dimensional inner product spaces, we will now construct an important linear map, \\mathsf{T}^T:W \\to V , called the transpose of \\mathsf{T} as follows: for any \\mathsf{v} \\in V and \\mathsf{w} \\in W , \\mathsf{T}^T\\mathsf{w} \\cdot \\mathsf{v} = \\mathsf{w} \\cdot \\mathsf{T}\\mathsf{v}. To get a handle on this definition and relate it to the more elementary notion of the transpose of a matrix, let us consider the representation of \\mathsf{T}^T with respect to orthonormal bases of V and W . Given an orthonormal basis (\\mathsf{f}_i) of V and (\\mathsf{g}_i) of W , we can easily compute the representation \\mathsf{T}^T with respect to these bases as follows: if \\mathsf{v} = \\sum v_i \\mathsf{f}_i and \\mathsf{w} = \\sum w_j \\mathsf{g}_j , then \\sum T^T_{ij} w_j v_i = T_{ji} w_j v_i \\quad\\Rightarrow\\quad T^T_{ij} = T_{ji}. Here T_{ij} = \\mathsf{g}_i \\cdot \\mathsf{T}\\mathsf{f}_j and T^T_{ij} = \\mathsf{f}_i \\cdot \\mathsf{T}^T\\mathsf{g}_j . In matrix notation, this amounts to the following equation: = [\\mathsf{T}]^T. We thus recover the familiar expression for the transpose of a matrix. The fact that [\\mathsf{T}^T]_{ij} = T_{ji} also leads to the following representation of \\mathsf{T}^T \\in L(W,V) with respect to the bases (\\mathsf{f}_i) and (\\mathsf{g}_i) of V and W , respectively: \\begin{split} \\mathsf{T}^T &= \\sum T^T_{ij} \\mathsf{f}_i \\otimes \\mathsf{g}_j,\\\\ &= \\sum T_{ji} \\mathsf{f}_i \\otimes \\mathsf{g}_j. \\end{split} In the special case of the linear map \\mathsf{w} \\otimes \\mathsf{v} \\in L(V,W) , where \\mathsf{v} \\in V and \\mathsf{w} \\in W , we see from this equation that (\\mathsf{w} \\otimes \\mathsf{v})^T = \\sum [\\mathsf{w} \\otimes \\mathsf{v}]_{ji} \\mathsf{f}_i \\otimes \\mathsf{g}_j = \\sum (v_i \\mathsf{f}_i) \\otimes (w_j \\mathsf{g}_j) = \\mathsf{v} \\otimes \\mathsf{w}. Note that the equation (\\mathsf{w} \\otimes \\mathsf{v})^T = (\\mathsf{v} \\otimes \\mathsf{w}) is valid in general, even though orthonormal bases were used used to prove the fact. The reason is that the bases do not appear in the final form of this equation. Alternatively, the fact that (\\mathsf{w} \\otimes \\mathsf{v})^T = (\\mathsf{v} \\otimes \\mathsf{w}) can be directly checked using the definition of the transpose. Let us now compute the representation of the transpose of a linear map \\mathsf{T}:V \\to W with respect to general bases (\\mathsf{f}_i) for V and (\\mathsf{g}_i) for W . With respect to these choice of basis, we have: \\begin{split} \\left(\\sum v_i \\mathsf{f}_i\\right) \\cdot \\mathsf{T}^T\\left(\\sum w_j \\mathsf{g}_j\\right) &= \\mathsf{T}\\left(\\sum v_i \\mathsf{f}_i\\right) \\cdot \\left(\\sum w_j \\mathsf{g}_j\\right),\\\\ \\sum v_i w_j \\mathsf{f}_i \\cdot \\mathsf{T}^T\\mathsf{g}_j &= \\sum v_i w_j \\mathsf{T}\\mathsf{f}_i\\cdot \\mathsf{g}_j,\\\\ \\sum v_i w_j g_{il} \\mathsf{f}^l \\cdot \\mathsf{T}^T\\mathsf{g}_j &= \\sum v_i w_j g_{kj} \\mathsf{g}^k \\cdot \\mathsf{T}\\mathsf{f}_i,\\\\ \\sum v_i w_j g_{il} T^T_{lj} = \\sum v_i w_j g_{kj} T_{ki}. \\end{split} Since this equation holds true for any choice of v_i and w_j , it follows immediately that \\sum g_{ik} T^T_{kj} = \\sum g_{jk}T_{ki}. In the special case when the bases (\\mathsf{f}_i) of V and (\\mathsf{g}_i) of W are orthonormal, the use of the relations f_{ij} = \\delta_{ij} and g_{ij} = \\delta_{ij} yields the familiar expression for the components of the transpose: T^T_{ij} = T_{ji} . The properties of the transpose of a linear map parallel that of the transpose of matrices. For instance, if \\mathsf{S}, \\mathsf{T} \\in L(V, W) , and a \\in \\mathbb{R} , then (\\mathsf{S} + \\mathsf{T})^T = \\mathsf{S}^T + \\mathsf{T}^T, \\qquad (a\\mathsf{S})^T = a\\,\\mathsf{S}^T. Given linear maps \\mathsf{S}:V \\to W and \\mathsf{T}:U \\to V , we have the following relation: (\\mathsf{S}\\mathsf{T})^T = \\mathsf{T}^T \\mathsf{S}^T. These properties are easy consequences of the definition of the transpose - it is left as an exercise to verify these claims.","title":"Linear Maps - I"},{"location":"linear_maps_1/#basic-definitions","text":"Let us first consider the case when V and W are real vector spaces, not necessarily endowed with an inner product. We call a map of the form \\mathsf{T}:V \\to W a vector space homomorphism , or more simply a linear map , if, for any \\mathsf{u}, \\mathsf{v} \\in V , and for any a, b \\in \\mathbb{R} , \\mathsf{T}(a\\mathsf{u} + b\\mathsf{v}) = a\\mathsf{T}(\\mathsf{u}) + b \\mathsf{T}(\\mathsf{v}). It is conventional to write \\mathsf{T}(\\mathsf{v}) as just \\mathsf{T}\\mathsf{v} when \\mathsf{T} is a linear map. Notice how linear maps preserve the linear structure: the action of a linear map on a linear combination of vectors is the linear combination of the action of the linear map on the individual vectors. Remark Note that, in the definition above, the vector addition and scalar multiplication in the term (a\\mathsf{u} + b\\mathsf{v}) are those defined in V , while the ones in the term a\\mathsf{T}(\\mathsf{u}) + b\\mathsf{T}(\\mathsf{v}) are those in W . The same notation is used just to keep the notation simple. Example Consider the map \\mathsf\\pi:\\mathbb{R}^3 \\to \\mathbb{R}^2 defined as follows: for any (x_1, x_2, x_3) \\in \\mathbb{R}^3 , \\mathsf\\pi(x_1, x_2, x_3) = (x_1, x_2). It is easy to verify that \\mathsf\\pi:\\mathbb{R}^3 \\to \\mathbb{R}^2 is a linear map. We will now present a few definitions associated with linear maps that are useful in practice. The kernel of the linear map \\mathsf{T}:V \\to W is the set of all elements in V that are mapped to \\mathsf{0} \\in W by \\mathsf{T} : \\text{ker}(\\mathsf{T}) = \\{\\mathsf{v}\\in V \\,|\\, \\mathsf{T}\\mathsf{v} = \\mathsf{0}\\}. \\text{ker}(\\mathsf{T}) is also called the null space of \\mathsf{T} , and is a linear subspace of V . Note that the kernel \\mathsf{T} is always non-empty since \\mathsf{0} \\in \\text{ker}(T) . The image of the linear map \\mathsf{T}:V \\to W between two vector spaces V and W , written as \\text{img}(\\mathsf{T}) is defined as the set of all those elements in W which are obtained by the action of \\mathsf{T} on some element of V : \\text{img}(\\mathsf{T}) = \\{\\mathsf{w} \\in W \\,|\\,\\text{for some }\\mathsf{v} \\in V,\\,\\mathsf{T}\\mathsf{v} = \\mathsf{w}\\}. The image of a linear map is a linear subspace of its codomain. The dimension of the image of the linear map \\mathsf{T}:V \\to W is called the rank of \\mathsf{T} . The following result holds for all linear maps \\mathsf{T}:V \\to W between finite dimensional vector spaces: \\text{dim}(V) = \\text{dim}(\\text{ker}(\\mathsf{T})) + \\text{dim}(\\text{img}(\\mathsf{T})). This is known as the rank-nullity theorem . Example Consider the map \\mathsf\\pi:\\mathbb{R}^3 \\to \\mathbb{R}^2 that was defined earlier as \\mathsf\\pi(x_1, x_2, x_3) = (x_1, x_2) for any (x_1, x_2, x_3) \\in \\mathbb{R}^3 . In this case the null space of \\pi is seen to be \\text{ker}(\\mathsf\\pi) = \\{(0,0,a)\\,|\\,a \\in \\mathbb{R}\\}. Note that \\text{dim}(\\text{ker}(\\mathsf\\pi)) = 1 . The range of \\mathsf\\pi is easily seen to be the whole of \\mathbb{R}^2 . We therefore obtain the relation \\text{dim}(\\text{img}(\\mathsf\\pi)) = 2 . From these results, we see that \\text{dim}(\\mathbb{R}^3) = \\text{dim}(\\text{ker}(\\mathsf\\pi)) + \\text{dim}(\\text{ker}(\\mathsf\\pi)) = 3, thereby verifying the rank-nullity theorem. If a linear map \\mathsf{T}:V \\to W is a bijection, then \\mathsf{T} is called a vector space isomorphism , or just an isomorphism . In this case V and W are said to be isomorphic - written as V \\cong W . Example The map \\mathsf{S}:\\mathbb{R}^2 \\to \\mathbb{R}^2 defined as \\mathsf{S}(x_1, x_2) = (x_2, x_1), for any (x_1, x_2) \\in \\mathbb{R}^2 , is easily seen to be an isomorphism on \\mathbb{R}^2 . Example Consider the set \\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) of all real-valued polynomials of one real variable of degree less than or equal to two: \\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) = \\{p:\\mathbb{R}\\to\\mathbb{R}\\,|\\,\\text{for any }x \\in \\mathbb{R},\\;p(x) = a_0 + a_1 x + a_2 x^2 \\text{ for some }a_0, a_1, a_2 \\in \\mathbb{R}\\}. It is easy to check that \\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) is a real vector space with addition and scalar multiplication defined as follows: given p(x) = a_0 + a_1x + a_2x^2 , q(x) = b_0 + b_1x + b_2x^2 , and c \\in \\mathbb{R} , (p + c\\cdot q) \\in P_2(\\mathbb{R},\\mathbb{R}) is defined as (p + cq)(x) = p(x) + c\\cdot q(x) = (a_0 + b_0) + (a_1 + b_1)x + (a_2 + b_2)x^2. Consider now the following map \\iota:\\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) \\to \\mathbb{R}^3 defined as follows: for any p \\in \\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) such that for any x \\in \\mathbb{R} , p(x) = a_0 + a_1x + a_2x^2 , \\iota(p) = (a_0, a_1, a_2). It is easy to check that \\iota:\\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) \\to \\mathbb{R}^3 is indeed an isomorphism. Finally, if \\mathsf{T}:V \\to W is a vector space isomorphism, then \\mathsf{T}^{-1}:W \\to V is also a linear map. To show this note that for any \\mathsf{u}, \\mathsf{v} \\in W , and a, b \\in \\mathbb{R} , \\begin{split} \\mathsf{T}^{-1}(a\\mathsf{u} + b\\mathsf{v}) &= \\mathsf{T}^{-1}(a\\mathsf{T}\\tilde{\\mathsf{u}} + b\\mathsf{T}\\tilde{\\mathsf{v}})\\\\ &= \\mathsf{T}^{-1}(\\mathsf{T}(a\\tilde{\\mathsf{u}} + b\\tilde{\\mathsf{v}}))\\\\ &= a\\tilde{\\mathsf{u}} + b\\tilde{\\mathsf{v}}\\\\ &= a \\mathsf{T}^{-1}\\mathsf{u} + b \\mathsf{T}^{-1}\\mathsf{v}. \\end{split} Here, \\tilde{\\mathsf{u}}, \\tilde{\\mathsf{v}} \\in V are the unique elements of V such that \\mathsf{T}\\tilde{\\mathsf{u}} = \\mathsf{u} and \\mathsf{T}\\tilde{\\mathsf{v}} = \\mathsf{v} . Note that the existence and uniqueness of these vectors follows from the fact that \\mathsf{T} is invertible.","title":"Basic definitions"},{"location":"linear_maps_1/#representation-of-linear-maps","text":"Let us now specialize the discussion to the case when V and W are finite dimensional inner product spaces of dimension m and n , respectively. For notational simplicity, we will denote the inner products in both V and W with the same symbol \\cdot , with the meaning assumed to be evident from the context. Let \\mathsf{T}:V \\to W be a linear map from V into W , and let (\\mathsf{e}_1, \\ldots, \\mathsf{e}_m) and (\\mathsf{f}_1, \\ldots, \\mathsf{f}_n) be orthonormal bases of V and W , respectively. Then, for any \\mathsf{v} \\in V , we see from the linearity of \\mathsf{T} that \\mathsf{T}\\mathsf{v} = \\mathsf{T}\\left(\\sum_{i=1}^m v_i \\mathsf{e}_i\\right) = \\sum_{i=1}^m v_i \\,\\mathsf{T}\\mathsf{e}_i. Since \\mathsf{T}\\mathsf{e}_i \\in W , we can express it in terms of the basis (\\mathsf{f}_1, \\ldots, \\mathsf{f}_n) of W as \\mathsf{T}\\mathsf{e}_i = \\sum_{j=1}^n T_{ji} \\mathsf{f}_j, for some constants T_{ji} \\in \\mathbb{R} for every 1 \\le i \\le m and 1 \\le j \\le n . Note the order in which the indices are placed! We can now exploit the availability of an inner product in both V and W to see that T_{ij} = \\mathsf{f}_i \\cdot \\mathsf{T}\\mathsf{e}_j, \\quad 1 \\le i \\le n, \\; 1 \\le j \\le m. We now collect together all the constants T_{ij} as an n \\times m matrix [\\mathsf{T}] whose (i,j)^{\\text{th}} entry is [\\mathsf{T}]_{ij} = T_{ij} . The elements T_{ij} are called the components of \\mathsf{T} with respect to the chosen bases. The matrix [\\mathsf{T}] is called the matrix representation of \\mathsf{T}:V \\to W with respect to the bases (\\mathsf{e}_i) of V and (\\mathsf{f}_i) of W . Remark Occasionally, the notation [\\mathsf{T}]_{(\\mathsf{e}_i)}^{(\\mathsf{f}_i)} will be used to denote the matrix corresponding to a given linear map \\mathsf{T}:V \\to W with respect to bases (\\mathsf{e}_i) of V and (\\mathsf{f}_i) of W . When the choice of bases is evident from the context, the simpler notation [\\mathsf{T}] will also be used. Example Let us revisit the linear map \\mathsf\\pi:\\mathbb{R}^3 \\to \\mathbb{R}^2 . With respect to the standard basis of \\mathbb{R}^3 and \\mathbb{R}^2 , \\mathsf\\pi has the following matrix representation: = \\begin{bmatrix} 1 & 0 & 0\\\\ 0 & 1 & 0 \\end{bmatrix}, as can be checked easily with a simple calculation. To see the advantage of representing a linear map as a matrix, let \\mathsf{w} = \\mathsf{T}\\mathsf{v} \\in W be the effect of the action of \\mathsf{T}:V \\to W on \\mathsf{v} \\in V . We can write this equation in component form , with respect to the orthonormal bases (\\mathsf{e}_1, \\ldots, \\mathsf{e}_m) and (\\mathsf{f}_1, \\ldots, \\mathsf{f}_n) of V and W , respectively, as \\sum w_i \\mathsf{f}_i = \\mathsf{w} = \\mathsf{T}\\mathsf{v} = \\sum v_j \\mathsf{T}\\mathsf{e}_j = \\sum T_{ij} v_j \\mathsf{f}_i \\quad\\Rightarrow\\quad w_i = \\sum T_{ij}v_j. In matrix notation, the component form of the equation \\mathsf{w} = \\mathsf{T}\\mathsf{v} reads \\begin{bmatrix} w_1\\\\ \\vdots\\\\ w_n \\end{bmatrix} = \\begin{bmatrix} T_{11} & \\ldots & T_{1m}\\\\ \\vdots & \\ddots & \\vdots\\\\ T_{n1} & \\ldots & T_{nm} \\end{bmatrix} \\begin{bmatrix} v_1\\\\ \\vdots\\\\ v_m \\end{bmatrix}. We thus see that \\mathsf{w} = \\mathsf{T}\\mathsf{v} \\quad\\Rightarrow\\quad [\\mathsf{w}] = [\\mathsf{T}][\\mathsf{v}]. The special choice of the placements of indices for T_{ij} is done so as to ensure a neat matrix equation of the form [\\mathsf{w}] = [\\mathsf{T}][\\mathsf{v}] for the components of the various quantities with respect to chosen bases. Remark The foregoing discussion can be summarized through the following commutative diagram , Commutative diagram illustrating the basis representation of linear maps Notice how we have interpreted the matrix [\\mathsf{T}] as a map of the form [\\mathsf{T}]:\\mathbb{R}^m \\to \\mathbb{R}^n that acts on a column vector of size m to produce a column vector of size n . This map is called the representation of \\mathsf{T} with respect to the chosen bases. Let us now consider the case when V and W are equipped with general bases. Given two general bases (\\mathsf{f}_i) and (\\mathsf{g}_i) of V and W , respectively, the linear map \\mathsf{T}:V \\to W can be represented as follows: fo any \\mathsf{u} \\in V , note that \\mathsf{T}\\mathsf{u} = \\mathsf{T}\\left(\\sum_{i=1}^n u_i \\mathsf{f}_i\\right) = \\sum u_i \\mathsf{T}\\mathsf{f}_i. Since \\mathsf{T}\\mathsf{f}_i \\in W , it can be expressed in terms of the basis (\\mathsf{g}_i) of W as \\mathsf{T}\\mathsf{f}_j = \\sum_{j=1}^m T_{ij} \\mathsf{g}_i \\quad\\Rightarrow\\quad T_{ij} = \\mathsf{g}^i \\cdot \\mathsf{T}\\mathsf{f}_j. Defining \\mathsf{v} = \\mathsf{T}\\mathsf{u} , it follows that \\mathsf{v} = \\mathsf{T}\\mathsf{u} = \\sum T_{ij} u_j \\mathsf{g}_i \\qquad\\Rightarrow\\quad v_i = \\sum T_{ij} u_j, where \\mathsf{v} = \\sum v_i \\mathsf{g}_i . Notice the similarity and difference with the corresponding expression for the components of [\\mathsf{T}] with respect to orthonormal bases in V and W . Example Let \\mathcal{P}_2([-1,1],\\mathbb{R}) and \\mathcal{P}_1([-1,1],\\mathbb{R}) represent real-valued polynomials of orders 2 and 1 , respectively, defined on the closed interval [-1,1]\\subseteq\\mathbb{R} . Given any p, q \\in \\mathcal{P}_2([-1,1],\\mathbb{R}) , define their inner product as p \\cdot q = \\int_{-1}^1 p(x)q(x) \\,dx. The inner product on \\mathcal{P}_1([-1,1],\\mathbb{R}) is similarly defined. Consider the bases (e_1, e_2, e_3) and (f_1, f_2) of \\mathcal{P}_2([-1,1],\\mathbb{R}) and \\mathcal{P}_1([-1,1],\\mathbb{R}) , respectively, defined as follows: for any x \\in [-1,1] , e_1(x) = f_1(x) = \\frac{1}{\\sqrt{2}}, \\quad e_2(x) = f_2(x) = \\sqrt{\\frac{3}{2}}x, \\quad e_3(x) = \\frac{\\sqrt{5}}{2\\sqrt{2}}(3x^2 - 1). It is not hard to check that (e_i) and (f_i) are orthonormal bases of \\mathcal{P}_2([-1,1],\\mathbb{R}) and \\mathcal{P}_1([-1,1],\\mathbb{R}) , respectively. Remark The basis (e_1, e_2, e_3) is obtained by applying the Gram-Schmidt orthogonalization procedure to the basis (\\tilde{e}_1, \\tilde{e}_2, \\tilde{e}_3) defined as follows: for any x \\in [-1,1] , \\tilde{e}_1(x) = 1 , \\tilde{e}_2(x) = x , and \\tilde{e}_3(x) = x^2 . The reader is invited to verify this. Let us now consider the map D:\\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) \\to \\mathcal{P}_1(\\mathbb{R},\\mathbb{R}) as follows: for any p \\in \\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) defined as x \\in \\mathbb{R} \\mapsto p(x) = a_0 + a_1x + a_2x^2 , Dp(x) = \\frac{dp(x)}{dx} = a_1 + 2a_2x. It is left as an easy exercise to verify that D:\\mathcal{P}_2(\\mathbb{R},\\mathbb{R}) \\to \\mathcal{P}_1(\\mathbb{R},\\mathbb{R}) is a linear map. We can work out the representation [D] of D with respect to the orthonormal bases defined earlier as follows: [D]_{ij} = f_i \\cdot De_j , for i = 1,2 and j = 1,2,3 . Doing the computation, we see that = \\begin{bmatrix} 0 & \\sqrt{3} & 0\\\\ 0 & 0 & \\sqrt{15} \\end{bmatrix} As an illustration, the computation of [D]_{22} and [D]_{23} are shown below: \\begin{split} [D]_{22} &= \\int_{-1}^1 f_2(x) De_2(x)\\,dx = \\int_{-1}^1 \\frac{3}{2}x \\, dx = 0,\\\\ [D]_{23} &= \\int_{-1}^1 f_2(x) De_3(x)\\,dx = \\int_{-1}^1 \\frac{3}{2}\\sqrt{15}\\,x^2\\,dx = \\sqrt{15}. \\end{split} The other components are computed similarly. To check that this representation is valid, let us verify that if p \\in \\mathcal{P}_2([-1,1],\\mathbb{R}) and q = Dp \\in \\mathcal{P}_1([-1,1],\\mathbb{R}) , then [q] = [D][p] . Suppose, without loss of generality, that for any x \\in [-1,1] , p(x) = a_0 + a_1 x + a_2 x^2 , for some a_0, a_1, a_2 \\in \\mathbb{R} . We can compute the representation of p with respect to the orthonormal basis (e_i) of \\mathcal{P}_2([-1,1],\\mathbb{R}) as follows: p = \\sum b_i e_i, \\quad b_i = p \\cdot e_i. A straightforward calculation shows that p = \\left(\\sqrt{2}a_0 + \\frac{2}{3\\sqrt{2}}a_2\\right) e_1 + \\sqrt{\\frac{2}{3}}a_1 e_2+ \\frac{4}{3\\sqrt{10}}a_2 e_3, \\quad\\Rightarrow\\quad [p] = \\begin{bmatrix}\\left(\\sqrt{2}a_0 + \\frac{2}{3\\sqrt{2}}a_2\\right)\\\\ \\sqrt{\\frac{2}{3}}a_1\\\\ \\frac{4}{3\\sqrt{10}}a_2\\end{bmatrix}. It is likewise easy to compute the representation of Dp with respect to the basis (f_1, f_2) of \\mathcal{P}_1([-1,1],\\mathbb{R}) as Dp = \\sum c_i f_i, \\;\\; c_i = Dp \\cdot f_i, \\quad\\Rightarrow\\quad [Dp] = \\begin{bmatrix}\\sqrt{2}a_1\\\\ \\frac{2\\sqrt{2}}{\\sqrt{3}}a_2 \\end{bmatrix}. We therefore verify by simple matrix multiplication that [Dp] = [D][p] : \\begin{bmatrix}\\sqrt{2}a_1\\\\ \\frac{2\\sqrt{2}}{\\sqrt{3}}a_2 \\end{bmatrix} = \\begin{bmatrix} 0 & \\sqrt{3} & 0\\\\ 0 & 0 & \\sqrt{15} \\end{bmatrix} \\begin{bmatrix}\\left(\\sqrt{2}a_0 + \\frac{2}{3\\sqrt{2}}a_2\\right)\\\\ \\sqrt{\\frac{2}{3}}a_1\\\\ \\frac{4}{3\\sqrt{10}}a_2\\end{bmatrix}. Remark When learning linear algebra for the first time, it is strongly recommended to work out all the details of this example step by step since it covers many of the concepts introduced earlier. Given three vector spaces U, V, W , let us consider the successive action of two linear maps \\mathsf{T}:U \\to V and \\mathsf{S}:V \\to W on a vector \\mathsf{u} \\in U . We define the product map \\mathsf{ST}:U \\to W as (\\mathsf{ST})(\\mathsf{u}) = \\mathsf{S}(\\mathsf{T}\\mathsf{u}) . It can be shown that this translates to = [\\mathsf{S}][\\mathsf{T}] in matrix notation, with respect to any choice of bases in U, V, W . To see this, let (\\mathsf{e}_i), (\\mathsf{f}_i), (\\mathsf{g}_i) be general bases of U, V, W , respectively. Then, for any \\mathsf{u} \\in U , we see that \\begin{split} \\mathsf{ST}\\mathsf{u} &= \\mathsf{S}\\left(\\mathsf{T}\\left(\\sum u_i \\mathsf{e}_i\\right)\\right)\\\\ &= \\mathsf{S}\\left(\\sum T_{ij} u_j \\mathsf{f}_i\\right)\\\\ &= \\sum S_{ij} T_{jk} u_k \\mathsf{g}_i. \\end{split} The matrix product in the right hand side of this equation thus corresponds to the familiar matrix multiplication. Combining the expression just derived with the definition [\\mathsf{ST}]_{ij} = \\mathsf{g}_i \\cdot \\mathsf{ST}\\mathsf{e}_j , we see that _{ij} = \\sum S_{ik}T_{kj} = ([\\mathsf{S}][\\mathsf{T}])_{ij} \\quad\\Rightarrow\\quad [\\mathsf{ST}] = [\\mathsf{S}][\\mathsf{T}]. In fact, the rationale for defining matrix multiplication in the specific way it is defined is to ensure that the matrix representation of the product map is the product of the matrix representations of the individual maps. Remark Some authors write \\mathsf{S} \\cdot \\mathsf{T} for the product map which we denote as \\mathsf{ST} \\equiv \\mathsf{S} \\circ \\mathsf{T} . We will reserve the \\cdot symbol almost exclusively for the inner product, and hence will write the product of \\mathsf{S} and \\mathsf{T} as \\mathsf{ST} . It\u2019s a good idea to be conscious of the specific notational choices whenever you consult other references.","title":"Representation of linear maps"},{"location":"linear_maps_1/#change-of-basis-for-linear-maps","text":"The representation of a linear map \\mathsf{T}:V \\to W between finite dimensional inner product spaces V and W , of dimension m and n , respectively, as a map [\\mathsf{T}]:\\mathbb{R}^m \\to \\mathbb{R}^n depends on the choice of bases for both V and W . Specifically, recall that if (\\mathsf{f}_i) and (\\mathsf{g}_i) are general bases of V and W , respectively, then the matrix representation of \\mathsf{T} is computed using the relation T_{ij} = \\mathsf{g}^i \\cdot \\mathsf{T} \\mathsf{f}_j. Suppose now that (\\tilde{\\mathsf{f}}_i) and (\\tilde{\\mathsf{g}}_i) are another choice of general bases for V and W , respectively. Then the components of the matrix representation of \\mathsf{T} , written \\tilde{T}_{ij} , are computed as \\tilde{T}_{ij} = \\tilde{\\mathsf{g}}^i \\cdot \\mathsf{T}\\tilde{\\mathsf{f}}_j. The equations relating \\tilde{T}_ij and T_{ij} are now worked. Though these calculations take a much simpler form with respect to a choice of orthonormal bases for both V and W , the slightly more involved case involving general bases is presented below as a good algebraic exercise. Let the new bases (\\tilde{\\mathsf{f}}_i) of V and (\\tilde{\\mathsf{g}}_i) of W depend on the old bases (\\mathsf{f}_i) of V and (\\mathsf{g}_i) of W as \\tilde{\\mathsf{f}}_i = \\sum A_{ji}\\mathsf{f}_j, \\qquad \\tilde{\\mathsf{g}}_i = \\sum B_{ji} \\mathsf{g}_j, where A_{ij} = \\mathsf{f}^i \\cdot \\tilde{\\mathsf{f}}_j and B_{ij} = \\mathsf{g}^i \\cdot \\tilde{\\mathsf{g}}_j . It follows then from an easy computation that \\begin{split} \\tilde{T}_{ij} &= \\tilde{\\mathsf{g}}^i \\cdot \\mathsf{T}\\tilde{\\mathsf{f}}_j\\\\ &= \\sum \\tilde{g}^{ik}\\tilde{\\mathsf{g}}_k \\cdot \\mathsf{T}\\tilde{\\mathsf{f}}_j\\\\ &= \\sum \\tilde{g}^{ik}B_{ck}\\,\\mathsf{g}_c \\cdot \\mathsf{T}\\mathsf{f}_b \\,A_{bj}\\\\ &= \\sum \\tilde{g}^{ik}B_{ck}g_{ca}\\,\\mathsf{g}^a \\cdot \\mathsf{T}\\mathsf{f}_b \\,A_{bj}\\\\ &= \\sum \\tilde{g}^{ik}B_{ck}g_{ca}T_{ab}A_{bj}, \\end{split} where g_{ij} = \\mathsf{g}_i \\cdot \\mathsf{g}_j and \\tilde{g}^{ij} = \\tilde{\\mathsf{g}}^i \\cdot \\tilde{\\mathsf{g}}^j . Noting that \\begin{split} \\sum \\tilde{g}^{ik}B_{ck}g_{ca} &= \\sum g_{ac}B_{ck}\\tilde{g}^{ki}\\\\ &= \\sum g_{ac} \\mathsf{g}^c \\cdot \\tilde{\\mathsf{g}}_k \\tilde{g}^{ki}\\\\ &= \\mathsf{g}_a \\cdot \\tilde{\\mathsf{g}}^i, \\end{split} and that \\tilde{\\mathsf{g}}_i = \\sum B_{ji}\\mathsf{g}_j \\quad\\Rightarrow\\quad \\mathsf{g}_i = \\sum B^{-1}_{ji}\\tilde{\\mathsf{g}}_j \\quad\\Rightarrow\\quad B^{-1}_{ij} = \\mathsf{g}_j \\cdot \\tilde{\\mathsf{g}}^i, it follows at once that \\tilde{T}_{ij} = \\sum B^{-1}_{ia} T_{ab} A_{bj}. Using matrix notation, we can write the foregoing equation in matrix form as _{(\\tilde{\\mathsf{f}}_i)}^{(\\tilde{\\mathsf{g}}_i)} = \\mathsf{B}^{-1}\\;[\\mathsf{T}]_{(\\mathsf{f}_i)}^{(\\mathsf{g}_i)}\\;\\mathsf{A}, where \\mathsf{A} and \\mathsf{B} are the matrices whose (i,j)^{\\text{th}} component is A_{ij} and B_{ij} , respectively. Remark The foregoing discussion is summarized in the following commutative diagram: Commutative diagram illustrating change of basis rules for linear maps Notice how the commutative diagram neatly summarizes the change in the representation of a linear map upon change of bases in the domain and codomain vector spaces. In the special case when all the bases are orthonormal, the transformation matrices are orthogonal. In this case, we can write the transformation rule as follows: [\\mathsf{T}]_{(\\tilde{\\mathsf{f}}_i)}^{(\\tilde{\\mathsf{g}}_i)} = \\mathsf{B}^T\\;[\\mathsf{T}]_{(\\mathsf{f}_i)}^{(\\mathsf{g}_i)} \\; \\mathsf{A}. This special case will turn out to be very useful in later applications. Example TO DO Change of basis for linear maps. WS 1. Problem 2.","title":"Change of basis for linear maps"},{"location":"linear_maps_1/#tensor-product-basis-for-lvw","text":"Suppose that V and W are inner product spaces of dimension m and n , respectively. Let us focus on the set L(V,W) , L(V,W) = \\{\\mathsf{T}:V \\to W \\,|\\, \\mathsf{T} \\text{ is linear }\\}, of all linear maps from V into W . We will now show that the set of all linear maps L(V,W) from V into W is also a vector space, and study a particularly useful basis called the tensor product basis for L(V,W) . Defining addition +:L(V,W) \\times L(V,W) \\to L(V,W) and scalar multiplication \\cdot:\\mathbb{R} \\times L(V,W) \\to L(V,W) as (\\mathsf{S} + \\mathsf{T})(\\mathsf{u}) = \\mathsf{S}\\mathsf{u} + \\mathsf{T}\\mathsf{u}, \\quad (a\\mathsf{T})(\\mathsf{u}) = a \\, \\mathsf{T}\\mathsf{u}, for any \\mathsf{S}, \\mathsf{T} \\in L(V,W) and \\mathsf{u} \\in V , it is easy to check that the set L(V,W) has the structure of a real vector space - the two operations introduced above satisfy all the axioms of a real vector space listed earlier. Remark It is possible to define a norm on L(V,W) as \\lVert \\mathsf{T} \\rVert = \\text{sup}_{\\lVert \\mathsf{v} \\rVert = 1} \\lVert \\mathsf{T}\\mathsf{v} \\rVert , for any \\mathsf{T} \\in L(V,W) and \\mathsf{v} \\in V . This is also called the sup norm . It is easy to show that \\lVert \\mathsf{T}\\mathsf{v} \\rVert \\le \\lVert \\mathsf{T} \\rVert \\lVert \\mathsf{v} \\rVert for any \\mathsf{T} \\in L(V,W) and \\mathsf{v} \\in V . An inner product for L(V,W) will be introduced later using the trace of a linear map. What is the dimension of L(V,W) ? To answer this question, it is helpful to introduce the notion of a tensor product map . Given vectors \\mathsf{v} \\in V and \\mathsf{w} \\in W , the tensor product map \\mathsf{w} \\otimes \\mathsf{v} \\in L(V,W) is defined as follows: for any \\mathsf{u} \\in V , (\\mathsf{w} \\otimes \\mathsf{v})(\\mathsf{u}) = (\\mathsf{v} \\cdot \\mathsf{u}) \\mathsf{w}. It is easily checked that this is in fact a linear map. It is convenient to consider first the special case when V and W are equipped with orthonormal bases. Let (\\mathsf{f}_i)_{i=1}^m and (\\mathsf{g}_i)_{i=1}^n be orthonormal bases of V and W , respectively. Let us study the mn linear maps \\mathsf{g}_i \\otimes \\mathsf{f}_j:V \\to W, for every 1 \\le i \\le n and 1 \\le j \\le m . Note that for any \\mathsf{v} = \\sum v_i \\mathsf{f}_i \\in V , (\\mathsf{g}_i \\otimes \\mathsf{f}_j)(\\mathsf{v}) = (\\mathsf{v} \\cdot \\mathsf{f}_j) \\mathsf{g}_i = v_j \\mathsf{g}_i. It is easily checked that the mn maps \\mathsf{g}_i \\otimes \\mathsf{f}_j \\in L(V,W) are linearly independent. Indeed, if for real numbers \\{a_{ij}\\} , where 1 \\le i \\le n and 1 \\le j \\le m , it is the case that \\sum a_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}_j = 0, then, for every 1 \\le k \\le m , \\sum a_{ij} (\\mathsf{g}_i \\otimes \\mathsf{f}_j)(\\mathsf{f}_k) = 0 \\quad\\Rightarrow\\quad \\sum a_{ik} \\mathsf{g}_i = 0 \\quad\\Rightarrow\\quad a_{ik} = 0. This shows that the mn linear maps \\{\\mathsf{g}_i \\otimes \\mathsf{f}_j\\} are linearly independent. Notice how the orthonormality of the basis (\\mathsf{f}_i) of V and the linear independence of the basis (\\mathsf{g}_i) of W is used in proving this. Suppose now that \\mathsf{T} \\in L(V,W) is any linear map. Then for any \\mathsf{v} = \\sum v_i \\mathsf{f}_i \\in V , we have \\mathsf{T}\\mathsf{v} = \\sum T_{ij} v_j \\mathsf{g}_i = \\sum T_{ij} (\\mathsf{g}_i \\otimes \\mathsf{f}_j)(\\mathsf{v}) = \\left(\\sum T_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}_j\\right)\\mathsf{v}. Since this is true for any \\mathsf{v} \\in V , we get the following identity: \\mathsf{T} = \\sum T_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}_j. This informs us that \\text{span}(\\{\\mathsf{g}_i \\otimes \\mathsf{f}_j\\}) = L(V,W) . The preceding two facts show that the mn maps (\\mathsf{g}_i \\otimes \\mathsf{f}_j) indeed form a basis of L(V,W) , called the tensor product basis of L(V,W) . Since there are mn such maps, we see that the dimension of L(V,W) is mn : \\text{dim}(L(V,W)) = \\text{dim}(V) \\text{dim}(W). Thus, L(V,W) is a finite dimensional vector space with dimension equal to the product of the dimensions of V and W . Remark The change of basis rule derived earlier for the components of a linear map can also be derived using the tensor product representation. It is a simple exercise to verify this. Let us briefly look at the representation of any \\mathsf{T} \\in L(V,W) with respect to general bases (\\mathsf{f}_i) of V and (\\mathsf{g}_i) . Given any any \\mathsf{v} \\in V , \\begin{split} \\mathsf{T}\\mathsf{v} &= \\sum T_{ij} v_j \\mathsf{g}_i\\\\ &= \\sum T_{ij} (\\mathsf{f}^j \\cdot \\mathsf{v}) \\mathsf{g}_i\\\\ &= \\sum T_{ij} (\\mathsf{g}_i \\otimes \\mathsf{f}^j)(\\mathsf{v}). \\end{split} Since this is true for any \\mathsf{v} \\in V , it is evident that \\mathsf{T} = \\sum T_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}^j. Notice how the reciprocal basis shows up when using general bases. Remark Note that the linear map \\mathsf{T}:V \\to W can be represented in a number of equivalent ways with respect to general bases (\\mathsf{f}_i) of V and (\\mathsf{g}_i) of W as follows: \\mathsf{T} = \\sum T_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}^j = \\sum T_{ij}f^{jk} \\mathsf{g}_i \\otimes \\mathsf{f}_k = \\sum g_{ki}T_{ij}\\mathsf{g}^k \\otimes \\mathsf{f}^j = \\sum g_{ki}T_{ij}f^{jl} \\mathsf{g}^k \\otimes \\mathsf{f}_l. The default representation will be chosen in these notes as \\mathsf{T} = \\sum T_{ij} \\mathsf{g}_i \\otimes \\mathsf{f}^j , but this is merely a matter of convention.","title":"Tensor product basis for L(V,W)"},{"location":"linear_maps_1/#transpose-of-a-linear-map","text":"Given a linear map \\mathsf{T}:V \\to W between finite dimensional inner product spaces, we will now construct an important linear map, \\mathsf{T}^T:W \\to V , called the transpose of \\mathsf{T} as follows: for any \\mathsf{v} \\in V and \\mathsf{w} \\in W , \\mathsf{T}^T\\mathsf{w} \\cdot \\mathsf{v} = \\mathsf{w} \\cdot \\mathsf{T}\\mathsf{v}. To get a handle on this definition and relate it to the more elementary notion of the transpose of a matrix, let us consider the representation of \\mathsf{T}^T with respect to orthonormal bases of V and W . Given an orthonormal basis (\\mathsf{f}_i) of V and (\\mathsf{g}_i) of W , we can easily compute the representation \\mathsf{T}^T with respect to these bases as follows: if \\mathsf{v} = \\sum v_i \\mathsf{f}_i and \\mathsf{w} = \\sum w_j \\mathsf{g}_j , then \\sum T^T_{ij} w_j v_i = T_{ji} w_j v_i \\quad\\Rightarrow\\quad T^T_{ij} = T_{ji}. Here T_{ij} = \\mathsf{g}_i \\cdot \\mathsf{T}\\mathsf{f}_j and T^T_{ij} = \\mathsf{f}_i \\cdot \\mathsf{T}^T\\mathsf{g}_j . In matrix notation, this amounts to the following equation: = [\\mathsf{T}]^T. We thus recover the familiar expression for the transpose of a matrix. The fact that [\\mathsf{T}^T]_{ij} = T_{ji} also leads to the following representation of \\mathsf{T}^T \\in L(W,V) with respect to the bases (\\mathsf{f}_i) and (\\mathsf{g}_i) of V and W , respectively: \\begin{split} \\mathsf{T}^T &= \\sum T^T_{ij} \\mathsf{f}_i \\otimes \\mathsf{g}_j,\\\\ &= \\sum T_{ji} \\mathsf{f}_i \\otimes \\mathsf{g}_j. \\end{split} In the special case of the linear map \\mathsf{w} \\otimes \\mathsf{v} \\in L(V,W) , where \\mathsf{v} \\in V and \\mathsf{w} \\in W , we see from this equation that (\\mathsf{w} \\otimes \\mathsf{v})^T = \\sum [\\mathsf{w} \\otimes \\mathsf{v}]_{ji} \\mathsf{f}_i \\otimes \\mathsf{g}_j = \\sum (v_i \\mathsf{f}_i) \\otimes (w_j \\mathsf{g}_j) = \\mathsf{v} \\otimes \\mathsf{w}. Note that the equation (\\mathsf{w} \\otimes \\mathsf{v})^T = (\\mathsf{v} \\otimes \\mathsf{w}) is valid in general, even though orthonormal bases were used used to prove the fact. The reason is that the bases do not appear in the final form of this equation. Alternatively, the fact that (\\mathsf{w} \\otimes \\mathsf{v})^T = (\\mathsf{v} \\otimes \\mathsf{w}) can be directly checked using the definition of the transpose. Let us now compute the representation of the transpose of a linear map \\mathsf{T}:V \\to W with respect to general bases (\\mathsf{f}_i) for V and (\\mathsf{g}_i) for W . With respect to these choice of basis, we have: \\begin{split} \\left(\\sum v_i \\mathsf{f}_i\\right) \\cdot \\mathsf{T}^T\\left(\\sum w_j \\mathsf{g}_j\\right) &= \\mathsf{T}\\left(\\sum v_i \\mathsf{f}_i\\right) \\cdot \\left(\\sum w_j \\mathsf{g}_j\\right),\\\\ \\sum v_i w_j \\mathsf{f}_i \\cdot \\mathsf{T}^T\\mathsf{g}_j &= \\sum v_i w_j \\mathsf{T}\\mathsf{f}_i\\cdot \\mathsf{g}_j,\\\\ \\sum v_i w_j g_{il} \\mathsf{f}^l \\cdot \\mathsf{T}^T\\mathsf{g}_j &= \\sum v_i w_j g_{kj} \\mathsf{g}^k \\cdot \\mathsf{T}\\mathsf{f}_i,\\\\ \\sum v_i w_j g_{il} T^T_{lj} = \\sum v_i w_j g_{kj} T_{ki}. \\end{split} Since this equation holds true for any choice of v_i and w_j , it follows immediately that \\sum g_{ik} T^T_{kj} = \\sum g_{jk}T_{ki}. In the special case when the bases (\\mathsf{f}_i) of V and (\\mathsf{g}_i) of W are orthonormal, the use of the relations f_{ij} = \\delta_{ij} and g_{ij} = \\delta_{ij} yields the familiar expression for the components of the transpose: T^T_{ij} = T_{ji} . The properties of the transpose of a linear map parallel that of the transpose of matrices. For instance, if \\mathsf{S}, \\mathsf{T} \\in L(V, W) , and a \\in \\mathbb{R} , then (\\mathsf{S} + \\mathsf{T})^T = \\mathsf{S}^T + \\mathsf{T}^T, \\qquad (a\\mathsf{S})^T = a\\,\\mathsf{S}^T. Given linear maps \\mathsf{S}:V \\to W and \\mathsf{T}:U \\to V , we have the following relation: (\\mathsf{S}\\mathsf{T})^T = \\mathsf{T}^T \\mathsf{S}^T. These properties are easy consequences of the definition of the transpose - it is left as an exercise to verify these claims.","title":"Transpose of a linear map"},{"location":"linear_maps_2/","text":"We now discuss certain additional topics related to linear maps that are useful in the study of continuum mechanics. Important invariants of linear maps The fact that the set of all volume forms on a finite dimensional inner product space is itself a vector space of dimension 1 permits a basis-independent means to define a variety of useful functions on linear maps. Two such functions, the determinant and trace of a linear map are discussed now. These functions are called invariants of linear maps since their definitions are independent of the choice of a basis. Determinant Given a linear map \\mathsf{T} \\in L(V,V) , where V is a finite dimensional inner product space of dimension n , the determinant of \\mathsf{T} , written \\text{det}(\\mathsf{T}) is defined as follows: for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_n \\in V , \\text{det}(\\mathsf{T}) = \\frac{\\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{u}_1, \\ldots, \\mathsf{T}\\mathsf{u}_n)}{\\mathsf{\\epsilon}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_n)}. Here, \\mathsf{\\epsilon} \\in \\Omega^n(V) is a chosen volume form on V . To see that this definition of the determinant is well-defined, note that it is possible to define a volume form \\mathsf{\\epsilon}_{\\mathsf{T}} \\in \\Omega^n(V) on V , given \\mathsf{\\epsilon} \\in \\Omega^n(V) and \\mathsf{T} \\in L(V,V) , as follows: for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_n \\in V , \\mathsf{\\epsilon}_{\\mathsf{T}}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_n) = \\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{u}_1, \\ldots, \\mathsf{T}\\mathsf{u}_n). The fact that the set of all volume forms is a vector space of dimension 1 implies that \\mathsf{\\epsilon}_{\\mathsf{T}} is a scalar multiple of \\mathsf{\\epsilon} . This scalar multiple is, in fact, defined as the determinant of \\mathsf{T} . To see how this definition is related to the determinant of a matrix encountered in elementary linear algebra, it is helpful to work in \\mathbb{R}^3 . The determinant of a linear map \\mathsf{T}:\\mathbb{R}^3 \\to \\mathbb{R}^3 is computed as follows: choosing \\mathsf{u}_1, \\mathsf{u}_2, \\mathsf{u}_3 \\in \\mathbb{R}^3 to be the standard basis (\\mathsf{e}_i) of \\mathbb{R}^3 , and \\mathsf{\\epsilon} to be the standard volume form on \\mathbb{R}^3 , \\begin{split} \\mathsf{\\epsilon}(\\mathsf{e}_i, \\mathsf{e}_j, \\mathsf{e}_k) \\text{det}(\\mathsf{T}) &= \\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{e}_i, \\mathsf{T}\\mathsf{e}_j, \\mathsf{T}\\mathsf{e}_k)\\\\ \\epsilon_{ijk} \\text{det}(\\mathsf{T}) &= \\mathsf{\\epsilon}\\left(\\sum T_{ai}\\mathsf{e}_a, \\sum T_{bj} \\mathsf{e}_b, T_{ck} \\mathsf{e}_c\\right)\\\\ &= \\sum T_{ai}T_{bj}T_{ck}\\mathsf{\\epsilon}(\\mathsf{e}_a, \\mathsf{e}_b, \\mathsf{e}_c)\\\\ &= \\sum \\epsilon_{abc} T_{ai} T_{bj} T_{ck}. \\end{split} Note that the final expression is the familiar expression for the determinant of the matrix [\\mathsf{T}] . It is a good exercise to expand this and check that it indeed reduces to the familiar expression for the determinant. To see the advantage in the abstract and basis-independent definition of the determinant provided here, consider the determinant of the product of two linear maps \\mathsf{S}, \\mathsf{T} \\in L(V,V) . Given any \\mathsf{u}_1, \\ldots, \\mathsf{u}_n \\in V , \\begin{split} \\text{det}(\\mathsf{S}\\mathsf{T}) &= \\frac{\\mathsf{\\epsilon}(\\mathsf{S}\\mathsf{T}\\mathsf{u}_1, \\ldots, \\mathsf{S}\\mathsf{T}\\mathsf{u}_n)}{\\mathsf{\\epsilon}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_n)}\\\\ &= \\frac{\\mathsf{\\epsilon}(\\mathsf{S}(\\mathsf{T}\\mathsf{u}_1), \\ldots, \\mathsf{S}(\\mathsf{T}\\mathsf{u}_n))}{\\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{u}_1, \\ldots, \\mathsf{T}\\mathsf{u}_n)} \\frac{\\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{u}_1, \\ldots, \\mathsf{T}\\mathsf{u}_n)}{\\mathsf{\\epsilon}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_n)}\\\\ &= \\text{det}(\\mathsf{S})\\text{det}(\\mathsf{T}). \\end{split} Notice how this proof of the fact that \\text{det}(\\mathsf{S}\\mathsf{T}) = \\text{det}(\\mathsf{S})\\text{det}(\\mathsf{T}) is significantly simpler than the proof in terms of the elementary definition of the determinant in terms of the components of a linear map with respect to suitable choice of bases. Trace The trace of the linear map \\mathsf{T} \\in L(V,V) , written \\text{tr}(\\mathsf{T}) , is defined as follows: for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_n \\in V , \\text{tr}(\\mathsf{T}) = \\sum_i \\frac{\\mathsf{\\epsilon}(\\mathsf{u}_1, \\ldots, \\mathsf{T}\\mathsf{e}_i, \\ldots, \\mathsf{u}_n)}{\\mathsf{\\epsilon}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_n)}. To understand this definition better, consider the special case of a linear map \\mathsf{T} \\in L(\\mathbb{R}^3,\\mathbb{R}^3) . In this case, it easily follows from the definition that \\begin{split} \\epsilon_{ijk}\\text{tr}(\\mathsf{T}) &= \\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{e}_i, \\mathsf{e}_j, \\mathsf{e}_k) + \\mathsf{\\epsilon}(\\mathsf{e}_i, \\mathsf{T}\\mathsf{e}_j, \\mathsf{e}_k) + \\mathsf{\\epsilon}(\\mathsf{e}_i, \\mathsf{e}_j, \\mathsf{T}\\mathsf{e}_k)\\\\ &= \\sum (\\epsilon_{ajk} T_{ai} + \\epsilon_{iak} T_{aj} + \\epsilon_{ija} T_{ak})\\\\ &= \\epsilon_{ijk}(T_{ii} + T_{jj} + T_{kk}). \\end{split} The last expression follows from the fact that \\epsilon_{ijk} is non-zero only when i,j,k are distinct. Note also that there is no summation over the repeated indices in the final expression. Choosing (i,j,k) = (1,2,3) the familiar expression for the trace of [\\mathsf{T}] is recovered: \\text{tr}(\\mathsf{T}) = \\sum T_{ii}. Despite the cumbersome form of the definition of the trace of a linear map adopted here, it will prove to be very convenient later on. Remark It is possible to introduce an inner product in the space L(V,W) of all linear maps from V into W as follows. Given \\mathsf{S},\\mathsf{T} \\in L(V,W) , the inner product \\cdot:L(V,W) \\times L(V,W) \\to \\mathbb{R} is defined as follows: for any \\mathsf{S},\\mathsf{T} \\in L(V,W) , \\mathsf{S} \\cdot \\mathsf{T} = \\text{tr}(\\mathsf{S}\\mathsf{T}^T). It is left as an easy exercise to verify that this is indeed an inner product on L(V,W) . Note also that \\text{tr}(\\mathsf{S}\\mathsf{T}^T) = \\text{tr}(\\mathsf{S}^T\\mathsf{T}) . Special groups of linear maps To conclude the current introductory discussion on tensor algebra, a few important class of linear maps are considered now. Throughout this discussion, V stands for an inner product space of dimension n , and attention is focused on certain special subsets of L(V,V) . Groups It is useful at this juncture to introduce the notion of a group . A set G is said to be a group if there exists a map \\odot:G \\times G \\to G that satisfies the following properties: Associativity of group operation: for any f, g, h \\in G , f \\odot (g \\odot h) = (f \\odot g) \\odot h , Existence of group identity : there exists e \\in G , called the group identity, such that for a g \\odot e = e \\odot g = g , Existence of inverse : for every g \\in G , there exists g^{-1} \\in G , called the inverse of g , such that g \\odot g^{-1} = g^{-1} \\odot g = e . If it is further the case that f \\odot g = g \\odot f for every f,g \\in G , then G is said to be a commutative group , or an Abelian group . A subset H \\subseteq G of G is said to be a sub-group of G if H is a group by itself, with respect to the same group operation. As a simple example, note that given any vector space V , V itself forms a commutative group with +:V \\times V \\to V as the group operation: for any \\mathsf{u}, \\mathsf{v} \\in V , \\mathsf{u} \\odot \\mathsf{v} = \\mathsf{u} + \\mathsf{v} . In this case \\mathsf{0} \\in V is the group identity, and for any \\mathsf{v} \\in V , -\\mathsf{v} is the inverse of \\mathsf{v} . As another example more relevant to the current discussion, consider the set M_n consisting of all invertible n \\times n matrices matrices, and define the binary map \\odot:M_n \\times M_n \\to M_n as follows: given A,B \\in M_n , A \\odot B = AB . Here, AB denotes the familiar matrix multiplication of the matrices A and B . It is easy to check that M_n is a group with respect to this binary operation. Note that M_n is not a commutative group. Additive groups of linear maps A linear map \\mathsf{T} \\in L(V,V) is said to be symmetric if \\mathsf{T}^T = \\mathsf{T} , and is said to be skew-symmetric if \\mathsf{T}^T = -\\mathsf{T} . To see the connection with the elementary definitions of symmetry and skew-symmetry, let (\\mathsf{g}_i) be an orthonormal basis of V . If \\mathsf{T} is symmetric, it follows that T^T_{ij} = \\mathsf{g}_i \\cdot \\mathsf{T}^T\\mathsf{g}_j = \\mathsf{T}\\mathsf{g}_i \\cdot \\mathsf{g}_j = T_{ji}. It follows from a similar argument that if \\mathsf{T} is skew-symmetric, then T^{T}_{ij} = -T_{ji} . The set of all symmetric linear maps on V is called the symmetric linear group on V , and written \\text{sym}(V) : \\text{sym}(V) = \\{ \\mathsf{T} \\in L(V,V) \\,|\\, \\mathsf{T}^T = \\mathsf{T} \\}. The skew-symmetric linear group on V , written \\text{skw}(V) , is similarly defined as \\text{skw}(V) = \\{ \\mathsf{T} \\in L(V,V) \\,|\\, \\mathsf{T}^T = -\\mathsf{T} \\}. The group operation for both \\text{sym}(V) and \\text{skw}(V) is the addition of linear maps. Note that both \\text{sym}(V) and \\text{skw}(V) are sub-groups of L(V,V) with respect to the group operation being the vector addition in L(V,V) . Remark There is an important relationship between skew-symmetric linear maps and the cross product, in the context of the three dimensional Euclidean space \\mathbb{R}^3 . Given any \\mathsf{v} \\in \\mathbb{R}^3 , associate with it the skew-symmetric linear map \\check{\\mathsf{v}} \\in \\text{skw}(\\mathbb{R}^3) , defined as follows: for any \\mathsf{u} \\in \\mathbb{R}^3 , \\check{\\mathsf{v}}\\mathsf{u} = \\mathsf{v} \\times \\mathsf{u}. It is straightforward to check that \\check{\\mathsf{v}} is indeed a linear map. To verify that it is skew symmetric, note that for any \\mathsf{u}, \\mathsf{w} \\in \\mathbb{R}^3 , \\begin{split} \\check{\\mathsf{v}}^T\\mathsf{u} \\cdot \\mathsf{w} &= \\mathsf{u} \\cdot \\check{\\mathsf{v}}\\mathsf{w}\\\\ &= \\mathsf{u} \\cdot (\\mathsf{v} \\times \\mathsf{w})\\\\ &= \\mathsf{\\epsilon}(\\mathsf{u}, \\mathsf{v}, \\mathsf{w})\\\\ &= -\\mathsf{\\epsilon}(\\mathsf{w}, \\mathsf{v}, \\mathsf{u})\\\\ &= -\\mathsf{w} \\cdot (\\mathsf{v} \\times \\mathsf{u})\\\\ &= -\\check{\\mathsf{v}}\\mathsf{u} \\cdot \\mathsf{w}. \\end{split} Since this is true for any \\mathsf{u}, \\mathsf{w} \\in \\mathbb{R}^3 , it follows that \\check{\\mathsf{v}}^T = -\\check{\\mathsf{v}} . The vector \\mathsf{v} is called the axial vector of the skew-symmetric linear map \\check{\\mathsf{v}} . In terms of the standard basis of \\mathbb{R}^3 , it can be verified using a simple calculation that, for any \\mathsf{v} \\in \\mathbb{R}^3 , \\check{v}_{ij} = \\epsilon_{ijk}v_k, \\qquad v_i = -\\frac{1}{2}\\epsilon_{ijk}\\check{v}_{jk}. Using matrix notation, the foregoing equations can be written as follows: = \\begin{bmatrix}v_1\\\\ v_2\\\\ v_3\\end{bmatrix}, \\qquad [\\check{\\mathsf{v}}] = \\begin{bmatrix} 0 & -v_3 & v_2\\\\ v_3 & 0 & -v_1\\\\ -v_2 & v_1 & 0\\end{bmatrix}. Note that the matrix representation of the skew-symmetric linear map \\check{\\mathsf{v}} is also skew-symmetric, as expected. A linear map \\mathsf{T} \\in L(V,V) is said to be positive definite if it is true that for any \\mathsf{v} \\in V , \\mathsf{v} \\cdot \\mathsf{T}\\mathsf{v} \\ge 0 , and \\mathsf{v} \\cdot \\mathsf{T}\\mathsf{v} = \\mathsf{0} iff \\mathsf{v} = 0 . An especially important sub-group of L(V,V) is the group \\text{sym}_+(V) , defined as \\text{sym}_+(V) = \\{ \\mathsf{T} \\in L(V,V) \\,|\\, \\mathsf{T} \\text{ is symmetric and positive-definite}\\}, of all symmetric and positive definite linear maps on V . Note that the group operation here is again addition in L(V,V) . This sub-group will turn out to be very useful in the study of continuum mechanics. Mutliplicative groups of linear maps The general linear group on V , written \\text{GL}(V) , is the set of all linear maps on V with non-zero determinant: \\text{GL}(V) = \\{\\mathsf{T} \\in L(V,V) \\,|\\, \\text{det}(\\mathsf{T}) \\neq 0\\}. The group operation, in this case, is the product of linear maps: given \\mathsf{S},\\mathsf{T} \\in \\text{GL}(V) , \\mathsf{S} \\odot \\mathsf{T} = \\mathsf{S}\\mathsf{T} . It is easy to check that \\text{GL}(V) is indeed a group: indeed, given any \\mathsf{S},\\mathsf{T} \\in \\text{GL}(V) , note that \\text{det}(\\mathsf{S}\\mathsf{T}) = \\text{det}(\\mathsf{S})\\text(\\mathsf{T}) \\neq 0 . Note that if \\mathsf{T} \\in \\text{GL}(V) , then \\mathsf{T} is invertible : this means that there exists a linear map \\mathsf{T}^{-1} \\in \\text{GL}(V) , called the inverse of \\mathsf{T} , such that \\mathsf{T} \\mathsf{T}^{-1} = \\mathsf{T}^{-1}\\mathsf{T} = \\mathsf{I}, where \\mathsf{I} \\in L(V,V) is the identity map in V . A few important sub-groups of \\text{GL}(V) are discussed next. The set of all linear maps with determinant 1 is called the special linear group on V , and written \\text{SL}(V) : \\text{SL}(V) = \\{ \\mathsf{T} \\in \\text{GL}(V) \\,|\\, \\text{det}(\\mathsf{T}) = 1\\}. A linear map \\mathsf{T} \\in \\text{GL}(V) is said to be orthogonal if the inverse of \\mathsf{T} is the transpose of \\mathsf{T} . The set of all orthogonal linear maps, written \\text{O}(V) is called the orthogonal group of V : \\text{O}(V) = \\{ \\mathsf{T} \\in \\text{GL}(V) \\,|\\, \\mathsf{T}^{-1} = \\mathsf{T}^T \\}. The determinant of an orthogonal linear map \\mathsf{T} \\in \\text{O}(V) is \\pm 1 . To see this, note that 1 = \\text{det}(I) = \\text{det}(\\mathsf{T}^{-1}\\mathsf{T}) = \\text{det}(\\mathsf{T}^T\\mathsf{T}) = (\\text{det}(\\mathsf{T}))^2 . In deriving this result, use has been made of the fact that \\text{det}(\\mathsf{T}^T) = \\text{det}(\\mathsf{T}) for any \\mathsf{T} \\in L(V,V) . The set of all orthogonal maps with determinant equal to 1 is called the special orthogonal group on V , written \\text{SO}(V) : \\text{SO}(V) = \\{\\mathsf{T} \\in \\text{GL}(V) \\,|\\, \\mathsf{T}^{-1} = \\mathsf{T}^T, \\; \\text{det}(\\mathsf{T}) = 1\\}. It is straightforward to check that \\text{SO}(V) \\subset \\text{O}(V) \\subset \\text{GL}(V) . Remark In the special case when V = \\mathbb{R}^n , it is customary to denote the various groups discussed above as \\text{GL}(n) , \\text{SL}(n) , \\text{O}(n) , and \\text{SO}(n) . Eigenvalues and Eigenvectors of linear maps To conclude the discussion of the algebraic preliminaries, a simplified introduction to the important ideas of eigenvalues and eigenvectors of a linear map are now discussed. Throughout this section, it is assumed that V is a finite dimensional inner product space of dimension n . Definition Given a linear map \\mathsf{T}:V \\to V , a vector \\mathsf{v} \\in V is called an eigenvector of \\mathsf{T} with respect to the eigenvalue \\mathsf{a} \\in \\mathbb{R} if \\mathsf{A}\\mathsf{v} = a\\mathsf{v}. Note that the zero vector \\mathsf{0} \\in V trivially satisfies this equation. It is implicitly assumed that this trivial eigenvector is excluded from the discussion. Remark It is noted that the eigenvalues can, in general be complex, and the vector space V is typically taken to be a complex vector space. Since only a special class of linear maps which admit only real eigenvalues are considered in this section, the more general theory is not developed here. The eigenvectors and eigenvalues of \\mathsf{T} \\in L(V,V) are readily computing by noting that the equation \\mathsf{T}\\mathsf{v} = a\\mathsf{v} can be written as (\\mathsf{T} - a\\mathsf{I})\\mathsf{v} = \\mathsf{0} , where \\mathsf{I} \\in L(V,V) is the identity map on V . The condition that this equation admits non-trivial solutions immediately yields the following condition: \\text{det}(\\mathsf{T} - a\\mathsf{I}) = 0. This is a polynomial equation of order n in a that can be solved to obtain the n eigenvalues (a_i) of \\mathsf{T} . Using these eigenvalues in the equations \\mathsf{A}\\mathsf{v}_i = a_i\\mathsf{v}_i , where i = 1,\\ldots,n , and solving them results in the n eigenvectors of \\mathsf{T} . Symmetric and positive definite linear maps Of special interest here are linear maps that are both symmetric and positive definite; thus, the discussion to follows focuses on linear maps \\mathsf{T} \\in \\text{sym}_+(V) . Given any \\mathsf{T} \\in \\text{sym}_+(V) , note that if \\mathsf{u}, \\mathsf{v} \\in V are eigenvectors of \\mathsf{T} corresponding to the eigenvalues a,b \\in \\mathbb{R} , respectively, it follows that b \\mathsf{u} \\cdot \\mathsf{v} = \\mathsf{u} \\cdot \\mathsf{T}\\mathsf{v} = \\mathsf{T}^T\\mathsf{u} \\cdot \\mathsf{v} = \\mathsf{T}\\mathsf{u} \\cdot \\mathsf{v} = a \\mathsf{u} \\cdot \\mathsf{v}. Thus, (b - a)\\mathsf{u} \\cdot \\mathsf{v} = 0 . This immediately shows that a \\neq b then \\mathsf{u} \\cdot \\mathsf{v} = 0 . In words, this expresses the fact that the eigenvectors of a symmetric and positive definite map corresponding to distinct eigenvalues are mutually orthogonal. Further, it follows from the positive definiteness of \\mathsf{T} that b \\lVert \\mathsf{v} \\rVert^2 = \\mathsf{v} \\cdot \\mathsf{T}\\mathsf{v} \\ge 0 \\quad\\Rightarrow\\quad b \\ge 0. Thus, the eigenvectors of a symmetric and positive definite linear map are real and positive. If (\\mathsf{v}_i)_{i=1}^n are the eigenvectors of \\mathsf{T} corresponding to the eigenvalues (a_i)_{i=1}^n , it can be shown that the eigenvectors (\\mathsf{v}_i) of T constitute a basis of V . Further, it can be checked using direct substitution that \\mathsf{T} admits the representation \\mathsf{T} = \\sum a_i \\mathsf{v}_i \\otimes \\mathsf{v}_i. This equation, which expresses the linear map \\mathsf{T} in terms of the basis of V formed by the eigenvectors of \\mathsf{T} is called the spectral representation of \\mathsf{T} . In the special case of symmetric and positive definite linear maps, the fact that its eigenvectors are positive can be used to defined various functions of linear maps. Specifically, if f:\\mathbb{R}_+ \\to \\mathbb{R} is a function that takes a positive real number and returns a real number, it can be extended to the linear space \\text{sym}_+(V) as follows: for any \\mathsf{T} \\in \\text{sym}_+(V) , define f(\\mathsf{T}) = \\sum f(a_i) \\mathsf{v}_i \\otimes \\mathsf{v}_i. As important examples of such functions, the logarithm of a symmetric and positive definite linear map \\mathsf{T} is defined as \\log \\mathsf{T} = \\sum \\log a_i \\mathsf{v}_i \\otimes \\mathsf{v}_i. Similarly, the square root of \\mathsf{T} is defined as \\sqrt{\\mathsf{T}} = \\sum \\sqrt{a_i} \\mathsf{v}_i \\otimes \\mathsf{v}_i. It can be checked with a simple calculation that \\sqrt{\\mathsf{T}}\\sqrt{\\mathsf{T}} = \\mathsf{T} , as expected. Invariants of a linear map Given a linear map \\mathsf{T} \\in L(V,V) , the equation \\text{det}(\\mathsf{T} - a\\mathsf{I}) = \\mathsf{0} is called the characteristic equation of \\mathsf{T} . When expanded, the characteristic equation takes the form (-a)^n + I_1(-a)^{n-1} + \\ldots + I_n = 0, where the set of constants \\{I_i\\}_{i=1}^n are called the invariants of the linear map \\mathsf{T} . The reason for calling them invariants is that their values do not depend on the choice of any basis for V , and are hence invariant with respect to the choice of basis. The Cayley-Hamilton theorem states that the linear map \\mathsf{T} also satisfies the characteristic equation: (-\\mathsf{T})^n + I_1(-\\mathsf{T})^{n-1} + \\ldots + I_n\\mathsf{I} = \\mathsf{0}. Here \\mathsf{T}^k is to be understood \\mathsf{T}\\mathsf{T}\\ldots\\mathsf{T} ( k factors). It is of interest to consider the case when V is a three dimensional vector space. In this case, the invariants of an invertible linear map \\mathsf{T} \\in \\text{GL}(V) can be shown to be as follows: for any \\mathsf{u},\\mathsf{v},\\mathsf{w} \\in V , \\begin{split} I_1 &= \\text{tr}(\\mathsf{T}) = \\frac{\\mathsf\\epsilon(\\mathsf{T}\\mathsf{u},\\mathsf{v},\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})} + \\frac{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{T}\\mathsf{v},\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})} + \\frac{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{T}\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})},\\\\ I_2 &= \\text{tr}(\\mathsf{T}^{-1})\\text{det}(\\mathsf{T}) = \\frac{\\mathsf\\epsilon(\\mathsf{T}\\mathsf{u},\\mathsf{T}\\mathsf{v},\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})} + \\frac{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{T}\\mathsf{v},\\mathsf{T}\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})} + \\frac{\\mathsf\\epsilon(\\mathsf{T}\\mathsf{u},\\mathsf{v},\\mathsf{T}\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})},\\\\ I_3 &= \\text{det}(\\mathsf{T}) = \\frac{\\mathsf\\epsilon(\\mathsf{T}\\mathsf{u},\\mathsf{T}\\mathsf{v},\\mathsf{T}\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})}. \\end{split} An elegant means to prove this is by applying the definition of the determinant, \\text{det}(\\mathsf{T})\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w}) = \\mathsf\\epsilon(\\mathsf{T}\\mathsf{u}, \\mathsf{T}\\mathsf{v},\\mathsf{T}\\mathsf{w}) to evaluate the determinant in the characteristic equation of \\mathsf{T} : \\text{det}(\\mathsf{T} - a\\mathsf{I}) .","title":"Linear Maps - II"},{"location":"linear_maps_2/#important-invariants-of-linear-maps","text":"The fact that the set of all volume forms on a finite dimensional inner product space is itself a vector space of dimension 1 permits a basis-independent means to define a variety of useful functions on linear maps. Two such functions, the determinant and trace of a linear map are discussed now. These functions are called invariants of linear maps since their definitions are independent of the choice of a basis.","title":"Important invariants of linear maps"},{"location":"linear_maps_2/#determinant","text":"Given a linear map \\mathsf{T} \\in L(V,V) , where V is a finite dimensional inner product space of dimension n , the determinant of \\mathsf{T} , written \\text{det}(\\mathsf{T}) is defined as follows: for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_n \\in V , \\text{det}(\\mathsf{T}) = \\frac{\\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{u}_1, \\ldots, \\mathsf{T}\\mathsf{u}_n)}{\\mathsf{\\epsilon}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_n)}. Here, \\mathsf{\\epsilon} \\in \\Omega^n(V) is a chosen volume form on V . To see that this definition of the determinant is well-defined, note that it is possible to define a volume form \\mathsf{\\epsilon}_{\\mathsf{T}} \\in \\Omega^n(V) on V , given \\mathsf{\\epsilon} \\in \\Omega^n(V) and \\mathsf{T} \\in L(V,V) , as follows: for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_n \\in V , \\mathsf{\\epsilon}_{\\mathsf{T}}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_n) = \\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{u}_1, \\ldots, \\mathsf{T}\\mathsf{u}_n). The fact that the set of all volume forms is a vector space of dimension 1 implies that \\mathsf{\\epsilon}_{\\mathsf{T}} is a scalar multiple of \\mathsf{\\epsilon} . This scalar multiple is, in fact, defined as the determinant of \\mathsf{T} . To see how this definition is related to the determinant of a matrix encountered in elementary linear algebra, it is helpful to work in \\mathbb{R}^3 . The determinant of a linear map \\mathsf{T}:\\mathbb{R}^3 \\to \\mathbb{R}^3 is computed as follows: choosing \\mathsf{u}_1, \\mathsf{u}_2, \\mathsf{u}_3 \\in \\mathbb{R}^3 to be the standard basis (\\mathsf{e}_i) of \\mathbb{R}^3 , and \\mathsf{\\epsilon} to be the standard volume form on \\mathbb{R}^3 , \\begin{split} \\mathsf{\\epsilon}(\\mathsf{e}_i, \\mathsf{e}_j, \\mathsf{e}_k) \\text{det}(\\mathsf{T}) &= \\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{e}_i, \\mathsf{T}\\mathsf{e}_j, \\mathsf{T}\\mathsf{e}_k)\\\\ \\epsilon_{ijk} \\text{det}(\\mathsf{T}) &= \\mathsf{\\epsilon}\\left(\\sum T_{ai}\\mathsf{e}_a, \\sum T_{bj} \\mathsf{e}_b, T_{ck} \\mathsf{e}_c\\right)\\\\ &= \\sum T_{ai}T_{bj}T_{ck}\\mathsf{\\epsilon}(\\mathsf{e}_a, \\mathsf{e}_b, \\mathsf{e}_c)\\\\ &= \\sum \\epsilon_{abc} T_{ai} T_{bj} T_{ck}. \\end{split} Note that the final expression is the familiar expression for the determinant of the matrix [\\mathsf{T}] . It is a good exercise to expand this and check that it indeed reduces to the familiar expression for the determinant. To see the advantage in the abstract and basis-independent definition of the determinant provided here, consider the determinant of the product of two linear maps \\mathsf{S}, \\mathsf{T} \\in L(V,V) . Given any \\mathsf{u}_1, \\ldots, \\mathsf{u}_n \\in V , \\begin{split} \\text{det}(\\mathsf{S}\\mathsf{T}) &= \\frac{\\mathsf{\\epsilon}(\\mathsf{S}\\mathsf{T}\\mathsf{u}_1, \\ldots, \\mathsf{S}\\mathsf{T}\\mathsf{u}_n)}{\\mathsf{\\epsilon}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_n)}\\\\ &= \\frac{\\mathsf{\\epsilon}(\\mathsf{S}(\\mathsf{T}\\mathsf{u}_1), \\ldots, \\mathsf{S}(\\mathsf{T}\\mathsf{u}_n))}{\\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{u}_1, \\ldots, \\mathsf{T}\\mathsf{u}_n)} \\frac{\\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{u}_1, \\ldots, \\mathsf{T}\\mathsf{u}_n)}{\\mathsf{\\epsilon}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_n)}\\\\ &= \\text{det}(\\mathsf{S})\\text{det}(\\mathsf{T}). \\end{split} Notice how this proof of the fact that \\text{det}(\\mathsf{S}\\mathsf{T}) = \\text{det}(\\mathsf{S})\\text{det}(\\mathsf{T}) is significantly simpler than the proof in terms of the elementary definition of the determinant in terms of the components of a linear map with respect to suitable choice of bases.","title":"Determinant"},{"location":"linear_maps_2/#trace","text":"The trace of the linear map \\mathsf{T} \\in L(V,V) , written \\text{tr}(\\mathsf{T}) , is defined as follows: for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_n \\in V , \\text{tr}(\\mathsf{T}) = \\sum_i \\frac{\\mathsf{\\epsilon}(\\mathsf{u}_1, \\ldots, \\mathsf{T}\\mathsf{e}_i, \\ldots, \\mathsf{u}_n)}{\\mathsf{\\epsilon}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_n)}. To understand this definition better, consider the special case of a linear map \\mathsf{T} \\in L(\\mathbb{R}^3,\\mathbb{R}^3) . In this case, it easily follows from the definition that \\begin{split} \\epsilon_{ijk}\\text{tr}(\\mathsf{T}) &= \\mathsf{\\epsilon}(\\mathsf{T}\\mathsf{e}_i, \\mathsf{e}_j, \\mathsf{e}_k) + \\mathsf{\\epsilon}(\\mathsf{e}_i, \\mathsf{T}\\mathsf{e}_j, \\mathsf{e}_k) + \\mathsf{\\epsilon}(\\mathsf{e}_i, \\mathsf{e}_j, \\mathsf{T}\\mathsf{e}_k)\\\\ &= \\sum (\\epsilon_{ajk} T_{ai} + \\epsilon_{iak} T_{aj} + \\epsilon_{ija} T_{ak})\\\\ &= \\epsilon_{ijk}(T_{ii} + T_{jj} + T_{kk}). \\end{split} The last expression follows from the fact that \\epsilon_{ijk} is non-zero only when i,j,k are distinct. Note also that there is no summation over the repeated indices in the final expression. Choosing (i,j,k) = (1,2,3) the familiar expression for the trace of [\\mathsf{T}] is recovered: \\text{tr}(\\mathsf{T}) = \\sum T_{ii}. Despite the cumbersome form of the definition of the trace of a linear map adopted here, it will prove to be very convenient later on. Remark It is possible to introduce an inner product in the space L(V,W) of all linear maps from V into W as follows. Given \\mathsf{S},\\mathsf{T} \\in L(V,W) , the inner product \\cdot:L(V,W) \\times L(V,W) \\to \\mathbb{R} is defined as follows: for any \\mathsf{S},\\mathsf{T} \\in L(V,W) , \\mathsf{S} \\cdot \\mathsf{T} = \\text{tr}(\\mathsf{S}\\mathsf{T}^T). It is left as an easy exercise to verify that this is indeed an inner product on L(V,W) . Note also that \\text{tr}(\\mathsf{S}\\mathsf{T}^T) = \\text{tr}(\\mathsf{S}^T\\mathsf{T}) .","title":"Trace"},{"location":"linear_maps_2/#special-groups-of-linear-maps","text":"To conclude the current introductory discussion on tensor algebra, a few important class of linear maps are considered now. Throughout this discussion, V stands for an inner product space of dimension n , and attention is focused on certain special subsets of L(V,V) .","title":"Special groups of linear maps"},{"location":"linear_maps_2/#groups","text":"It is useful at this juncture to introduce the notion of a group . A set G is said to be a group if there exists a map \\odot:G \\times G \\to G that satisfies the following properties: Associativity of group operation: for any f, g, h \\in G , f \\odot (g \\odot h) = (f \\odot g) \\odot h , Existence of group identity : there exists e \\in G , called the group identity, such that for a g \\odot e = e \\odot g = g , Existence of inverse : for every g \\in G , there exists g^{-1} \\in G , called the inverse of g , such that g \\odot g^{-1} = g^{-1} \\odot g = e . If it is further the case that f \\odot g = g \\odot f for every f,g \\in G , then G is said to be a commutative group , or an Abelian group . A subset H \\subseteq G of G is said to be a sub-group of G if H is a group by itself, with respect to the same group operation. As a simple example, note that given any vector space V , V itself forms a commutative group with +:V \\times V \\to V as the group operation: for any \\mathsf{u}, \\mathsf{v} \\in V , \\mathsf{u} \\odot \\mathsf{v} = \\mathsf{u} + \\mathsf{v} . In this case \\mathsf{0} \\in V is the group identity, and for any \\mathsf{v} \\in V , -\\mathsf{v} is the inverse of \\mathsf{v} . As another example more relevant to the current discussion, consider the set M_n consisting of all invertible n \\times n matrices matrices, and define the binary map \\odot:M_n \\times M_n \\to M_n as follows: given A,B \\in M_n , A \\odot B = AB . Here, AB denotes the familiar matrix multiplication of the matrices A and B . It is easy to check that M_n is a group with respect to this binary operation. Note that M_n is not a commutative group.","title":"Groups"},{"location":"linear_maps_2/#additive-groups-of-linear-maps","text":"A linear map \\mathsf{T} \\in L(V,V) is said to be symmetric if \\mathsf{T}^T = \\mathsf{T} , and is said to be skew-symmetric if \\mathsf{T}^T = -\\mathsf{T} . To see the connection with the elementary definitions of symmetry and skew-symmetry, let (\\mathsf{g}_i) be an orthonormal basis of V . If \\mathsf{T} is symmetric, it follows that T^T_{ij} = \\mathsf{g}_i \\cdot \\mathsf{T}^T\\mathsf{g}_j = \\mathsf{T}\\mathsf{g}_i \\cdot \\mathsf{g}_j = T_{ji}. It follows from a similar argument that if \\mathsf{T} is skew-symmetric, then T^{T}_{ij} = -T_{ji} . The set of all symmetric linear maps on V is called the symmetric linear group on V , and written \\text{sym}(V) : \\text{sym}(V) = \\{ \\mathsf{T} \\in L(V,V) \\,|\\, \\mathsf{T}^T = \\mathsf{T} \\}. The skew-symmetric linear group on V , written \\text{skw}(V) , is similarly defined as \\text{skw}(V) = \\{ \\mathsf{T} \\in L(V,V) \\,|\\, \\mathsf{T}^T = -\\mathsf{T} \\}. The group operation for both \\text{sym}(V) and \\text{skw}(V) is the addition of linear maps. Note that both \\text{sym}(V) and \\text{skw}(V) are sub-groups of L(V,V) with respect to the group operation being the vector addition in L(V,V) . Remark There is an important relationship between skew-symmetric linear maps and the cross product, in the context of the three dimensional Euclidean space \\mathbb{R}^3 . Given any \\mathsf{v} \\in \\mathbb{R}^3 , associate with it the skew-symmetric linear map \\check{\\mathsf{v}} \\in \\text{skw}(\\mathbb{R}^3) , defined as follows: for any \\mathsf{u} \\in \\mathbb{R}^3 , \\check{\\mathsf{v}}\\mathsf{u} = \\mathsf{v} \\times \\mathsf{u}. It is straightforward to check that \\check{\\mathsf{v}} is indeed a linear map. To verify that it is skew symmetric, note that for any \\mathsf{u}, \\mathsf{w} \\in \\mathbb{R}^3 , \\begin{split} \\check{\\mathsf{v}}^T\\mathsf{u} \\cdot \\mathsf{w} &= \\mathsf{u} \\cdot \\check{\\mathsf{v}}\\mathsf{w}\\\\ &= \\mathsf{u} \\cdot (\\mathsf{v} \\times \\mathsf{w})\\\\ &= \\mathsf{\\epsilon}(\\mathsf{u}, \\mathsf{v}, \\mathsf{w})\\\\ &= -\\mathsf{\\epsilon}(\\mathsf{w}, \\mathsf{v}, \\mathsf{u})\\\\ &= -\\mathsf{w} \\cdot (\\mathsf{v} \\times \\mathsf{u})\\\\ &= -\\check{\\mathsf{v}}\\mathsf{u} \\cdot \\mathsf{w}. \\end{split} Since this is true for any \\mathsf{u}, \\mathsf{w} \\in \\mathbb{R}^3 , it follows that \\check{\\mathsf{v}}^T = -\\check{\\mathsf{v}} . The vector \\mathsf{v} is called the axial vector of the skew-symmetric linear map \\check{\\mathsf{v}} . In terms of the standard basis of \\mathbb{R}^3 , it can be verified using a simple calculation that, for any \\mathsf{v} \\in \\mathbb{R}^3 , \\check{v}_{ij} = \\epsilon_{ijk}v_k, \\qquad v_i = -\\frac{1}{2}\\epsilon_{ijk}\\check{v}_{jk}. Using matrix notation, the foregoing equations can be written as follows: = \\begin{bmatrix}v_1\\\\ v_2\\\\ v_3\\end{bmatrix}, \\qquad [\\check{\\mathsf{v}}] = \\begin{bmatrix} 0 & -v_3 & v_2\\\\ v_3 & 0 & -v_1\\\\ -v_2 & v_1 & 0\\end{bmatrix}. Note that the matrix representation of the skew-symmetric linear map \\check{\\mathsf{v}} is also skew-symmetric, as expected. A linear map \\mathsf{T} \\in L(V,V) is said to be positive definite if it is true that for any \\mathsf{v} \\in V , \\mathsf{v} \\cdot \\mathsf{T}\\mathsf{v} \\ge 0 , and \\mathsf{v} \\cdot \\mathsf{T}\\mathsf{v} = \\mathsf{0} iff \\mathsf{v} = 0 . An especially important sub-group of L(V,V) is the group \\text{sym}_+(V) , defined as \\text{sym}_+(V) = \\{ \\mathsf{T} \\in L(V,V) \\,|\\, \\mathsf{T} \\text{ is symmetric and positive-definite}\\}, of all symmetric and positive definite linear maps on V . Note that the group operation here is again addition in L(V,V) . This sub-group will turn out to be very useful in the study of continuum mechanics.","title":"Additive groups of linear maps"},{"location":"linear_maps_2/#mutliplicative-groups-of-linear-maps","text":"The general linear group on V , written \\text{GL}(V) , is the set of all linear maps on V with non-zero determinant: \\text{GL}(V) = \\{\\mathsf{T} \\in L(V,V) \\,|\\, \\text{det}(\\mathsf{T}) \\neq 0\\}. The group operation, in this case, is the product of linear maps: given \\mathsf{S},\\mathsf{T} \\in \\text{GL}(V) , \\mathsf{S} \\odot \\mathsf{T} = \\mathsf{S}\\mathsf{T} . It is easy to check that \\text{GL}(V) is indeed a group: indeed, given any \\mathsf{S},\\mathsf{T} \\in \\text{GL}(V) , note that \\text{det}(\\mathsf{S}\\mathsf{T}) = \\text{det}(\\mathsf{S})\\text(\\mathsf{T}) \\neq 0 . Note that if \\mathsf{T} \\in \\text{GL}(V) , then \\mathsf{T} is invertible : this means that there exists a linear map \\mathsf{T}^{-1} \\in \\text{GL}(V) , called the inverse of \\mathsf{T} , such that \\mathsf{T} \\mathsf{T}^{-1} = \\mathsf{T}^{-1}\\mathsf{T} = \\mathsf{I}, where \\mathsf{I} \\in L(V,V) is the identity map in V . A few important sub-groups of \\text{GL}(V) are discussed next. The set of all linear maps with determinant 1 is called the special linear group on V , and written \\text{SL}(V) : \\text{SL}(V) = \\{ \\mathsf{T} \\in \\text{GL}(V) \\,|\\, \\text{det}(\\mathsf{T}) = 1\\}. A linear map \\mathsf{T} \\in \\text{GL}(V) is said to be orthogonal if the inverse of \\mathsf{T} is the transpose of \\mathsf{T} . The set of all orthogonal linear maps, written \\text{O}(V) is called the orthogonal group of V : \\text{O}(V) = \\{ \\mathsf{T} \\in \\text{GL}(V) \\,|\\, \\mathsf{T}^{-1} = \\mathsf{T}^T \\}. The determinant of an orthogonal linear map \\mathsf{T} \\in \\text{O}(V) is \\pm 1 . To see this, note that 1 = \\text{det}(I) = \\text{det}(\\mathsf{T}^{-1}\\mathsf{T}) = \\text{det}(\\mathsf{T}^T\\mathsf{T}) = (\\text{det}(\\mathsf{T}))^2 . In deriving this result, use has been made of the fact that \\text{det}(\\mathsf{T}^T) = \\text{det}(\\mathsf{T}) for any \\mathsf{T} \\in L(V,V) . The set of all orthogonal maps with determinant equal to 1 is called the special orthogonal group on V , written \\text{SO}(V) : \\text{SO}(V) = \\{\\mathsf{T} \\in \\text{GL}(V) \\,|\\, \\mathsf{T}^{-1} = \\mathsf{T}^T, \\; \\text{det}(\\mathsf{T}) = 1\\}. It is straightforward to check that \\text{SO}(V) \\subset \\text{O}(V) \\subset \\text{GL}(V) . Remark In the special case when V = \\mathbb{R}^n , it is customary to denote the various groups discussed above as \\text{GL}(n) , \\text{SL}(n) , \\text{O}(n) , and \\text{SO}(n) .","title":"Mutliplicative groups of linear maps"},{"location":"linear_maps_2/#eigenvalues-and-eigenvectors-of-linear-maps","text":"To conclude the discussion of the algebraic preliminaries, a simplified introduction to the important ideas of eigenvalues and eigenvectors of a linear map are now discussed. Throughout this section, it is assumed that V is a finite dimensional inner product space of dimension n .","title":"Eigenvalues and Eigenvectors of linear maps"},{"location":"linear_maps_2/#definition","text":"Given a linear map \\mathsf{T}:V \\to V , a vector \\mathsf{v} \\in V is called an eigenvector of \\mathsf{T} with respect to the eigenvalue \\mathsf{a} \\in \\mathbb{R} if \\mathsf{A}\\mathsf{v} = a\\mathsf{v}. Note that the zero vector \\mathsf{0} \\in V trivially satisfies this equation. It is implicitly assumed that this trivial eigenvector is excluded from the discussion. Remark It is noted that the eigenvalues can, in general be complex, and the vector space V is typically taken to be a complex vector space. Since only a special class of linear maps which admit only real eigenvalues are considered in this section, the more general theory is not developed here. The eigenvectors and eigenvalues of \\mathsf{T} \\in L(V,V) are readily computing by noting that the equation \\mathsf{T}\\mathsf{v} = a\\mathsf{v} can be written as (\\mathsf{T} - a\\mathsf{I})\\mathsf{v} = \\mathsf{0} , where \\mathsf{I} \\in L(V,V) is the identity map on V . The condition that this equation admits non-trivial solutions immediately yields the following condition: \\text{det}(\\mathsf{T} - a\\mathsf{I}) = 0. This is a polynomial equation of order n in a that can be solved to obtain the n eigenvalues (a_i) of \\mathsf{T} . Using these eigenvalues in the equations \\mathsf{A}\\mathsf{v}_i = a_i\\mathsf{v}_i , where i = 1,\\ldots,n , and solving them results in the n eigenvectors of \\mathsf{T} .","title":"Definition"},{"location":"linear_maps_2/#symmetric-and-positive-definite-linear-maps","text":"Of special interest here are linear maps that are both symmetric and positive definite; thus, the discussion to follows focuses on linear maps \\mathsf{T} \\in \\text{sym}_+(V) . Given any \\mathsf{T} \\in \\text{sym}_+(V) , note that if \\mathsf{u}, \\mathsf{v} \\in V are eigenvectors of \\mathsf{T} corresponding to the eigenvalues a,b \\in \\mathbb{R} , respectively, it follows that b \\mathsf{u} \\cdot \\mathsf{v} = \\mathsf{u} \\cdot \\mathsf{T}\\mathsf{v} = \\mathsf{T}^T\\mathsf{u} \\cdot \\mathsf{v} = \\mathsf{T}\\mathsf{u} \\cdot \\mathsf{v} = a \\mathsf{u} \\cdot \\mathsf{v}. Thus, (b - a)\\mathsf{u} \\cdot \\mathsf{v} = 0 . This immediately shows that a \\neq b then \\mathsf{u} \\cdot \\mathsf{v} = 0 . In words, this expresses the fact that the eigenvectors of a symmetric and positive definite map corresponding to distinct eigenvalues are mutually orthogonal. Further, it follows from the positive definiteness of \\mathsf{T} that b \\lVert \\mathsf{v} \\rVert^2 = \\mathsf{v} \\cdot \\mathsf{T}\\mathsf{v} \\ge 0 \\quad\\Rightarrow\\quad b \\ge 0. Thus, the eigenvectors of a symmetric and positive definite linear map are real and positive. If (\\mathsf{v}_i)_{i=1}^n are the eigenvectors of \\mathsf{T} corresponding to the eigenvalues (a_i)_{i=1}^n , it can be shown that the eigenvectors (\\mathsf{v}_i) of T constitute a basis of V . Further, it can be checked using direct substitution that \\mathsf{T} admits the representation \\mathsf{T} = \\sum a_i \\mathsf{v}_i \\otimes \\mathsf{v}_i. This equation, which expresses the linear map \\mathsf{T} in terms of the basis of V formed by the eigenvectors of \\mathsf{T} is called the spectral representation of \\mathsf{T} . In the special case of symmetric and positive definite linear maps, the fact that its eigenvectors are positive can be used to defined various functions of linear maps. Specifically, if f:\\mathbb{R}_+ \\to \\mathbb{R} is a function that takes a positive real number and returns a real number, it can be extended to the linear space \\text{sym}_+(V) as follows: for any \\mathsf{T} \\in \\text{sym}_+(V) , define f(\\mathsf{T}) = \\sum f(a_i) \\mathsf{v}_i \\otimes \\mathsf{v}_i. As important examples of such functions, the logarithm of a symmetric and positive definite linear map \\mathsf{T} is defined as \\log \\mathsf{T} = \\sum \\log a_i \\mathsf{v}_i \\otimes \\mathsf{v}_i. Similarly, the square root of \\mathsf{T} is defined as \\sqrt{\\mathsf{T}} = \\sum \\sqrt{a_i} \\mathsf{v}_i \\otimes \\mathsf{v}_i. It can be checked with a simple calculation that \\sqrt{\\mathsf{T}}\\sqrt{\\mathsf{T}} = \\mathsf{T} , as expected.","title":"Symmetric and positive definite linear maps"},{"location":"linear_maps_2/#invariants-of-a-linear-map","text":"Given a linear map \\mathsf{T} \\in L(V,V) , the equation \\text{det}(\\mathsf{T} - a\\mathsf{I}) = \\mathsf{0} is called the characteristic equation of \\mathsf{T} . When expanded, the characteristic equation takes the form (-a)^n + I_1(-a)^{n-1} + \\ldots + I_n = 0, where the set of constants \\{I_i\\}_{i=1}^n are called the invariants of the linear map \\mathsf{T} . The reason for calling them invariants is that their values do not depend on the choice of any basis for V , and are hence invariant with respect to the choice of basis. The Cayley-Hamilton theorem states that the linear map \\mathsf{T} also satisfies the characteristic equation: (-\\mathsf{T})^n + I_1(-\\mathsf{T})^{n-1} + \\ldots + I_n\\mathsf{I} = \\mathsf{0}. Here \\mathsf{T}^k is to be understood \\mathsf{T}\\mathsf{T}\\ldots\\mathsf{T} ( k factors). It is of interest to consider the case when V is a three dimensional vector space. In this case, the invariants of an invertible linear map \\mathsf{T} \\in \\text{GL}(V) can be shown to be as follows: for any \\mathsf{u},\\mathsf{v},\\mathsf{w} \\in V , \\begin{split} I_1 &= \\text{tr}(\\mathsf{T}) = \\frac{\\mathsf\\epsilon(\\mathsf{T}\\mathsf{u},\\mathsf{v},\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})} + \\frac{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{T}\\mathsf{v},\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})} + \\frac{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{T}\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})},\\\\ I_2 &= \\text{tr}(\\mathsf{T}^{-1})\\text{det}(\\mathsf{T}) = \\frac{\\mathsf\\epsilon(\\mathsf{T}\\mathsf{u},\\mathsf{T}\\mathsf{v},\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})} + \\frac{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{T}\\mathsf{v},\\mathsf{T}\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})} + \\frac{\\mathsf\\epsilon(\\mathsf{T}\\mathsf{u},\\mathsf{v},\\mathsf{T}\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})},\\\\ I_3 &= \\text{det}(\\mathsf{T}) = \\frac{\\mathsf\\epsilon(\\mathsf{T}\\mathsf{u},\\mathsf{T}\\mathsf{v},\\mathsf{T}\\mathsf{w})}{\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w})}. \\end{split} An elegant means to prove this is by applying the definition of the determinant, \\text{det}(\\mathsf{T})\\mathsf\\epsilon(\\mathsf{u},\\mathsf{v},\\mathsf{w}) = \\mathsf\\epsilon(\\mathsf{T}\\mathsf{u}, \\mathsf{T}\\mathsf{v},\\mathsf{T}\\mathsf{w}) to evaluate the determinant in the characteristic equation of \\mathsf{T} : \\text{det}(\\mathsf{T} - a\\mathsf{I}) .","title":"Invariants of a linear map"},{"location":"matrices/","text":"A few elementary facts about matrices with real entries are recalled here. The purpose of this appendix is to provide a refresher of key concepts in matrix algebra that are needed for the main development of linear algebra in the notes. Many proofs are omitted, and the stress is on acquiring a working knowledge of matrix algebra. Basic definitions A matrix or order m \\times n (read \" m cross n \") is a rectangular array of real numbers, as shown below: \\begin{bmatrix} A_{11} & A_{12} & \\ldots & A_{1n}\\\\ A_{21} & A_{22} & \\ldots & A_{2n}\\\\ \\vdots & & \\ddots & \\vdots\\\\ A_{m1} & A_{m2} & \\ldots & A_{mn} \\end{bmatrix}. Here \\{A_{ij}\\} are real numbers for every 1 \\le i \\le m and 1 \\le j \\le m . A horizontal section of the matrix is called a row , while a vertical section is called a column . Every m \\times n matrix thus has m rows and n columns. The element in the i^{\\text{th}} row and j^{\\text{th}} column of this matrix is A_{ij} . It is conventional to call the index i in A_{ij} as the row index and the index j as the column index . We will typically use a symbol like \\mathsf{A} to represent a matrix like the one shown above. The fact that the (i,j)^{\\text{th}} entry of \\mathsf{A} is A_{ij} is written as follows: \\mathsf{A}_{ij} = A_{ij} . Since the matrix \\mathsf{A} has mn real numbers \\{A_{ij}\\} arranged as an m \\times n rectangular array, we will refer to the set of all m \\times n matrices using the notation \\mathbb{R}^{m \\times n} . Thus, \\mathsf{A} \\in \\mathbb{R}^{m \\times n} . An m \\times 1 matrix is called a column vector , or just a vector , and a 1 \\times n matrix is called a row vector . For row and column matrices, it is conventional to use the following shortcut notation: The (i,1)^{\\text{th}} entry of \\mathsf{A} \\in \\mathbb{R}^{n \\times 1} is written as A_i instead of A_{i1} ; a similar comment applies for row vectors. A matrix of the form \\mathsf{A} \\in \\mathbb{R}^{n \\times n} is called a square matrix of order n . An important example of a square matrix is the identity matrix of order n , written \\mathsf{I} \\in \\mathbb{R}^{n \\times n} , defined as follows: \\mathsf{I} = \\begin{bmatrix} 1 & 0 & \\ldots & 0\\\\ 0 & 1 & \\ldots & 0\\\\ \\vdots & & \\ddots & \\vdots\\\\ 0 & 0 & \\ldots & 1 \\end{bmatrix}. Given a square matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} , the ordered set of elements (A_{11}, A_{22}, \\ldots, A_{nn}) is called the leading diagonal of \\mathsf{A} . The identity matrix thus has 1 in each element of the leading diagonal and has entries zero everywhere else. An important important generalization of this kind of square matrix is a diagonal matrix , whose entries are all zero except on its leading diagonal. For instance, consider the matrix \\mathsf{D} \\in \\mathbb{R}^{n \\times n} defined as follows: \\mathsf{D} = \\begin{bmatrix} d_1 & 0 & \\ldots & 0\\\\ 0 & d_2 & \\ldots & 0\\\\ \\vdots & & \\ddots & \\vdots\\\\ 0 & 0 & \\ldots & d_n \\end{bmatrix}. It is conventional to denote the matrix \\mathsf{D} as \\text{diag}(d_1, d_2, \\ldots, d_n) . Using this notation, it can be see that \\mathsf{I} = \\text{diag}(1, 1, \\ldots, 1) . Given an m \\times n matrix \\mathsf{A} \\in \\mathbb{R}^{m \\times n} , the transpose of \\mathsf{A} is defined as the n \\times m matrix whose (i,j)^{\\text{th}} entry is the (j,i)^{\\text{th}} entry of \\mathsf{A} . It is conventional to denote the transpose of \\mathsf{A}\\in\\mathbb{R}^{m\\times n} as \\mathsf{A}^T \\in \\mathbb{R}^{n \\times m} . Thus, \\mathsf{A}^T_{ij} = \\mathsf{A}_{ji} . An n \\times n matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} is said to be symmetric if \\mathsf{A}^T = \\mathsf{A} , and skew-symmetric , or antisymmetric , if \\mathsf{A}^T = -\\mathsf{A} . Here, -\\mathsf{A} \\in \\mathbb{R}^{n \\times n} is the matrix whose (i,j)^{\\text{th}} entry is -A_{ij} : (-\\mathsf{A})_{ij} = -A_{ij} . Example Given any matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} , it is the case that (\\mathsf{A}^T)^T = \\mathsf{A} . To see this note that (\\mathsf{A}^T)_{ij} = A_{ji}, whence it follows that ((\\mathsf{A}^T)^T)_{ij} = A_{ij} , thereby proving the result. Algebraic operations on matrices Given two m \\times n matrices \\mathsf{A},\\mathsf{B} \\in \\mathbb{R}^{m \\times n} , their sum is defined as the matrix \\mathsf{A} + \\mathsf{B} \\in \\mathbb{R}^{m \\times n} such that (\\mathsf{A} + \\mathsf{B})_{ij} = A_{ij} + B_{ij}. Similarly, the scalar multiple of the matrix \\mathsf{A} with a real number c \\in \\mathbb{R} is defined as the matrix c\\mathsf{A} \\in \\mathbb{R}^{m \\times n} such that (c\\mathsf{A})_{ij} = c \\, A_{ij}. The product of two matrices is defined only under special conditions. Given an m \\times n matrix \\mathsf{A} \\in \\mathbb{R}^{m \\times n} and an n \\times k matrix \\mathsf{B} \\in \\mathsf{R}^{n \\times k} , the matrix product of \\mathsf{A} and \\mathsf{B} is the m \\times k matrix \\mathsf{AB} \\in \\mathbb{R}^{m \\times k} defined as follows: (\\mathsf{AB})_{ij} = \\sum_{a = 1}^n A_{ia} B_{aj}. Note that the product of two square matrices of the same order is always defined. Note further that for any \\mathsf{A} \\in \\mathbb{R}^{n \\times n} , \\mathsf{AI} = \\mathsf{IA} = \\mathsf{A} , where \\mathsf{I} \\in \\mathbb{R}^{n \\times n} is the identity matrix of order n . Example Suppose that we are given two matrices \\mathsf{A},\\mathsf{B} \\in \\mathbb{R}^{n \\times n} of order n . Then, the following equation holds: (\\mathsf{A}\\mathsf{B})^T = \\mathsf{B}^T \\mathsf{A}^T. To see this, note that if \\mathsf{C} = \\mathsf{AB} \\in \\mathbb{R}^{n \\times n} , then (\\mathsf{AB})^T_{ij} = \\mathsf{C}^T_{ij} = C_{ji} = \\sum_{k=1}^n A_{jk}B_{ki}. Notice also that (\\mathsf{B}^T\\mathsf{A}^T)_{ij} = \\sum_{k=1}^n \\mathsf{B}^T_{ik}\\mathsf{A}^T_{kj} = \\sum_{k=1}^n B_{ki}A_{jk} = \\sum_{k=1}^n A_{jk}B_{ki}. Comparing these two expressions yields the identity (\\mathsf{AB})^T = \\mathsf{B}^T \\mathsf{A}^T . Example Any matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} can be written as the sum of a symmetric and skew-symmetric matrix. To see this, note that A_{ij} = \\frac{1}{2}(A_{ij} + A_{ji}) + \\frac{1}{2}(A_{ij} - A_{ji}). Define the matrices \\mathsf{A}_S \\in \\mathbb{R}^{n \\times n} and \\mathsf{A}_A \\in \\mathbb{R}^{n \\times n} as (\\mathsf{A}_S)_{ij} = \\frac{1}{2}(A_{ij} + A_{ji}), \\qquad (\\mathsf{A}_A)_{ij} = \\frac{1}{2}(A_{ij} - A_{ji}). Equivalently, \\mathsf{A}_S = \\frac{1}{2}(\\mathsf{A} + \\mathsf{A}^T), \\qquad \\mathsf{A}_A = \\frac{1}{2}(\\mathsf{A} - \\mathsf{A}^T). It follows that \\mathsf{A} = \\mathsf{A}_S + \\mathsf{A}_A. It is easy to check that \\mathsf{A}_S is symmetric and \\mathsf{A}_A is skew-symmetric. Trace and determinant of square matrices Suppose that \\mathsf{A} \\in \\mathbb{R}^{n \\times n} is a square matrix of order n . We will now define two important scalars associated with \\mathsf{A} . The first, called the trace of \\mathsf{A} , written \\text{tr}(\\mathsf{A}) \\in \\mathbb{R} , is defined as follows: \\text{tr}(\\mathsf{A}) = \\sum_{k=1}^n A_{kk}. Thus the trace of a given matrix is the sum of the elements on its leading diagonal. Example Suppose that \\mathsf{A} \\in \\mathbb{R}^{n \\times n} is a symmetric matrix, and \\mathsf{B} \\in \\mathbb{R}^{n \\times n} is a skew-symmetric matrix. Then \\text{tr}(\\mathsf{AB}) = 0 . To see this note that \\begin{split} (\\mathsf{AB})_{ij} = \\sum_{k=1}^n A_{ik} B_{kj} &= \\sum_{k=1}^n \\frac{1}{2}(A_{ik} + A_{ki})B_{kj}\\\\ &= \\frac{1}{2}\\sum_{k=1}^n A_{ik}B_{kj} + \\frac{1}{2}\\sum_{k=1}^n A_{ki}B_{kj}\\\\ &= \\frac{1}{2}\\sum_{k=1}^n A_{ik}B_{kj} - \\frac{1}{2}\\sum_{k=1}^n A_{ki}B_{jk}. \\end{split} Taking the trace on both sides, it follows that \\sum_{j=1}^n \\sum_{k=1}^n A_{jk}B_{kj} = \\frac{1}{2}\\sum_{j=1}^n\\sum_{k=1}^n A_{jk}B_{kj} - \\frac{1}{2}\\sum_{j=1}^n\\sum_{k=1}^n A_{kj}B_{jk} = 0. This shows that \\text{tr}(\\mathsf{AB}) = 0 . The second important scalar associated with a square matrix is its determinant . It is simpler to define the determinant of an n \\times n matrix inductively. The determinant of a 1 \\times 1 matrix \\mathsf{A} = [A_{11}] \\in \\mathbb{R}^{1 \\times 1} , written \\text{det}(\\mathsf{A}) , is defined as follows: \\text{det}(\\mathsf{A}) = A_{11} . The determinant of a 2 \\times 2 matrix \\mathsf{A} \\in \\mathbb{R}^{2 \\times 2} is defined as follows: \\text{det}(\\mathsf{A}) = \\text{det}\\begin{bmatrix} A_{11} & A_{12}\\\\ A_{21} & A_{22}\\end{bmatrix} = A_{11}A_{22} - A_{12}A_{21}. For any n > 2 , the determinant of an n \\times n matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} is defined inductively as follows. The (i,j)^{\\text{th}} minor of \\mathsf{A} is defined as the (n - 1) \\times (n - 1) matrix M_{(i,j)}(\\mathsf{A}) \\in \\mathbb{R}^{(n-1)\\times(n-1)} that is obtained by removing the i^{\\text{th}} row and j^{\\text{th}} column of \\mathsf{A} . The determinant of \\mathsf{A} is defined using the determinant of the minor as follows: for any i \\in \\{1, 2, \\ldots, n\\} , \\text{det}(\\mathsf{A}) = \\sum_{j=1}^n (-1)^{i + j} A_{ij} \\, \\text{det}(M_{(i,j)}(\\mathsf{A})). Note that i in the equation above can be chosen to be any row index. The definition of the determinant is best illustrated with a few examples. Example Consider the following 3 \\times 3 matrix \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} : \\mathsf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13}\\\\ A_{21} & A_{22} & A_{23}\\\\ A_{31} & A_{32} & A_{33} \\end{bmatrix} The determinant of \\mathsf{A} is computed as follows: \\begin{split} \\text{det}(\\mathsf{A}) &= \\sum_{j=1}^3 (-1)^{1 + j} A_{1j} \\text{det}(M_{(1,j)}(\\mathsf{A}))\\\\ &= A_{11} \\text{det} \\begin{bmatrix} A_{22} & A_{23}\\\\ A_{32} & A_{33}\\end{bmatrix} - A_{21} \\text{det} \\begin{bmatrix} A_{12} & A_{13}\\\\ A_{31} & A_{33}\\end{bmatrix} + A_{13} \\text{det} \\begin{bmatrix} A_{21} & A_{22}\\\\ A_{31} & A_{32}\\end{bmatrix}\\\\ &= A_{11}(A_{22}A_{33} - A_{23}A_{32}) - A_{22}(A_{21}A_{33} - A_{31}A_{23}) + A_{33}(A_{21}A_{32} - A_{22}A_{31}). \\end{split} Note that the determinant is expanded using the first row here. It is left as a simple exercise to verify that the value of the determinant is the same irrespective of which row is chosen. Inverse of a matrix A matrix \\mathsf{A} \\in \\mathbb{R}^{n\\times n} is said to be invertible if there exists another matrix \\mathsf{B} \\in \\mathbb{R}^{n \\times n} such that \\mathsf{A}\\mathsf{B} = \\mathsf{B}\\mathsf{A} = \\mathsf{I} . In this case, is conventional to write \\mathsf{B} as \\mathsf{A}^{-1} . An explicit formula for the inverse of an invertible square matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} can be provided. Define first the cofactor matrix of \\mathsf{A} , written \\text{cof}(\\mathsf{A}) \\in \\mathbb{R}^{n \\times n} as follows: \\text{cof}(\\mathsf{A})_{ij} = (-1)^{i + j} M_{(i,j)}(\\mathsf{A}), where M_{(i,j)}(\\mathsf{A}) \\in \\mathbb{R}^{(n-1)\\times(n -1)} is the (i,j)^{\\text{th}} minor of \\mathsf{A} . The transpose of the cofactor matrix of \\mathsf{A} is called the adjoint of \\mathsf{A} , written \\text{adj}(\\mathsf{A}) \\in \\mathbb{R}^{n \\times n} : \\text{adj}(\\mathsf{A}) = (\\text{cof}(\\mathsf{A}))^T. With these definitions in place, the inverse of an invertible square matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} can be written as \\mathsf{A}^{-1} = \\frac{1}{\\text{det}(\\mathsf{A})}\\text{adj}(\\mathsf{A}). Note that this also informs us that the matrix \\mathsf{A} is invertible if and only if its determinant is non-zero. This fact is frequently used in applications. Example Suppose that \\mathsf{A}, \\mathsf{B} \\in \\mathbb{R}^{n \\times n} are invertible matrices. Then (\\mathsf{AB})^{-1} = \\mathsf{B}^{-1}\\mathsf{A}^{-1} . To show this, note that (\\mathsf{B}^{-1}\\mathsf{A}^{-1})\\mathsf{AB} = \\mathsf{B}^{-1}(\\mathsf{A}^{-1}\\mathsf{A})\\mathsf{B} = \\mathsf{B}^{-1}\\mathsf{B} = \\mathsf{I}. Similarly, it can be shown that \\mathsf{AB}(\\mathsf{B}^{-1}\\mathsf{A}^{-1}) = \\mathsf{I} , thereby proving the claim. Example As an elementary illustration of the computation of the inverse, let us now compute the inverse of the matrix \\mathsf{A} \\in \\mathbb{R}^{2 \\times 2} , where \\mathsf{A} = \\begin{bmatrix} A_{11} & A_{12}\\\\ A_{21} & A_{22}\\end{bmatrix}. The cofactor of \\mathsf{A} is easily computed as \\text{cof}(\\mathsf{A}) = \\begin{bmatrix} A_{22} & -A_{21}\\\\ -A_{12} & A_{11}\\end{bmatrix}. The determinant of \\mathsf{A} is computed easily as (A_{11}A_{22} - A_{12}A_{21}) . Putting all this together, it follows from a simple computation that \\begin{bmatrix} A_{11} & A_{12}\\\\ A_{21} & A_{22}\\end{bmatrix}^{-1} = \\frac{1}{(A_{11}A_{22} - A_{12}A_{21})}\\begin{bmatrix} A_{22} & -A_{12}\\\\ -A_{21} & A_{11}\\end{bmatrix}. It can be checked by means of a direct substitution that this is indeed the inverse of \\mathsf{A} . Linear systems of equations An important application where matrices naturally find use is the solution of linear systems of equations. To understand what this means, suppose that we are given constants \\{A_{ij}\\} and \\{b_i\\} where i, j = 1, \\ldots, n . We are interested in finding real numbers (v_i)_{i=1}^n that satisfy the following set of equations: \\begin{split} A_{11} v_1 + A_{12} v_2 + \\ldots + A_{1n} v_n &= b_1,\\\\ A_{21} v_1 + A_{22} v_2 + \\ldots + A_{2n} v_n &= b_2,\\\\ \\ldots & \\ldots\\\\ A_{n1} v_1 + A_{n2} v_2 + \\ldots + A_{nn} v_n &= b_n. \\end{split} These equations can be succinctly written as follows: for every i \\in \\{1, \\ldots, n\\} , \\sum_{j=1}^n A_{ij} v_j = b_i. These equations can be written even more succinctly in the matrix form \\mathsf{A}\\mathsf{v} = \\mathsf{b}, where \\mathsf{A} = \\begin{bmatrix} A_{11} & \\ldots & A_{1n}\\\\ \\vdots & \\ddots & \\vdots\\\\ A_{n1} & \\ldots & A_{nn} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}, \\quad \\mathsf{v} = \\begin{bmatrix} v_1\\\\ \\vdots\\\\ v_n\\end{bmatrix} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathsf{b} = \\begin{bmatrix} b_1\\\\ \\vdots\\\\ b_n\\end{bmatrix} \\in \\mathbb{R}^{n \\times 1}. Thus, the system of linear equations has been reduced to an equation involving matrices. The system of equations \\mathsf{Av} = \\mathsf{b} does not always possess a unique solution. In the special case, however, when \\text{det}(\\mathsf{A}) \\neq 0 , the matrix \\mathsf{A} is invertible, and a unique solution for the system of equations \\mathsf{Av} = \\mathsf{b} can be found as \\mathsf{v} = \\mathsf{A}^{-1}\\mathsf{b}, as can be checked with direct substitution. Remark It is to be noted that when \\mathsf{A} is invertible, it is rare in practice to compute the solution of the set of linear equations \\mathsf{Av} = \\mathsf{b} using the relation \\mathsf{v} = \\mathsf{A}^{-1}\\mathsf{v} . This is due to the fact that calculating the inverse is computationally expensive for large matrices. Example Perhaps the simplest method to solve the system of equations \\mathsf{Av} = \\mathsf{b} is the Gaussian elimination algorithm . The idea behind this algorithm is to systematically reduce the system of equations to the successive solution of equations with just one unknown variable. Rather than explaining the general approach, it is instructive to look at a specific example. Suppose that we are interested in solving the following system of equations: \\begin{bmatrix} A_{11} & A_{12}\\\\ A_{21} & A_{22} \\end{bmatrix} \\begin{bmatrix} v_1\\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} b_1\\\\ b_2 \\end{bmatrix} It is assumed that the matrix \\mathsf{A} \\in \\mathbb{R}^{2 \\times 2} whose (i,j)^{\\text{th}} entry is A_{ij} is invertible, and, without loss of generality, that A_{21} \\neq 0 Begin by multiplying the second equation in \\sum_{j=1}^2 A_{ij}v_j = b_j by A_{11}/A_{21} . This yields the following modified set of equations \\begin{split} A_{11} v_1 + A_{12} v_2 &= b_1,\\\\ A_{11} v_1 + \\frac{A_{11}}{A_{21}}A_{22} v_2 &= \\frac{A_{11}}{A_{21}}b_2. \\end{split} Retaining the first equation as is and subtracting the first from the second equation, we get \\begin{split} A_{11} v_1 + A_{12} v_2 &= b_1\\\\ \\left(\\frac{A_{11}}{A_{21}}A_{22} - A_{12}\\right)v_2 &= \\frac{A_{11}}{A_{21}}b_2 - b_1. \\end{split} Solving the last equation for v_2 , we immediately see that v_2 = \\frac{1}{A_{11}A_{22} - A_{12}A_{21}}(-A_{21}b_1 + A_{11}b_2). Substituting this expression for v_2 in the first equation and solving for v_1 , we get, after some algebraic manipulation that v_1 = \\frac{1}{A_{11}A_{22} - A_{12}A_{21}}(A_{22}b_1 - A_{12}b_2). Note that this is exactly the solution obtained by solving the system of equations \\mathsf{Av} = \\mathsf{b} using the formula \\mathsf{v} = \\mathsf{A}^{-1}\\mathsf{b} . The foregoing calculations can be visualized as follows. Begin by collecting together the elements of the matrices \\mathsf{A} and \\mathsf{b} as follows: \\begin{bmatrix} A_{11} & A_{12} & b_1\\\\ A_{21} & A_{22} & b_2 \\end{bmatrix} The sequence of transformations carried out earlier can be summarized as follows: \\begin{bmatrix} A_{11} & A_{12} & b_1\\\\ A_{21} & A_{22} & b_2 \\end{bmatrix} \\quad\\rightarrow\\quad \\begin{bmatrix} A_{11} & A_{12} & b_1\\\\ 0 & \\left(\\dfrac{A_{11}}{A_{21}}A_{22} - A_{12}\\right) & \\left(\\dfrac{A_{11}}{A_{21}}b_2 - b_1\\right) \\end{bmatrix}. The matrix in the right hand side of the foregoing transformation is said to be in the upper triangular form and can be solved by back-substitution from the last equation upwards . The solution of a general linear system of equations is computed using an analogous and straightforward extension of this approach. Example As a concrete illustration of the Gaussian elimination algorithm, consider the solution of the following set of linear equations: \\begin{bmatrix} 1 & -2 & 4\\\\ 2 & -3 & 1\\\\ 3 & 2 & -1 \\end{bmatrix} \\begin{bmatrix} v_1\\\\ v_2\\\\ v_3 \\end{bmatrix} = \\begin{bmatrix} 9\\\\ -1\\\\ 4 \\end{bmatrix}. The Gaussian elimination procedure can be illustrated in a sequence of two steps. In the first step, appropriate multiples of the first equations are used to eliminate v_1 from the second and third equations. In the second step, an appropriate multiple of the second equation is used to eliminate v_2 from the third equation. These steps are summarized below: \\begin{bmatrix} 1 & -2 & 4 & 9\\\\ 2 & -3 & 1 & -1\\\\ 3 & 2 & -1 & 4 \\end{bmatrix} \\quad\\rightarrow\\quad \\begin{bmatrix} 1 & -2 & 4 & 9\\\\ 0 & -1 & 7 & 19\\\\ 0 & -8 & 13 & 23 \\end{bmatrix} \\quad\\rightarrow\\quad \\begin{bmatrix} 1 & -2 & 4 & 9\\\\ 0 & -1 & 7 & 19\\\\ 0 & 0 & 43 & 129 \\end{bmatrix} These equations are solve by back-substitution as follows: \\begin{split} 43v_3 = 129 &\\quad\\Rightarrow\\quad v_3 = \\frac{129}{43} = 3,\\\\ -v_2 + 7v_3 = 19 &\\quad\\Rightarrow\\quad v_2 = -(19 - 7\\times3) = 2,\\\\ v_1 - 2v_2 + 4v_3 = 9 &\\quad\\Rightarrow\\quad v_1 = 9 + 2\\times2 - 4\\times3 = 1. \\end{split} We thus obtain the solution of the linear system of equations as v_1 = 1 , v_2 = 2 , and v_3 = 3 . The fact that this is indeed a solution of the linear system of equations presented above can be verified by direct substitution. Eigenvalues and eigenvectors To wrap up the discussion of elementary matrix algebra, let us consider the eigenvalue problem . Suppose that we are given a matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} of order n . If there exists a vector \\mathsf{v} \\in \\mathbb{R}^{n \\times 1} and a real number a \\in \\mathbb{R} such that \\mathsf{Av} = a\\mathsf{v}, then a is called the eigenvalue of \\mathsf{A} corresponding to the eigenvector \\mathsf{v} . Remark Note that the zero vector \\mathsf{0} \\in \\mathbb{R}^{n \\times 1} trivially satisfies this equation for any choice of a \\in \\mathbb{R} . We will exclude this trivial solution and assume henceforth that every eigenvector is non-zero. Note that the equation \\mathsf{A}\\mathsf{v} = a\\mathsf{v} can be written as the equation (\\mathsf{A} - a\\mathsf{I})\\mathsf{v} = \\mathsf{0}. For this equation to have a non-trivial solution, it follows at once that \\text{det}(\\mathsf{A} - a\\mathsf{I}) = 0. This a polynomial equation in a of degree n , called the characteristic equation of \\mathsf{A} , and has n solutions in general. These solutions correspond to the eigenvalues of the matrix \\mathsf{A} . Example Consider the matrix \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} given as follows: \\mathsf{A} = \\begin{bmatrix} -2 & -4 & 2\\\\ -2 & 1 & 2\\\\ 4 & 2 & 5 \\end{bmatrix}. Let us compute the eigenvalues of \\mathsf{A} by computing its characteristic polynomial. Note first that the matrix \\mathsf{A} - a\\mathsf{I} has the following form: \\mathsf{A} - a\\mathsf{I} = \\begin{bmatrix} -2-a & -4 & 2\\\\ -2 & 1-a & 2\\\\ 4 & 2 & 5-a \\end{bmatrix}. Computing the determinant of this equation and setting it to zero, the characteristic equation \\text{det}(\\mathsf{A} - a\\mathsf{I}) = 0 is obtained as -a^3 + 4a^2 + 27a - 90 = 0, \\quad\\equiv\\quad -(a - 3)(a - 6)(a + 5) = 0. This shows at once that the eigenvalues of the matrix \\mathsf{A} are 3 , 6 , and -5 . The characteristic equation, when expanded fully, has the following structure: (-a)^n + I_1 (-a)^{n-1} + \\ldots + I_n = 0. The constants I_1, I_2, \\ldots, I_n are called the invariants of \\mathsf{A} , and can be related to the eigenvalues of \\mathsf{A} . Notably, the invariants I_1 and I_n are computed as \\begin{split} I_1 &= \\text{tr}(\\mathsf{A}) = \\sum_{i=1}^n a_i,\\\\ I_3 &= \\text{det}(\\mathsf{A}) = \\prod_{i=1}^n a_i. \\end{split} The other invariants of \\mathsf{A} can be similarly related to the eigenvalues of \\mathsf{A} , but they do not have a simple interpretation as I_1 and I_n . Example For the matrix \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} considered in the previous example, the characteristic equation takes the form -a^3 + I_1 a_2 - I_2 a + I_3 = 0. It can be checked that \\begin{split} I_1 = 4 = 3 + 6 - 5 = a_1 + a_2 + a_3,\\\\ I_3 = -90 = 3\\times6\\times(-5) = a_1 a_2 a_3. \\end{split} Further more, for matrices of order three, it is true that I_2 = a_1a_2 + a_2a_3 + a_3a_1, as can be checked with a simple calculation. The Cayley-Hamilton theorem states that the matrix \\mathsf{A} satisfies the characteristic equation: (-\\mathsf{A})^n + I_1 (-\\mathsf{A})^{n-1} + \\ldots + I_n\\mathsf{I} = \\mathsf{0}. This equation is often used in practice to simplify a variety of calculations. The following example provides an elementary illustration of the Cayley-Hamilton theorem. Example Suppose that \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} is an invertible matrix. Then, its inverse can be computed using the Cayley-Hamilton theorem as follows: \\begin{split} & -\\mathsf{A}^3 + I_1\\mathsf{A}^2 - I_2\\mathsf{A} + I_3\\mathsf{I} = 0\\\\ \\Rightarrow & I_3\\mathsf{I} = \\mathsf{A}^3 - I_1\\mathsf{A}^2 + I_2\\mathsf{A}\\\\ \\Rightarrow & \\mathsf{A}^{-1} = \\frac{1}{I_3}\\left(\\mathsf{A}^2 - I_1\\mathsf{A} + I_2\\right). \\end{split} Notice how the fact that I_3 = \\text{det}(\\mathsf{A}) \\neq 0 is used in the last step of this calculation. The Cayley-Hamilton theorem thus provides a convenient expression for the inverse of \\mathsf{A} ; compare this with the general expression for the inverse provided earlier. The eigenvectors of \\mathsf{A} \\in \\mathbb{R}^{n \\times n} can be obtained by substituting the eigenvalues, successively in the equation \\mathsf{A}\\mathsf{v} = a\\mathsf{v} . Notice that \\mathsf{v} is an eigenvector of \\mathsf{A} corresponding to the eigenvalue a , then c\\mathsf{v} is also an eigenvector of \\mathsf{A} corresponding to the same eigenvalue a for any c \\in \\mathbb{R} . This fact is often used to single out a particular value of c , and hence a particular eigenvector. This is best illustrated with an example. Example Consider the matrix \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} considered earlier: \\mathsf{A} = \\begin{bmatrix} -2 & -4 & 2\\\\ -2 & 1 & 2\\\\ 4 & 2 & 5 \\end{bmatrix}. The eigenvalues of \\mathsf{A} were computed earlier as 3, 6, -5 . Let us now compute the corresponding eigenvectors. Consider first the eigenvalue a_1 = 3 . Substituting this in the equation \\mathsf{A}\\mathsf{v}_1 = a_1\\mathsf{v}_1 , we get \\begin{bmatrix} -2 & -4 & 2\\\\ -2 & 1 & 2\\\\ 4 & 2 & 5 \\end{bmatrix} \\begin{bmatrix} v_{1,1}\\\\ v_{1,2}\\\\ v_{1,3} \\end{bmatrix} = 3 \\begin{bmatrix} v_{1,1}\\\\ v_{1,2}\\\\ v_{1,3} \\end{bmatrix} \\quad\\equiv\\quad \\begin{bmatrix} -5 & -4 & 2\\\\ -2 & -2 & 2\\\\ 4 & 2 & 2 \\end{bmatrix} \\begin{bmatrix} v_{1,1}\\\\ v_{1,2}\\\\ v_{1,3} \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix}. In the equations above, we have used the notation (\\mathsf{v}_1)_i = v_{1,i} . Notice that this matrix equation does not have a unique solution since \\text{det}(\\mathsf{A} - a_1\\mathsf{I}) = 0 . To compute all possible solutions, let us compute the components v_{1,1} and v_{1,2} in terms of v_{1,3} . Begin by rewriting the first two equations as \\begin{split} 5v_{1,1} + 4v_{1,2} &= 2v_{1,3},\\\\ 2v_{1,1} + 2v_{1,2} &= 2v_{1,3}. \\end{split} These equations can be readily solved to yield v_{1,1} = -2v_{1,3}, \\qquad v_{2,1} = 3v_{1,3}. Thus, every vector of the form [-2a \\; 3a \\; a]^T \\in \\mathbb{R}^{3 \\times 1} , where a \\in \\mathbb{R} , is an eigenvector of \\mathsf{A} corresponding to the eigenvalue 3 . Different choices for a can be chosen depending on the context. For instance, choosing a = 1 , we obtain the eigenvector [-2 \\; 3 \\; 1]^T . Requiring that the eigenvector has unit length, on the other hand yields the eigenvector [-2/\\sqrt{14} \\; 3/\\sqrt{14} \\; 1/\\sqrt{14}]^T . Remark The dot product of two vectors \\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^{n \\times 1} , written \\mathsf{u}\\cdot\\mathsf{v} \\in \\mathbb{R} , is defined as \\mathsf{u}^T \\mathsf{v} . Note that it follows from the definition that \\mathsf{u} \\cdot \\mathsf{v} = \\mathsf{v}^T \\mathsf{u} . The length of a vector \\mathsf{u} \\in \\mathbb{R}^{n \\times 1} is defined as \\sqrt{\\mathsf{u}^T\\mathsf{u}} . The other eigenvectors of \\mathsf{A} can be computed similarly. It is left as an exercise to check that [1 \\; 6 \\; 16]^T and [-2 \\; -1 \\; 1]^T are the eigenvectors of \\mathsf{A} corresponding to the eigenvalues 6 and -5 , respectively. Suppose that \\mathsf{v}_1, \\ldots, \\mathsf{v}_n \\in \\mathbb{R}^{n \\times 1} are the eigenvectors of \\mathsf{A} \\in \\mathbb{R}^{n \\times n} corresponding to the eigenvalues a_1, \\ldots, a_n \\in \\mathbb{R} , respectively. Then the set of equations \\mathsf{A}\\mathsf{v}_i = a\\mathsf{v}_i , where i = 1, \\ldots, n , can be written compactly as \\mathsf{A}\\mathsf{V} = \\mathsf{V}\\mathsf{D}, where \\mathsf{V} = \\begin{bmatrix} v_{1,1} & v_{2,1} & \\ldots & v_{n,1}\\\\ v_{1,2} & v_{2,2} & \\ldots & v_{n,2}\\\\ \\vdots & & \\ddots & \\vdots\\\\ v_{1,n} & v_{2,n} & \\ldots & v_{n,n} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}, \\qquad \\mathsf{D} = \\text{diag}(a_1, a_2, \\ldots, a_n) \\in \\mathbb{R}^{n \\times n}. Notice that the first column of \\mathsf{V} is the first eigenvector of \\mathsf{A} , the second column of \\mathsf{V} is the second eigenvector of \\mathsf{A} , and so on. It can be shown that if the eigenvectors are linearly independent (A proper definition of this term is provided in these notes in the general context of finite dimensional vector spaces.), then the matrix \\mathsf{V} is invertible. In this case, \\mathsf{A} = \\mathsf{V} \\mathsf{D} \\mathsf{V}^{-1}. This equation is called the eigendecomposition of \\mathsf{A} , and plays an important role in many applications. Example Consider the matrix \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} considered in the previous examples: \\mathsf{A} = \\begin{bmatrix} -2 & -4 & 2\\\\ -2 & 1 & 2\\\\ 4 & 2 & 5 \\end{bmatrix}. The matrices \\mathsf{V} \\in \\mathbb{R}^{3 \\times 3} and \\mathsf{D}\\in \\mathsf{3 \\times 3} can be constructed as follows: \\mathsf{V} = \\begin{bmatrix} -2 & 1 & -2\\\\ 3 & 6 & -1\\\\ 1 & 16 & 1 \\end{bmatrix}, \\qquad \\mathsf{D} = \\text{diag}(3,6,-5). Note that the matrix \\mathsf{V} is invertible in this case. It is left as a simple exercise to check that \\begin{bmatrix} -2 & -4 & 2\\\\ -2 & 1 & 2\\\\ 4 & 2 & 5 \\end{bmatrix} = \\begin{bmatrix} -2 & 1 & -2\\\\ 3 & 6 & -1\\\\ 1 & 16 & 1 \\end{bmatrix} \\begin{bmatrix} 3 & 0 & 0\\\\ 0 & 6 & 0\\\\ 0 & 0 & -5 \\end{bmatrix} \\begin{bmatrix} -2 & 1 & -2\\\\ 3 & 6 & -1\\\\ 1 & 16 & 1 \\end{bmatrix}^{-1}, thereby verifying the eigendecomposition \\mathsf{A} = \\mathsf{V}\\mathsf{D}\\mathsf{V}^{-1} of \\mathsf{A} . TO DO Degenerate cases","title":"Appendix - Matrices"},{"location":"matrices/#basic-definitions","text":"A matrix or order m \\times n (read \" m cross n \") is a rectangular array of real numbers, as shown below: \\begin{bmatrix} A_{11} & A_{12} & \\ldots & A_{1n}\\\\ A_{21} & A_{22} & \\ldots & A_{2n}\\\\ \\vdots & & \\ddots & \\vdots\\\\ A_{m1} & A_{m2} & \\ldots & A_{mn} \\end{bmatrix}. Here \\{A_{ij}\\} are real numbers for every 1 \\le i \\le m and 1 \\le j \\le m . A horizontal section of the matrix is called a row , while a vertical section is called a column . Every m \\times n matrix thus has m rows and n columns. The element in the i^{\\text{th}} row and j^{\\text{th}} column of this matrix is A_{ij} . It is conventional to call the index i in A_{ij} as the row index and the index j as the column index . We will typically use a symbol like \\mathsf{A} to represent a matrix like the one shown above. The fact that the (i,j)^{\\text{th}} entry of \\mathsf{A} is A_{ij} is written as follows: \\mathsf{A}_{ij} = A_{ij} . Since the matrix \\mathsf{A} has mn real numbers \\{A_{ij}\\} arranged as an m \\times n rectangular array, we will refer to the set of all m \\times n matrices using the notation \\mathbb{R}^{m \\times n} . Thus, \\mathsf{A} \\in \\mathbb{R}^{m \\times n} . An m \\times 1 matrix is called a column vector , or just a vector , and a 1 \\times n matrix is called a row vector . For row and column matrices, it is conventional to use the following shortcut notation: The (i,1)^{\\text{th}} entry of \\mathsf{A} \\in \\mathbb{R}^{n \\times 1} is written as A_i instead of A_{i1} ; a similar comment applies for row vectors. A matrix of the form \\mathsf{A} \\in \\mathbb{R}^{n \\times n} is called a square matrix of order n . An important example of a square matrix is the identity matrix of order n , written \\mathsf{I} \\in \\mathbb{R}^{n \\times n} , defined as follows: \\mathsf{I} = \\begin{bmatrix} 1 & 0 & \\ldots & 0\\\\ 0 & 1 & \\ldots & 0\\\\ \\vdots & & \\ddots & \\vdots\\\\ 0 & 0 & \\ldots & 1 \\end{bmatrix}. Given a square matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} , the ordered set of elements (A_{11}, A_{22}, \\ldots, A_{nn}) is called the leading diagonal of \\mathsf{A} . The identity matrix thus has 1 in each element of the leading diagonal and has entries zero everywhere else. An important important generalization of this kind of square matrix is a diagonal matrix , whose entries are all zero except on its leading diagonal. For instance, consider the matrix \\mathsf{D} \\in \\mathbb{R}^{n \\times n} defined as follows: \\mathsf{D} = \\begin{bmatrix} d_1 & 0 & \\ldots & 0\\\\ 0 & d_2 & \\ldots & 0\\\\ \\vdots & & \\ddots & \\vdots\\\\ 0 & 0 & \\ldots & d_n \\end{bmatrix}. It is conventional to denote the matrix \\mathsf{D} as \\text{diag}(d_1, d_2, \\ldots, d_n) . Using this notation, it can be see that \\mathsf{I} = \\text{diag}(1, 1, \\ldots, 1) . Given an m \\times n matrix \\mathsf{A} \\in \\mathbb{R}^{m \\times n} , the transpose of \\mathsf{A} is defined as the n \\times m matrix whose (i,j)^{\\text{th}} entry is the (j,i)^{\\text{th}} entry of \\mathsf{A} . It is conventional to denote the transpose of \\mathsf{A}\\in\\mathbb{R}^{m\\times n} as \\mathsf{A}^T \\in \\mathbb{R}^{n \\times m} . Thus, \\mathsf{A}^T_{ij} = \\mathsf{A}_{ji} . An n \\times n matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} is said to be symmetric if \\mathsf{A}^T = \\mathsf{A} , and skew-symmetric , or antisymmetric , if \\mathsf{A}^T = -\\mathsf{A} . Here, -\\mathsf{A} \\in \\mathbb{R}^{n \\times n} is the matrix whose (i,j)^{\\text{th}} entry is -A_{ij} : (-\\mathsf{A})_{ij} = -A_{ij} . Example Given any matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} , it is the case that (\\mathsf{A}^T)^T = \\mathsf{A} . To see this note that (\\mathsf{A}^T)_{ij} = A_{ji}, whence it follows that ((\\mathsf{A}^T)^T)_{ij} = A_{ij} , thereby proving the result.","title":"Basic definitions"},{"location":"matrices/#algebraic-operations-on-matrices","text":"Given two m \\times n matrices \\mathsf{A},\\mathsf{B} \\in \\mathbb{R}^{m \\times n} , their sum is defined as the matrix \\mathsf{A} + \\mathsf{B} \\in \\mathbb{R}^{m \\times n} such that (\\mathsf{A} + \\mathsf{B})_{ij} = A_{ij} + B_{ij}. Similarly, the scalar multiple of the matrix \\mathsf{A} with a real number c \\in \\mathbb{R} is defined as the matrix c\\mathsf{A} \\in \\mathbb{R}^{m \\times n} such that (c\\mathsf{A})_{ij} = c \\, A_{ij}. The product of two matrices is defined only under special conditions. Given an m \\times n matrix \\mathsf{A} \\in \\mathbb{R}^{m \\times n} and an n \\times k matrix \\mathsf{B} \\in \\mathsf{R}^{n \\times k} , the matrix product of \\mathsf{A} and \\mathsf{B} is the m \\times k matrix \\mathsf{AB} \\in \\mathbb{R}^{m \\times k} defined as follows: (\\mathsf{AB})_{ij} = \\sum_{a = 1}^n A_{ia} B_{aj}. Note that the product of two square matrices of the same order is always defined. Note further that for any \\mathsf{A} \\in \\mathbb{R}^{n \\times n} , \\mathsf{AI} = \\mathsf{IA} = \\mathsf{A} , where \\mathsf{I} \\in \\mathbb{R}^{n \\times n} is the identity matrix of order n . Example Suppose that we are given two matrices \\mathsf{A},\\mathsf{B} \\in \\mathbb{R}^{n \\times n} of order n . Then, the following equation holds: (\\mathsf{A}\\mathsf{B})^T = \\mathsf{B}^T \\mathsf{A}^T. To see this, note that if \\mathsf{C} = \\mathsf{AB} \\in \\mathbb{R}^{n \\times n} , then (\\mathsf{AB})^T_{ij} = \\mathsf{C}^T_{ij} = C_{ji} = \\sum_{k=1}^n A_{jk}B_{ki}. Notice also that (\\mathsf{B}^T\\mathsf{A}^T)_{ij} = \\sum_{k=1}^n \\mathsf{B}^T_{ik}\\mathsf{A}^T_{kj} = \\sum_{k=1}^n B_{ki}A_{jk} = \\sum_{k=1}^n A_{jk}B_{ki}. Comparing these two expressions yields the identity (\\mathsf{AB})^T = \\mathsf{B}^T \\mathsf{A}^T . Example Any matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} can be written as the sum of a symmetric and skew-symmetric matrix. To see this, note that A_{ij} = \\frac{1}{2}(A_{ij} + A_{ji}) + \\frac{1}{2}(A_{ij} - A_{ji}). Define the matrices \\mathsf{A}_S \\in \\mathbb{R}^{n \\times n} and \\mathsf{A}_A \\in \\mathbb{R}^{n \\times n} as (\\mathsf{A}_S)_{ij} = \\frac{1}{2}(A_{ij} + A_{ji}), \\qquad (\\mathsf{A}_A)_{ij} = \\frac{1}{2}(A_{ij} - A_{ji}). Equivalently, \\mathsf{A}_S = \\frac{1}{2}(\\mathsf{A} + \\mathsf{A}^T), \\qquad \\mathsf{A}_A = \\frac{1}{2}(\\mathsf{A} - \\mathsf{A}^T). It follows that \\mathsf{A} = \\mathsf{A}_S + \\mathsf{A}_A. It is easy to check that \\mathsf{A}_S is symmetric and \\mathsf{A}_A is skew-symmetric.","title":"Algebraic operations on matrices"},{"location":"matrices/#trace-and-determinant-of-square-matrices","text":"Suppose that \\mathsf{A} \\in \\mathbb{R}^{n \\times n} is a square matrix of order n . We will now define two important scalars associated with \\mathsf{A} . The first, called the trace of \\mathsf{A} , written \\text{tr}(\\mathsf{A}) \\in \\mathbb{R} , is defined as follows: \\text{tr}(\\mathsf{A}) = \\sum_{k=1}^n A_{kk}. Thus the trace of a given matrix is the sum of the elements on its leading diagonal. Example Suppose that \\mathsf{A} \\in \\mathbb{R}^{n \\times n} is a symmetric matrix, and \\mathsf{B} \\in \\mathbb{R}^{n \\times n} is a skew-symmetric matrix. Then \\text{tr}(\\mathsf{AB}) = 0 . To see this note that \\begin{split} (\\mathsf{AB})_{ij} = \\sum_{k=1}^n A_{ik} B_{kj} &= \\sum_{k=1}^n \\frac{1}{2}(A_{ik} + A_{ki})B_{kj}\\\\ &= \\frac{1}{2}\\sum_{k=1}^n A_{ik}B_{kj} + \\frac{1}{2}\\sum_{k=1}^n A_{ki}B_{kj}\\\\ &= \\frac{1}{2}\\sum_{k=1}^n A_{ik}B_{kj} - \\frac{1}{2}\\sum_{k=1}^n A_{ki}B_{jk}. \\end{split} Taking the trace on both sides, it follows that \\sum_{j=1}^n \\sum_{k=1}^n A_{jk}B_{kj} = \\frac{1}{2}\\sum_{j=1}^n\\sum_{k=1}^n A_{jk}B_{kj} - \\frac{1}{2}\\sum_{j=1}^n\\sum_{k=1}^n A_{kj}B_{jk} = 0. This shows that \\text{tr}(\\mathsf{AB}) = 0 . The second important scalar associated with a square matrix is its determinant . It is simpler to define the determinant of an n \\times n matrix inductively. The determinant of a 1 \\times 1 matrix \\mathsf{A} = [A_{11}] \\in \\mathbb{R}^{1 \\times 1} , written \\text{det}(\\mathsf{A}) , is defined as follows: \\text{det}(\\mathsf{A}) = A_{11} . The determinant of a 2 \\times 2 matrix \\mathsf{A} \\in \\mathbb{R}^{2 \\times 2} is defined as follows: \\text{det}(\\mathsf{A}) = \\text{det}\\begin{bmatrix} A_{11} & A_{12}\\\\ A_{21} & A_{22}\\end{bmatrix} = A_{11}A_{22} - A_{12}A_{21}. For any n > 2 , the determinant of an n \\times n matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} is defined inductively as follows. The (i,j)^{\\text{th}} minor of \\mathsf{A} is defined as the (n - 1) \\times (n - 1) matrix M_{(i,j)}(\\mathsf{A}) \\in \\mathbb{R}^{(n-1)\\times(n-1)} that is obtained by removing the i^{\\text{th}} row and j^{\\text{th}} column of \\mathsf{A} . The determinant of \\mathsf{A} is defined using the determinant of the minor as follows: for any i \\in \\{1, 2, \\ldots, n\\} , \\text{det}(\\mathsf{A}) = \\sum_{j=1}^n (-1)^{i + j} A_{ij} \\, \\text{det}(M_{(i,j)}(\\mathsf{A})). Note that i in the equation above can be chosen to be any row index. The definition of the determinant is best illustrated with a few examples. Example Consider the following 3 \\times 3 matrix \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} : \\mathsf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13}\\\\ A_{21} & A_{22} & A_{23}\\\\ A_{31} & A_{32} & A_{33} \\end{bmatrix} The determinant of \\mathsf{A} is computed as follows: \\begin{split} \\text{det}(\\mathsf{A}) &= \\sum_{j=1}^3 (-1)^{1 + j} A_{1j} \\text{det}(M_{(1,j)}(\\mathsf{A}))\\\\ &= A_{11} \\text{det} \\begin{bmatrix} A_{22} & A_{23}\\\\ A_{32} & A_{33}\\end{bmatrix} - A_{21} \\text{det} \\begin{bmatrix} A_{12} & A_{13}\\\\ A_{31} & A_{33}\\end{bmatrix} + A_{13} \\text{det} \\begin{bmatrix} A_{21} & A_{22}\\\\ A_{31} & A_{32}\\end{bmatrix}\\\\ &= A_{11}(A_{22}A_{33} - A_{23}A_{32}) - A_{22}(A_{21}A_{33} - A_{31}A_{23}) + A_{33}(A_{21}A_{32} - A_{22}A_{31}). \\end{split} Note that the determinant is expanded using the first row here. It is left as a simple exercise to verify that the value of the determinant is the same irrespective of which row is chosen.","title":"Trace and determinant of square matrices"},{"location":"matrices/#inverse-of-a-matrix","text":"A matrix \\mathsf{A} \\in \\mathbb{R}^{n\\times n} is said to be invertible if there exists another matrix \\mathsf{B} \\in \\mathbb{R}^{n \\times n} such that \\mathsf{A}\\mathsf{B} = \\mathsf{B}\\mathsf{A} = \\mathsf{I} . In this case, is conventional to write \\mathsf{B} as \\mathsf{A}^{-1} . An explicit formula for the inverse of an invertible square matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} can be provided. Define first the cofactor matrix of \\mathsf{A} , written \\text{cof}(\\mathsf{A}) \\in \\mathbb{R}^{n \\times n} as follows: \\text{cof}(\\mathsf{A})_{ij} = (-1)^{i + j} M_{(i,j)}(\\mathsf{A}), where M_{(i,j)}(\\mathsf{A}) \\in \\mathbb{R}^{(n-1)\\times(n -1)} is the (i,j)^{\\text{th}} minor of \\mathsf{A} . The transpose of the cofactor matrix of \\mathsf{A} is called the adjoint of \\mathsf{A} , written \\text{adj}(\\mathsf{A}) \\in \\mathbb{R}^{n \\times n} : \\text{adj}(\\mathsf{A}) = (\\text{cof}(\\mathsf{A}))^T. With these definitions in place, the inverse of an invertible square matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} can be written as \\mathsf{A}^{-1} = \\frac{1}{\\text{det}(\\mathsf{A})}\\text{adj}(\\mathsf{A}). Note that this also informs us that the matrix \\mathsf{A} is invertible if and only if its determinant is non-zero. This fact is frequently used in applications. Example Suppose that \\mathsf{A}, \\mathsf{B} \\in \\mathbb{R}^{n \\times n} are invertible matrices. Then (\\mathsf{AB})^{-1} = \\mathsf{B}^{-1}\\mathsf{A}^{-1} . To show this, note that (\\mathsf{B}^{-1}\\mathsf{A}^{-1})\\mathsf{AB} = \\mathsf{B}^{-1}(\\mathsf{A}^{-1}\\mathsf{A})\\mathsf{B} = \\mathsf{B}^{-1}\\mathsf{B} = \\mathsf{I}. Similarly, it can be shown that \\mathsf{AB}(\\mathsf{B}^{-1}\\mathsf{A}^{-1}) = \\mathsf{I} , thereby proving the claim. Example As an elementary illustration of the computation of the inverse, let us now compute the inverse of the matrix \\mathsf{A} \\in \\mathbb{R}^{2 \\times 2} , where \\mathsf{A} = \\begin{bmatrix} A_{11} & A_{12}\\\\ A_{21} & A_{22}\\end{bmatrix}. The cofactor of \\mathsf{A} is easily computed as \\text{cof}(\\mathsf{A}) = \\begin{bmatrix} A_{22} & -A_{21}\\\\ -A_{12} & A_{11}\\end{bmatrix}. The determinant of \\mathsf{A} is computed easily as (A_{11}A_{22} - A_{12}A_{21}) . Putting all this together, it follows from a simple computation that \\begin{bmatrix} A_{11} & A_{12}\\\\ A_{21} & A_{22}\\end{bmatrix}^{-1} = \\frac{1}{(A_{11}A_{22} - A_{12}A_{21})}\\begin{bmatrix} A_{22} & -A_{12}\\\\ -A_{21} & A_{11}\\end{bmatrix}. It can be checked by means of a direct substitution that this is indeed the inverse of \\mathsf{A} .","title":"Inverse of a matrix"},{"location":"matrices/#linear-systems-of-equations","text":"An important application where matrices naturally find use is the solution of linear systems of equations. To understand what this means, suppose that we are given constants \\{A_{ij}\\} and \\{b_i\\} where i, j = 1, \\ldots, n . We are interested in finding real numbers (v_i)_{i=1}^n that satisfy the following set of equations: \\begin{split} A_{11} v_1 + A_{12} v_2 + \\ldots + A_{1n} v_n &= b_1,\\\\ A_{21} v_1 + A_{22} v_2 + \\ldots + A_{2n} v_n &= b_2,\\\\ \\ldots & \\ldots\\\\ A_{n1} v_1 + A_{n2} v_2 + \\ldots + A_{nn} v_n &= b_n. \\end{split} These equations can be succinctly written as follows: for every i \\in \\{1, \\ldots, n\\} , \\sum_{j=1}^n A_{ij} v_j = b_i. These equations can be written even more succinctly in the matrix form \\mathsf{A}\\mathsf{v} = \\mathsf{b}, where \\mathsf{A} = \\begin{bmatrix} A_{11} & \\ldots & A_{1n}\\\\ \\vdots & \\ddots & \\vdots\\\\ A_{n1} & \\ldots & A_{nn} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}, \\quad \\mathsf{v} = \\begin{bmatrix} v_1\\\\ \\vdots\\\\ v_n\\end{bmatrix} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathsf{b} = \\begin{bmatrix} b_1\\\\ \\vdots\\\\ b_n\\end{bmatrix} \\in \\mathbb{R}^{n \\times 1}. Thus, the system of linear equations has been reduced to an equation involving matrices. The system of equations \\mathsf{Av} = \\mathsf{b} does not always possess a unique solution. In the special case, however, when \\text{det}(\\mathsf{A}) \\neq 0 , the matrix \\mathsf{A} is invertible, and a unique solution for the system of equations \\mathsf{Av} = \\mathsf{b} can be found as \\mathsf{v} = \\mathsf{A}^{-1}\\mathsf{b}, as can be checked with direct substitution. Remark It is to be noted that when \\mathsf{A} is invertible, it is rare in practice to compute the solution of the set of linear equations \\mathsf{Av} = \\mathsf{b} using the relation \\mathsf{v} = \\mathsf{A}^{-1}\\mathsf{v} . This is due to the fact that calculating the inverse is computationally expensive for large matrices. Example Perhaps the simplest method to solve the system of equations \\mathsf{Av} = \\mathsf{b} is the Gaussian elimination algorithm . The idea behind this algorithm is to systematically reduce the system of equations to the successive solution of equations with just one unknown variable. Rather than explaining the general approach, it is instructive to look at a specific example. Suppose that we are interested in solving the following system of equations: \\begin{bmatrix} A_{11} & A_{12}\\\\ A_{21} & A_{22} \\end{bmatrix} \\begin{bmatrix} v_1\\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} b_1\\\\ b_2 \\end{bmatrix} It is assumed that the matrix \\mathsf{A} \\in \\mathbb{R}^{2 \\times 2} whose (i,j)^{\\text{th}} entry is A_{ij} is invertible, and, without loss of generality, that A_{21} \\neq 0 Begin by multiplying the second equation in \\sum_{j=1}^2 A_{ij}v_j = b_j by A_{11}/A_{21} . This yields the following modified set of equations \\begin{split} A_{11} v_1 + A_{12} v_2 &= b_1,\\\\ A_{11} v_1 + \\frac{A_{11}}{A_{21}}A_{22} v_2 &= \\frac{A_{11}}{A_{21}}b_2. \\end{split} Retaining the first equation as is and subtracting the first from the second equation, we get \\begin{split} A_{11} v_1 + A_{12} v_2 &= b_1\\\\ \\left(\\frac{A_{11}}{A_{21}}A_{22} - A_{12}\\right)v_2 &= \\frac{A_{11}}{A_{21}}b_2 - b_1. \\end{split} Solving the last equation for v_2 , we immediately see that v_2 = \\frac{1}{A_{11}A_{22} - A_{12}A_{21}}(-A_{21}b_1 + A_{11}b_2). Substituting this expression for v_2 in the first equation and solving for v_1 , we get, after some algebraic manipulation that v_1 = \\frac{1}{A_{11}A_{22} - A_{12}A_{21}}(A_{22}b_1 - A_{12}b_2). Note that this is exactly the solution obtained by solving the system of equations \\mathsf{Av} = \\mathsf{b} using the formula \\mathsf{v} = \\mathsf{A}^{-1}\\mathsf{b} . The foregoing calculations can be visualized as follows. Begin by collecting together the elements of the matrices \\mathsf{A} and \\mathsf{b} as follows: \\begin{bmatrix} A_{11} & A_{12} & b_1\\\\ A_{21} & A_{22} & b_2 \\end{bmatrix} The sequence of transformations carried out earlier can be summarized as follows: \\begin{bmatrix} A_{11} & A_{12} & b_1\\\\ A_{21} & A_{22} & b_2 \\end{bmatrix} \\quad\\rightarrow\\quad \\begin{bmatrix} A_{11} & A_{12} & b_1\\\\ 0 & \\left(\\dfrac{A_{11}}{A_{21}}A_{22} - A_{12}\\right) & \\left(\\dfrac{A_{11}}{A_{21}}b_2 - b_1\\right) \\end{bmatrix}. The matrix in the right hand side of the foregoing transformation is said to be in the upper triangular form and can be solved by back-substitution from the last equation upwards . The solution of a general linear system of equations is computed using an analogous and straightforward extension of this approach. Example As a concrete illustration of the Gaussian elimination algorithm, consider the solution of the following set of linear equations: \\begin{bmatrix} 1 & -2 & 4\\\\ 2 & -3 & 1\\\\ 3 & 2 & -1 \\end{bmatrix} \\begin{bmatrix} v_1\\\\ v_2\\\\ v_3 \\end{bmatrix} = \\begin{bmatrix} 9\\\\ -1\\\\ 4 \\end{bmatrix}. The Gaussian elimination procedure can be illustrated in a sequence of two steps. In the first step, appropriate multiples of the first equations are used to eliminate v_1 from the second and third equations. In the second step, an appropriate multiple of the second equation is used to eliminate v_2 from the third equation. These steps are summarized below: \\begin{bmatrix} 1 & -2 & 4 & 9\\\\ 2 & -3 & 1 & -1\\\\ 3 & 2 & -1 & 4 \\end{bmatrix} \\quad\\rightarrow\\quad \\begin{bmatrix} 1 & -2 & 4 & 9\\\\ 0 & -1 & 7 & 19\\\\ 0 & -8 & 13 & 23 \\end{bmatrix} \\quad\\rightarrow\\quad \\begin{bmatrix} 1 & -2 & 4 & 9\\\\ 0 & -1 & 7 & 19\\\\ 0 & 0 & 43 & 129 \\end{bmatrix} These equations are solve by back-substitution as follows: \\begin{split} 43v_3 = 129 &\\quad\\Rightarrow\\quad v_3 = \\frac{129}{43} = 3,\\\\ -v_2 + 7v_3 = 19 &\\quad\\Rightarrow\\quad v_2 = -(19 - 7\\times3) = 2,\\\\ v_1 - 2v_2 + 4v_3 = 9 &\\quad\\Rightarrow\\quad v_1 = 9 + 2\\times2 - 4\\times3 = 1. \\end{split} We thus obtain the solution of the linear system of equations as v_1 = 1 , v_2 = 2 , and v_3 = 3 . The fact that this is indeed a solution of the linear system of equations presented above can be verified by direct substitution.","title":"Linear systems of equations"},{"location":"matrices/#eigenvalues-and-eigenvectors","text":"To wrap up the discussion of elementary matrix algebra, let us consider the eigenvalue problem . Suppose that we are given a matrix \\mathsf{A} \\in \\mathbb{R}^{n \\times n} of order n . If there exists a vector \\mathsf{v} \\in \\mathbb{R}^{n \\times 1} and a real number a \\in \\mathbb{R} such that \\mathsf{Av} = a\\mathsf{v}, then a is called the eigenvalue of \\mathsf{A} corresponding to the eigenvector \\mathsf{v} . Remark Note that the zero vector \\mathsf{0} \\in \\mathbb{R}^{n \\times 1} trivially satisfies this equation for any choice of a \\in \\mathbb{R} . We will exclude this trivial solution and assume henceforth that every eigenvector is non-zero. Note that the equation \\mathsf{A}\\mathsf{v} = a\\mathsf{v} can be written as the equation (\\mathsf{A} - a\\mathsf{I})\\mathsf{v} = \\mathsf{0}. For this equation to have a non-trivial solution, it follows at once that \\text{det}(\\mathsf{A} - a\\mathsf{I}) = 0. This a polynomial equation in a of degree n , called the characteristic equation of \\mathsf{A} , and has n solutions in general. These solutions correspond to the eigenvalues of the matrix \\mathsf{A} . Example Consider the matrix \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} given as follows: \\mathsf{A} = \\begin{bmatrix} -2 & -4 & 2\\\\ -2 & 1 & 2\\\\ 4 & 2 & 5 \\end{bmatrix}. Let us compute the eigenvalues of \\mathsf{A} by computing its characteristic polynomial. Note first that the matrix \\mathsf{A} - a\\mathsf{I} has the following form: \\mathsf{A} - a\\mathsf{I} = \\begin{bmatrix} -2-a & -4 & 2\\\\ -2 & 1-a & 2\\\\ 4 & 2 & 5-a \\end{bmatrix}. Computing the determinant of this equation and setting it to zero, the characteristic equation \\text{det}(\\mathsf{A} - a\\mathsf{I}) = 0 is obtained as -a^3 + 4a^2 + 27a - 90 = 0, \\quad\\equiv\\quad -(a - 3)(a - 6)(a + 5) = 0. This shows at once that the eigenvalues of the matrix \\mathsf{A} are 3 , 6 , and -5 . The characteristic equation, when expanded fully, has the following structure: (-a)^n + I_1 (-a)^{n-1} + \\ldots + I_n = 0. The constants I_1, I_2, \\ldots, I_n are called the invariants of \\mathsf{A} , and can be related to the eigenvalues of \\mathsf{A} . Notably, the invariants I_1 and I_n are computed as \\begin{split} I_1 &= \\text{tr}(\\mathsf{A}) = \\sum_{i=1}^n a_i,\\\\ I_3 &= \\text{det}(\\mathsf{A}) = \\prod_{i=1}^n a_i. \\end{split} The other invariants of \\mathsf{A} can be similarly related to the eigenvalues of \\mathsf{A} , but they do not have a simple interpretation as I_1 and I_n . Example For the matrix \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} considered in the previous example, the characteristic equation takes the form -a^3 + I_1 a_2 - I_2 a + I_3 = 0. It can be checked that \\begin{split} I_1 = 4 = 3 + 6 - 5 = a_1 + a_2 + a_3,\\\\ I_3 = -90 = 3\\times6\\times(-5) = a_1 a_2 a_3. \\end{split} Further more, for matrices of order three, it is true that I_2 = a_1a_2 + a_2a_3 + a_3a_1, as can be checked with a simple calculation. The Cayley-Hamilton theorem states that the matrix \\mathsf{A} satisfies the characteristic equation: (-\\mathsf{A})^n + I_1 (-\\mathsf{A})^{n-1} + \\ldots + I_n\\mathsf{I} = \\mathsf{0}. This equation is often used in practice to simplify a variety of calculations. The following example provides an elementary illustration of the Cayley-Hamilton theorem. Example Suppose that \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} is an invertible matrix. Then, its inverse can be computed using the Cayley-Hamilton theorem as follows: \\begin{split} & -\\mathsf{A}^3 + I_1\\mathsf{A}^2 - I_2\\mathsf{A} + I_3\\mathsf{I} = 0\\\\ \\Rightarrow & I_3\\mathsf{I} = \\mathsf{A}^3 - I_1\\mathsf{A}^2 + I_2\\mathsf{A}\\\\ \\Rightarrow & \\mathsf{A}^{-1} = \\frac{1}{I_3}\\left(\\mathsf{A}^2 - I_1\\mathsf{A} + I_2\\right). \\end{split} Notice how the fact that I_3 = \\text{det}(\\mathsf{A}) \\neq 0 is used in the last step of this calculation. The Cayley-Hamilton theorem thus provides a convenient expression for the inverse of \\mathsf{A} ; compare this with the general expression for the inverse provided earlier. The eigenvectors of \\mathsf{A} \\in \\mathbb{R}^{n \\times n} can be obtained by substituting the eigenvalues, successively in the equation \\mathsf{A}\\mathsf{v} = a\\mathsf{v} . Notice that \\mathsf{v} is an eigenvector of \\mathsf{A} corresponding to the eigenvalue a , then c\\mathsf{v} is also an eigenvector of \\mathsf{A} corresponding to the same eigenvalue a for any c \\in \\mathbb{R} . This fact is often used to single out a particular value of c , and hence a particular eigenvector. This is best illustrated with an example. Example Consider the matrix \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} considered earlier: \\mathsf{A} = \\begin{bmatrix} -2 & -4 & 2\\\\ -2 & 1 & 2\\\\ 4 & 2 & 5 \\end{bmatrix}. The eigenvalues of \\mathsf{A} were computed earlier as 3, 6, -5 . Let us now compute the corresponding eigenvectors. Consider first the eigenvalue a_1 = 3 . Substituting this in the equation \\mathsf{A}\\mathsf{v}_1 = a_1\\mathsf{v}_1 , we get \\begin{bmatrix} -2 & -4 & 2\\\\ -2 & 1 & 2\\\\ 4 & 2 & 5 \\end{bmatrix} \\begin{bmatrix} v_{1,1}\\\\ v_{1,2}\\\\ v_{1,3} \\end{bmatrix} = 3 \\begin{bmatrix} v_{1,1}\\\\ v_{1,2}\\\\ v_{1,3} \\end{bmatrix} \\quad\\equiv\\quad \\begin{bmatrix} -5 & -4 & 2\\\\ -2 & -2 & 2\\\\ 4 & 2 & 2 \\end{bmatrix} \\begin{bmatrix} v_{1,1}\\\\ v_{1,2}\\\\ v_{1,3} \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix}. In the equations above, we have used the notation (\\mathsf{v}_1)_i = v_{1,i} . Notice that this matrix equation does not have a unique solution since \\text{det}(\\mathsf{A} - a_1\\mathsf{I}) = 0 . To compute all possible solutions, let us compute the components v_{1,1} and v_{1,2} in terms of v_{1,3} . Begin by rewriting the first two equations as \\begin{split} 5v_{1,1} + 4v_{1,2} &= 2v_{1,3},\\\\ 2v_{1,1} + 2v_{1,2} &= 2v_{1,3}. \\end{split} These equations can be readily solved to yield v_{1,1} = -2v_{1,3}, \\qquad v_{2,1} = 3v_{1,3}. Thus, every vector of the form [-2a \\; 3a \\; a]^T \\in \\mathbb{R}^{3 \\times 1} , where a \\in \\mathbb{R} , is an eigenvector of \\mathsf{A} corresponding to the eigenvalue 3 . Different choices for a can be chosen depending on the context. For instance, choosing a = 1 , we obtain the eigenvector [-2 \\; 3 \\; 1]^T . Requiring that the eigenvector has unit length, on the other hand yields the eigenvector [-2/\\sqrt{14} \\; 3/\\sqrt{14} \\; 1/\\sqrt{14}]^T . Remark The dot product of two vectors \\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^{n \\times 1} , written \\mathsf{u}\\cdot\\mathsf{v} \\in \\mathbb{R} , is defined as \\mathsf{u}^T \\mathsf{v} . Note that it follows from the definition that \\mathsf{u} \\cdot \\mathsf{v} = \\mathsf{v}^T \\mathsf{u} . The length of a vector \\mathsf{u} \\in \\mathbb{R}^{n \\times 1} is defined as \\sqrt{\\mathsf{u}^T\\mathsf{u}} . The other eigenvectors of \\mathsf{A} can be computed similarly. It is left as an exercise to check that [1 \\; 6 \\; 16]^T and [-2 \\; -1 \\; 1]^T are the eigenvectors of \\mathsf{A} corresponding to the eigenvalues 6 and -5 , respectively. Suppose that \\mathsf{v}_1, \\ldots, \\mathsf{v}_n \\in \\mathbb{R}^{n \\times 1} are the eigenvectors of \\mathsf{A} \\in \\mathbb{R}^{n \\times n} corresponding to the eigenvalues a_1, \\ldots, a_n \\in \\mathbb{R} , respectively. Then the set of equations \\mathsf{A}\\mathsf{v}_i = a\\mathsf{v}_i , where i = 1, \\ldots, n , can be written compactly as \\mathsf{A}\\mathsf{V} = \\mathsf{V}\\mathsf{D}, where \\mathsf{V} = \\begin{bmatrix} v_{1,1} & v_{2,1} & \\ldots & v_{n,1}\\\\ v_{1,2} & v_{2,2} & \\ldots & v_{n,2}\\\\ \\vdots & & \\ddots & \\vdots\\\\ v_{1,n} & v_{2,n} & \\ldots & v_{n,n} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}, \\qquad \\mathsf{D} = \\text{diag}(a_1, a_2, \\ldots, a_n) \\in \\mathbb{R}^{n \\times n}. Notice that the first column of \\mathsf{V} is the first eigenvector of \\mathsf{A} , the second column of \\mathsf{V} is the second eigenvector of \\mathsf{A} , and so on. It can be shown that if the eigenvectors are linearly independent (A proper definition of this term is provided in these notes in the general context of finite dimensional vector spaces.), then the matrix \\mathsf{V} is invertible. In this case, \\mathsf{A} = \\mathsf{V} \\mathsf{D} \\mathsf{V}^{-1}. This equation is called the eigendecomposition of \\mathsf{A} , and plays an important role in many applications. Example Consider the matrix \\mathsf{A} \\in \\mathbb{R}^{3 \\times 3} considered in the previous examples: \\mathsf{A} = \\begin{bmatrix} -2 & -4 & 2\\\\ -2 & 1 & 2\\\\ 4 & 2 & 5 \\end{bmatrix}. The matrices \\mathsf{V} \\in \\mathbb{R}^{3 \\times 3} and \\mathsf{D}\\in \\mathsf{3 \\times 3} can be constructed as follows: \\mathsf{V} = \\begin{bmatrix} -2 & 1 & -2\\\\ 3 & 6 & -1\\\\ 1 & 16 & 1 \\end{bmatrix}, \\qquad \\mathsf{D} = \\text{diag}(3,6,-5). Note that the matrix \\mathsf{V} is invertible in this case. It is left as a simple exercise to check that \\begin{bmatrix} -2 & -4 & 2\\\\ -2 & 1 & 2\\\\ 4 & 2 & 5 \\end{bmatrix} = \\begin{bmatrix} -2 & 1 & -2\\\\ 3 & 6 & -1\\\\ 1 & 16 & 1 \\end{bmatrix} \\begin{bmatrix} 3 & 0 & 0\\\\ 0 & 6 & 0\\\\ 0 & 0 & -5 \\end{bmatrix} \\begin{bmatrix} -2 & 1 & -2\\\\ 3 & 6 & -1\\\\ 1 & 16 & 1 \\end{bmatrix}^{-1}, thereby verifying the eigendecomposition \\mathsf{A} = \\mathsf{V}\\mathsf{D}\\mathsf{V}^{-1} of \\mathsf{A} . TO DO Degenerate cases","title":"Eigenvalues and eigenvectors"},{"location":"nonlinear_maps/","text":"The foregoing sections dealt with, among other things, the theory of linear maps between finite dimensional inner product spaces. A basic introduction to the treatment of nonlinear maps is now presented. Only certain essential ideas that are needed for the later development of tensor analysis are presented here. Linear approximation of real valued functions of a real variable To begin with, consider the case of a nonlinear function of the form f:\\mathbb{R} \\to\\mathbb{R} . It assumed that f is differentiable, and that its derivative is continuous. The standard approach to study such nonlinear functions is by locally linearizing them. To understand what this means, consider the tangent L_{x_0}:\\mathbb{R}\\to\\mathbb{R} to f at x_0 \\in \\mathbb{R} . The equation for the tangent is given by L_{x_0}(x) = f(x_0) + f'(x_0)(x - x_0). Note that the function L_{x_0} is not linear on account of the constant term f(x_0) . Functions of this form that are linear except for an additive constant are said to be affine . The affine function L_{x_0} is said to locally linearize the nonlinear function f . In what follows, this notion is generalized to the case of nonlinear maps between finite dimensional inner product spaces. Basis representation of nonlinear maps Let \\mathsf{F}:V \\to W be a nonlinear map between finite dimensional inner product spaces V and W of dimension n and m , respectively. Let (\\mathsf{g}_i) and (\\mathsf{f}_i) be bases of V and W , respectively. Define basis maps \\mathsf{\\phi}_V:V \\to \\mathbb{R}^n and \\mathsf{\\phi}_W:W \\to \\mathbb{R}^m as follows: for any \\mathsf{v} \\in V and \\mathsf{w} \\in W , \\begin{split} \\mathsf\\phi_V(\\mathsf{v}) &= \\mathsf\\phi_V\\left(\\sum v_i \\mathsf{g}_i\\right) = (v_1, \\ldots, v_n),\\\\ \\mathsf\\phi_W(\\mathsf{w}) &= \\mathsf\\phi_W\\left(\\sum w_i \\mathsf{f}_i\\right) = (w_1, \\ldots, w_m). \\end{split} The representation of \\mathsf{F}:V \\to W is defined as the map \\mathsf\\phi_W \\circ \\mathsf{F} \\circ \\mathsf\\phi^{-1}_V:\\mathbb{R}^n \\to \\mathbb{R}^m . The following relation readily follows from the definition: (w_1, \\ldots, w_m) = \\mathsf{F}_{V,W}(v_1, \\ldots, v_n), where \\mathsf{F}_{V,W} = \\mathsf\\phi_W \\circ \\mathsf{F} \\circ \\mathsf\\phi^{-1}_V . Since any nonlinear map between finite dimensional inner product spaces can be represented using a nonlinear map between the corresponding Euclidean spaces using the foregoing technique, it suffices to study nonlinear maps between Euclidean spaces only. Basic topological notions in $\\mathbb{R}^n$ In the following development, it will be necessary to study how various quantities vary as we move from a given point \\mathsf{x} \\in U \\subseteq \\mathbb{R}^n to one of its neighboring points. Notice how we call elements of \\mathbb{R}^n as points here; the reason for this terminology is that the eventual application of these ideas is in the context of tensor fields . To define precisely what a neighboring point means, it is helpful to introduce a few definitions. An open ball of radius r centered at \\mathsf{x} \\in \\mathbb{R}^n is the set B_r(\\mathsf{x}) defined as follows: B_r(\\mathsf{x}) = \\{\\mathsf{y} \\in \\mathbb{R}^n \\,|\\, \\lVert \\mathsf{y} - \\mathsf{x} \\rVert < r\\}. B_r(\\mathsf{x}) thus contains all points \\mathsf{y} within a sphere of radius r centered at \\mathsf{x} . A set U \\subseteq \\mathbb{R}^n is said to be open if for every \\mathsf{x} \\in U , there exists an r \\in \\mathbb{R} such that B_r(\\mathsf{x}) \\subseteq U . In a loose sense, every point in an open set is sufficiently inside the set. The reason for why open sets are so useful in practice is the following: when U \\subseteq \\mathbb{R}^n is open, every point \\mathsf{x} \\in U has a neighboring point arbitrarily close to it. Indeed, choosing r \\in \\mathbb{R} such that B_r(\\mathsf{x}) \\subseteq U , the point \\mathsf{x} + t(\\mathsf{y} - \\mathsf{x}) , where 0 < t < 1 and \\mathsf{y} \\in B_r(\\mathsf{x}) , gets arbitrarily close to \\mathsf{x} as t \\to 0 and still remains within U . Consider now a nonlinear map of the form \\mathsf{F}:U \\to \\mathbb{R}^m , where U \\subseteq \\mathbb{R}^n is an open subset of \\mathbb{R}^n . The map \\mathsf{F} is said to be continuous at \\mathsf{x} \\in U if for every scalar \\epsilon > 0 , there exists a scalar \\delta > 0 such that \\lVert \\mathsf{y} - \\mathsf{x} \\rVert < \\delta \\quad\\Rightarrow\\quad \\lVert \\mathsf{F}(\\mathsf{y}) - \\mathsf{F}(\\mathsf{x}) \\rVert < \\epsilon. What this definition encapsulates is the intuitive idea that if \\mathsf{F} is continuous at \\mathsf{x} , then \\mathsf{F}(\\mathsf{y}) gets closer and closer to \\mathsf{F}(\\mathsf{x}) as \\mathsf{y} gets closer and closer to \\mathsf{x} . The map \\mathsf{F} is said to be continuous on U if it is continuous at every \\mathsf{x} \\in U . It is customary to denote the set of all continuous maps from U to \\mathbb{R}^m as C^0(U,\\mathbb{R}^m) . Differentiability of nonlinear maps Let \\mathsf{F}:U \\subseteq\\mathbb{R}^n \\to \\mathbb{R}^m be a nonlinear map from an open subset U of \\mathbb{R}^n into \\mathbb{R}^m , as before. The nonlinear map \\mathsf{F} is said to be differentiable at \\mathsf{x} \\in U if there exists a linear map D_{\\mathsf{x}}\\mathsf{F}:\\mathbb{R}^n \\to \\mathbb{R}^m such that, for any \\mathsf{h} \\in \\mathbb{R}^n , \\lim_{\\lVert\\mathsf{h}\\rVert \\to 0} \\frac{\\lVert \\mathsf{F}(\\mathsf{x} + \\mathsf{h}) - \\mathsf{F}(\\mathsf{x}) - D_{\\mathsf{x}}\\mathsf{F}(\\mathsf{x})\\mathsf{h} \\rVert}{\\lVert \\mathsf{h} \\rVert} = 0, The linear map D_{\\mathsf{x}}\\mathsf{F}(\\mathsf{x}) is called the Fr\u00e9chet derivative of \\mathsf{F} at \\mathsf{x} . If \\mathsf{F} is differentiable at every \\mathsf{x} \\in U then \\mathsf{F} is said to be differentiable on U . The set of all differentiable maps from U \\subseteq \\mathbb{R}^n into \\mathbb{R}^m is notated as C^1(U,\\mathbb{R}^m) . Remark It can be shown that C^1(U,\\mathbb{R}^m) \\subset C^0(U,\\mathbb{R}^m) : every differentiable map is also continuous. The converse is not true. If \\mathsf{F}:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}^m is differentiable on U , then it is convenient to introduce the map D\\mathsf{F}:U \\to L(\\mathbb{R}^n,\\mathbb{R}^m) as follows: for any \\mathsf{x} \\in U , D\\mathsf{F}(\\mathsf{x}) = D_{\\mathsf{x}}\\mathsf{F}. Note that D\\mathsf{F} is, in general, a nonlinear map. Remark Note that given a linear map \\mathsf{T}:\\mathbb{R}^n \\to \\mathbb{R}^m , it is an easy consequence of the definition of the Fr\u00e9chet derivative that D\\mathsf{T} = \\mathsf{T} . Remark Since L(\\mathbb{R}^n,\\mathbb{R}^m) is itself a linear space, it is possible, therefore, to extend the notion of differentiability to D\\mathsf{F} , and define the Fr\u00e9chet derivative D^2\\mathsf{F}:U \\to L(\\mathbb{R}^n,L(\\mathbb{R}^n,\\mathbb{R}^m)) of D\\mathsf{F} as before, after defining a suitable inner product on L(\\mathbb{R}^n,\\mathbb{R}^m) . In this case, \\mathsf{F} is said to be twice differentiable , and it is conventional to denote the set of all such twice differentiable maps as C^2(U,\\mathbb{R}^m) . Higher order derivatives of \\mathsf{F} are defined analogously. If the Fr\u00e9chet derivative of \\mathsf{F} of any order exists, then \\mathsf{F} is said to be a smooth nonlinear map. The set of all smooth maps from U\\subseteq\\mathbb{R}^n into \\mathbb{R}^m is denoted as C^\\infty(U,\\mathbb{R}^m) . All maps considered henceforth will be assumed to be smooth unless stated otherwise. The Fr\u00e9chet derivative D\\mathsf{F} of the nonlinear map \\mathsf{F} provides a locally linear approximation of \\mathsf{F} . The linearization of \\mathsf{F} at \\mathsf{x}_0 \\in U is defined as the map \\mathsf{L}_{\\mathsf{x}_0}:\\mathbb{R}^n \\to \\mathbb{R}^m defined as \\mathsf{L}_{\\mathsf{x}_0}(\\mathsf{x}) = \\mathsf{F}(\\mathsf{x}_0) + D_{\\mathsf{x}_0}\\mathsf{F}\\,(\\mathsf{x} - \\mathsf{x}_0), where \\mathsf{x} \\in \\mathbb{R}^n . Notice how this expression generalizes the tangent line to a real valued function of a real variable discussed earlier. To compute the component representation of the Fr\u00e9chet derivative D_{\\mathsf{x}_0}\\mathsf{F} \\in L(\\mathbb{R}^n,\\mathbb{R}^m) at \\mathsf{x}_0 \\in U \\subseteq \\mathbb{R}^n with respect to the standard bases of \\mathbb{R}^n and \\mathbb{R}^m , note that \\begin{split} [D_{\\mathsf{x}_0}\\mathsf{F}]_{ij} &= \\mathsf{e}_i \\cdot D_{\\mathsf{x}_0}\\mathsf{F} (\\mathsf{e}_j)\\\\ &= \\partial_j F_i(\\mathsf{x}_0). \\end{split} Here, \\mathsf{F} = (F_1, \\ldots, F_m) , where each F_i:U \\to \\mathbb{R} is a real valued function of n variables, and \\partial_j F_i(\\mathsf{x}_0) denote the j^{\\text{th}} partial derivative of F_i evaluated at \\mathsf{x}_0 . The matrix with components \\partial_j F_i(\\mathsf{x}_0) is called the Jacobian matrix of \\mathsf{F} at \\mathsf{x}_0 . The foregoing argument also shows that the basis representation of D_{\\mathsf{x}_0}\\mathsf{F} with respect to the standard bases of \\mathbb{R}^n and \\mathbb{R}^m is D_{\\mathsf{x}_0}\\mathsf{F} = \\sum \\partial_j F_i(\\mathsf{x}_0) \\mathsf{e}_i \\otimes \\mathsf{e}_j. The representation of D_{\\mathsf{x}_0}\\mathsf{F} with respect to arbitrary bases of \\mathbb{R}^n and \\mathbb{R}^m can be computed analogously, but is omitted here in the interest of keeping the development simple. Properties of the Fr\u00e9chet derivative Two important properties of the Fr\u00e9chet derivative of nonlinear maps are now discussed briefly. The first is known as the chain rule of differentiation. Given differentiable maps \\mathsf{F}:\\mathbb{R}^m \\to \\mathbb{R}^k and \\mathsf{G}:\\mathbb{R}^n \\to \\mathbb{R}^m , it can be shown that D(\\mathsf{F} \\circ \\mathsf{G}) = (D\\mathsf{F} \\circ \\mathsf{G}) \\circ D\\mathsf{G}. This is a generalization of the chain rule of differentiation in single variable calculus. This result can be established easily by working in the component representation of the Fr\u00e9chet derivatives. Choosing the standard bases of all the Euclidean spaces involved, it can be shown that _{ij} = \\sum \\partial_k F_i(\\mathsf{G}(\\mathsf{x}_0)) \\, \\partial_j G_k(\\mathsf{x}_0), for any \\mathsf{x}_0 \\in \\mathbb{R}^n . The second property of the Fr\u00e9chet derivative that is useful in applications relates to the local invertibility of nonlinear maps. Let \\mathsf{F}:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}^n be a given differentiable nonlinear map. If D_{\\mathsf{x}_0}\\mathsf{F}:\\mathbb{R}^n \\to \\mathbb{R}^n is invertible, where \\mathsf{x}_0 \\in U , then the inverse function theorem states that the map \\mathsf{F} is locally invertible at \\mathsf{F}(\\mathsf{x}_0) \\in \\mathbb{R}^n , and further that D_{\\mathsf{F}(\\mathsf{x}_0)}\\mathsf{F}^{-1} = (D_{\\mathsf{x}_0}\\mathsf{F})^{-1}. The proof of this theorem is non-trivial, and can be found in any good book on multivariable calculus. Directional derivatives Given a nonlinear map \\mathsf{F}:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}^m , where U is open in \\mathbb{R}^n , it is convenient to define a weaker notion of a derivative than the Fr\u00e9chet derivative called the directional derivative of \\mathsf{F} . The basic idea is to study how \\mathsf{F} varies along a particular direction. The map \\mathsf{F}:U \\to \\mathbb{R}^m is said to be G\u00e2teaux differentiable at \\mathsf{x}_0 \\in U if there exists a map \\delta_{\\mathsf{x}_0} \\mathsf{F}:\\mathbb{R}^n \\to \\mathbb{R}^m such that \\delta_{\\mathsf{x}_0} \\mathsf{F}(\\mathsf{h}) = \\lim_{t \\to 0} \\frac{\\mathsf{F}(\\mathsf{x}_0 + t\\mathsf{h}) - \\mathsf{F}(\\mathsf{x}_0)}{t} = \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{F}(\\mathsf{x}_0 + t\\mathsf{h}), for any \\mathsf{h} \\in \\mathbb{R}^n . The map \\delta_{\\mathsf{x}_0}\\mathsf{F} is also called the directional derivative of \\mathsf{F} at \\mathsf{x}_0 along the direction \\mathsf{h} . If \\mathsf{F} is G\u00e2teaux differentiable at every \\mathsf{x}_0 \\in U , then the G\u00e2teaux differential \\delta \\mathsf{F}:U \\times \\mathbb{R}^n \\to \\mathbb{R}^m of \\mathsf{F} is defined as \\delta \\mathsf{F}(\\mathsf{x}_0, \\mathsf{h}) = \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{F}(\\mathsf{x}_0 + t\\mathsf{h}), where \\mathsf{h} \\in \\mathbb{R}^n . Remark It is to be emphasized that \\delta_{\\mathsf{x}_0} \\mathsf{F}:\\mathbb{R}^n \\to \\mathbb{R}^m is not necessarily a linear map. If it turns out that the G\u00e2teaux differential \\delta \\mathsf{F}:U \\times \\mathbb{R}^n \\to \\mathbb{R}^m is linear in its second argument, it is usually written using one of the following equivalent notations: \\delta \\mathsf{F}(\\mathsf{x}_0, \\mathsf{h}) = \\frac{\\delta \\mathsf{F}}{\\delta \\mathsf{x}}\\bigg\\vert_{\\mathsf{x}_0}\\mathsf{h} = \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0) \\mathsf{h}, where \\mathsf{x}_0 \\in U and \\mathsf{h} \\in \\mathbb{R}^n . The latter notation will be adopted more frequently in these notes. The linear map \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0):\\mathbb{R}^n \\to \\mathbb{R}^m is called the G\u00e2teaux derivative , or the functional derivative , of \\mathsf{F} at \\mathsf{x}_0 . If the map \\mathsf{F}:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}^m is Fr\u00e9chet differentiable, then it is necessarily G\u00e2teaux differentiable on U . Further, in this case, the G\u00e2teaux differential is linear in its second argument, and (D_{\\mathsf{x}_0}\\mathsf{F})\\mathsf{h} = \\delta \\mathsf{F}(\\mathsf{x}_0, \\mathsf{h}) = \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0) \\mathsf{h}, for any \\mathsf{x}_0 \\in U and \\mathsf{h} \\in \\mathbb{R}^n . This shows, in particular, that under the assumption of Fr\u00e9chet differentiability, the G\u00e2teaux differential can be used to compute the Fr\u00e9chet derivative. Remark The converse is not true: the existence of the G\u00e2teaux differential does not imply Fr\u00e9chet differntiability, except when the G\u00e2teaux differential satisfies additional conditions. In these notes, all maps between vector spaces are assumed to be Fr\u00e9chet differentiable, unless stated otherwise. Gradient of nonlinear maps We will now introduce an important notion called the gradient of a nonlinear map. Suppose that \\mathsf{F}:V \\to W is a nonlinear map between finite dimensional inner product spaces as before that is differentiable. In this case, the Fr\\'echet derivative of \\mathsf{F} is equal to its G\\^ateaux derivative, as we just discussed. To introduce the notion of the gradient of \\mathsf{F} , it is helpful to first introduce the notion of tensor product spaces . Given finite dimensional inner product spaces V, W , the tensor product space W \\otimes V is defined as follows: W \\otimes V = \\{\\mathsf{T}:W \\times V \\to \\mathbb{R} \\,|\\, \\mathsf{T} \\text{ is a multilinear map}\\}. This is best illustrated with the help of some examples. Example TO DO ... The gradient of \\mathsf{F} at \\mathsf{x}_0 \\in V is defined as the tensor \\text{grad }\\mathsf{F}(\\mathsf{x}_0) \\in W \\otimes V such that, for any \\mathsf{h} \\in V , \\text{grad }\\mathsf{F}(\\mathsf{x}_0) \\cdot \\mathsf{h} = \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0)\\mathsf{h}. Remark It is conventional to denote the gradient of \\mathsf{F} using the same symbol as the G\\^ateaux derivative of F . Thus, we will often write \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0) to denote the gradient of \\mathsf{F} at \\mathsf{x}_0 , \\text{grad }\\mathsf{F}(\\mathsf{x}_0) . The meaning of a term like \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0) should thus be carefully interpreted depending on the context.","title":"Linearization of Nonlinear Maps"},{"location":"nonlinear_maps/#linear-approximation-of-real-valued-functions-of-a-real-variable","text":"To begin with, consider the case of a nonlinear function of the form f:\\mathbb{R} \\to\\mathbb{R} . It assumed that f is differentiable, and that its derivative is continuous. The standard approach to study such nonlinear functions is by locally linearizing them. To understand what this means, consider the tangent L_{x_0}:\\mathbb{R}\\to\\mathbb{R} to f at x_0 \\in \\mathbb{R} . The equation for the tangent is given by L_{x_0}(x) = f(x_0) + f'(x_0)(x - x_0). Note that the function L_{x_0} is not linear on account of the constant term f(x_0) . Functions of this form that are linear except for an additive constant are said to be affine . The affine function L_{x_0} is said to locally linearize the nonlinear function f . In what follows, this notion is generalized to the case of nonlinear maps between finite dimensional inner product spaces.","title":"Linear approximation of real valued functions of a real variable"},{"location":"nonlinear_maps/#basis-representation-of-nonlinear-maps","text":"Let \\mathsf{F}:V \\to W be a nonlinear map between finite dimensional inner product spaces V and W of dimension n and m , respectively. Let (\\mathsf{g}_i) and (\\mathsf{f}_i) be bases of V and W , respectively. Define basis maps \\mathsf{\\phi}_V:V \\to \\mathbb{R}^n and \\mathsf{\\phi}_W:W \\to \\mathbb{R}^m as follows: for any \\mathsf{v} \\in V and \\mathsf{w} \\in W , \\begin{split} \\mathsf\\phi_V(\\mathsf{v}) &= \\mathsf\\phi_V\\left(\\sum v_i \\mathsf{g}_i\\right) = (v_1, \\ldots, v_n),\\\\ \\mathsf\\phi_W(\\mathsf{w}) &= \\mathsf\\phi_W\\left(\\sum w_i \\mathsf{f}_i\\right) = (w_1, \\ldots, w_m). \\end{split} The representation of \\mathsf{F}:V \\to W is defined as the map \\mathsf\\phi_W \\circ \\mathsf{F} \\circ \\mathsf\\phi^{-1}_V:\\mathbb{R}^n \\to \\mathbb{R}^m . The following relation readily follows from the definition: (w_1, \\ldots, w_m) = \\mathsf{F}_{V,W}(v_1, \\ldots, v_n), where \\mathsf{F}_{V,W} = \\mathsf\\phi_W \\circ \\mathsf{F} \\circ \\mathsf\\phi^{-1}_V . Since any nonlinear map between finite dimensional inner product spaces can be represented using a nonlinear map between the corresponding Euclidean spaces using the foregoing technique, it suffices to study nonlinear maps between Euclidean spaces only.","title":"Basis representation of nonlinear maps"},{"location":"nonlinear_maps/#basic-topological-notions-in-mathbbrn","text":"In the following development, it will be necessary to study how various quantities vary as we move from a given point \\mathsf{x} \\in U \\subseteq \\mathbb{R}^n to one of its neighboring points. Notice how we call elements of \\mathbb{R}^n as points here; the reason for this terminology is that the eventual application of these ideas is in the context of tensor fields . To define precisely what a neighboring point means, it is helpful to introduce a few definitions. An open ball of radius r centered at \\mathsf{x} \\in \\mathbb{R}^n is the set B_r(\\mathsf{x}) defined as follows: B_r(\\mathsf{x}) = \\{\\mathsf{y} \\in \\mathbb{R}^n \\,|\\, \\lVert \\mathsf{y} - \\mathsf{x} \\rVert < r\\}. B_r(\\mathsf{x}) thus contains all points \\mathsf{y} within a sphere of radius r centered at \\mathsf{x} . A set U \\subseteq \\mathbb{R}^n is said to be open if for every \\mathsf{x} \\in U , there exists an r \\in \\mathbb{R} such that B_r(\\mathsf{x}) \\subseteq U . In a loose sense, every point in an open set is sufficiently inside the set. The reason for why open sets are so useful in practice is the following: when U \\subseteq \\mathbb{R}^n is open, every point \\mathsf{x} \\in U has a neighboring point arbitrarily close to it. Indeed, choosing r \\in \\mathbb{R} such that B_r(\\mathsf{x}) \\subseteq U , the point \\mathsf{x} + t(\\mathsf{y} - \\mathsf{x}) , where 0 < t < 1 and \\mathsf{y} \\in B_r(\\mathsf{x}) , gets arbitrarily close to \\mathsf{x} as t \\to 0 and still remains within U . Consider now a nonlinear map of the form \\mathsf{F}:U \\to \\mathbb{R}^m , where U \\subseteq \\mathbb{R}^n is an open subset of \\mathbb{R}^n . The map \\mathsf{F} is said to be continuous at \\mathsf{x} \\in U if for every scalar \\epsilon > 0 , there exists a scalar \\delta > 0 such that \\lVert \\mathsf{y} - \\mathsf{x} \\rVert < \\delta \\quad\\Rightarrow\\quad \\lVert \\mathsf{F}(\\mathsf{y}) - \\mathsf{F}(\\mathsf{x}) \\rVert < \\epsilon. What this definition encapsulates is the intuitive idea that if \\mathsf{F} is continuous at \\mathsf{x} , then \\mathsf{F}(\\mathsf{y}) gets closer and closer to \\mathsf{F}(\\mathsf{x}) as \\mathsf{y} gets closer and closer to \\mathsf{x} . The map \\mathsf{F} is said to be continuous on U if it is continuous at every \\mathsf{x} \\in U . It is customary to denote the set of all continuous maps from U to \\mathbb{R}^m as C^0(U,\\mathbb{R}^m) .","title":"Basic topological notions in $\\mathbb{R}^n$"},{"location":"nonlinear_maps/#differentiability-of-nonlinear-maps","text":"Let \\mathsf{F}:U \\subseteq\\mathbb{R}^n \\to \\mathbb{R}^m be a nonlinear map from an open subset U of \\mathbb{R}^n into \\mathbb{R}^m , as before. The nonlinear map \\mathsf{F} is said to be differentiable at \\mathsf{x} \\in U if there exists a linear map D_{\\mathsf{x}}\\mathsf{F}:\\mathbb{R}^n \\to \\mathbb{R}^m such that, for any \\mathsf{h} \\in \\mathbb{R}^n , \\lim_{\\lVert\\mathsf{h}\\rVert \\to 0} \\frac{\\lVert \\mathsf{F}(\\mathsf{x} + \\mathsf{h}) - \\mathsf{F}(\\mathsf{x}) - D_{\\mathsf{x}}\\mathsf{F}(\\mathsf{x})\\mathsf{h} \\rVert}{\\lVert \\mathsf{h} \\rVert} = 0, The linear map D_{\\mathsf{x}}\\mathsf{F}(\\mathsf{x}) is called the Fr\u00e9chet derivative of \\mathsf{F} at \\mathsf{x} . If \\mathsf{F} is differentiable at every \\mathsf{x} \\in U then \\mathsf{F} is said to be differentiable on U . The set of all differentiable maps from U \\subseteq \\mathbb{R}^n into \\mathbb{R}^m is notated as C^1(U,\\mathbb{R}^m) . Remark It can be shown that C^1(U,\\mathbb{R}^m) \\subset C^0(U,\\mathbb{R}^m) : every differentiable map is also continuous. The converse is not true. If \\mathsf{F}:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}^m is differentiable on U , then it is convenient to introduce the map D\\mathsf{F}:U \\to L(\\mathbb{R}^n,\\mathbb{R}^m) as follows: for any \\mathsf{x} \\in U , D\\mathsf{F}(\\mathsf{x}) = D_{\\mathsf{x}}\\mathsf{F}. Note that D\\mathsf{F} is, in general, a nonlinear map. Remark Note that given a linear map \\mathsf{T}:\\mathbb{R}^n \\to \\mathbb{R}^m , it is an easy consequence of the definition of the Fr\u00e9chet derivative that D\\mathsf{T} = \\mathsf{T} . Remark Since L(\\mathbb{R}^n,\\mathbb{R}^m) is itself a linear space, it is possible, therefore, to extend the notion of differentiability to D\\mathsf{F} , and define the Fr\u00e9chet derivative D^2\\mathsf{F}:U \\to L(\\mathbb{R}^n,L(\\mathbb{R}^n,\\mathbb{R}^m)) of D\\mathsf{F} as before, after defining a suitable inner product on L(\\mathbb{R}^n,\\mathbb{R}^m) . In this case, \\mathsf{F} is said to be twice differentiable , and it is conventional to denote the set of all such twice differentiable maps as C^2(U,\\mathbb{R}^m) . Higher order derivatives of \\mathsf{F} are defined analogously. If the Fr\u00e9chet derivative of \\mathsf{F} of any order exists, then \\mathsf{F} is said to be a smooth nonlinear map. The set of all smooth maps from U\\subseteq\\mathbb{R}^n into \\mathbb{R}^m is denoted as C^\\infty(U,\\mathbb{R}^m) . All maps considered henceforth will be assumed to be smooth unless stated otherwise. The Fr\u00e9chet derivative D\\mathsf{F} of the nonlinear map \\mathsf{F} provides a locally linear approximation of \\mathsf{F} . The linearization of \\mathsf{F} at \\mathsf{x}_0 \\in U is defined as the map \\mathsf{L}_{\\mathsf{x}_0}:\\mathbb{R}^n \\to \\mathbb{R}^m defined as \\mathsf{L}_{\\mathsf{x}_0}(\\mathsf{x}) = \\mathsf{F}(\\mathsf{x}_0) + D_{\\mathsf{x}_0}\\mathsf{F}\\,(\\mathsf{x} - \\mathsf{x}_0), where \\mathsf{x} \\in \\mathbb{R}^n . Notice how this expression generalizes the tangent line to a real valued function of a real variable discussed earlier. To compute the component representation of the Fr\u00e9chet derivative D_{\\mathsf{x}_0}\\mathsf{F} \\in L(\\mathbb{R}^n,\\mathbb{R}^m) at \\mathsf{x}_0 \\in U \\subseteq \\mathbb{R}^n with respect to the standard bases of \\mathbb{R}^n and \\mathbb{R}^m , note that \\begin{split} [D_{\\mathsf{x}_0}\\mathsf{F}]_{ij} &= \\mathsf{e}_i \\cdot D_{\\mathsf{x}_0}\\mathsf{F} (\\mathsf{e}_j)\\\\ &= \\partial_j F_i(\\mathsf{x}_0). \\end{split} Here, \\mathsf{F} = (F_1, \\ldots, F_m) , where each F_i:U \\to \\mathbb{R} is a real valued function of n variables, and \\partial_j F_i(\\mathsf{x}_0) denote the j^{\\text{th}} partial derivative of F_i evaluated at \\mathsf{x}_0 . The matrix with components \\partial_j F_i(\\mathsf{x}_0) is called the Jacobian matrix of \\mathsf{F} at \\mathsf{x}_0 . The foregoing argument also shows that the basis representation of D_{\\mathsf{x}_0}\\mathsf{F} with respect to the standard bases of \\mathbb{R}^n and \\mathbb{R}^m is D_{\\mathsf{x}_0}\\mathsf{F} = \\sum \\partial_j F_i(\\mathsf{x}_0) \\mathsf{e}_i \\otimes \\mathsf{e}_j. The representation of D_{\\mathsf{x}_0}\\mathsf{F} with respect to arbitrary bases of \\mathbb{R}^n and \\mathbb{R}^m can be computed analogously, but is omitted here in the interest of keeping the development simple.","title":"Differentiability of nonlinear maps"},{"location":"nonlinear_maps/#properties-of-the-frechet-derivative","text":"Two important properties of the Fr\u00e9chet derivative of nonlinear maps are now discussed briefly. The first is known as the chain rule of differentiation. Given differentiable maps \\mathsf{F}:\\mathbb{R}^m \\to \\mathbb{R}^k and \\mathsf{G}:\\mathbb{R}^n \\to \\mathbb{R}^m , it can be shown that D(\\mathsf{F} \\circ \\mathsf{G}) = (D\\mathsf{F} \\circ \\mathsf{G}) \\circ D\\mathsf{G}. This is a generalization of the chain rule of differentiation in single variable calculus. This result can be established easily by working in the component representation of the Fr\u00e9chet derivatives. Choosing the standard bases of all the Euclidean spaces involved, it can be shown that _{ij} = \\sum \\partial_k F_i(\\mathsf{G}(\\mathsf{x}_0)) \\, \\partial_j G_k(\\mathsf{x}_0), for any \\mathsf{x}_0 \\in \\mathbb{R}^n . The second property of the Fr\u00e9chet derivative that is useful in applications relates to the local invertibility of nonlinear maps. Let \\mathsf{F}:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}^n be a given differentiable nonlinear map. If D_{\\mathsf{x}_0}\\mathsf{F}:\\mathbb{R}^n \\to \\mathbb{R}^n is invertible, where \\mathsf{x}_0 \\in U , then the inverse function theorem states that the map \\mathsf{F} is locally invertible at \\mathsf{F}(\\mathsf{x}_0) \\in \\mathbb{R}^n , and further that D_{\\mathsf{F}(\\mathsf{x}_0)}\\mathsf{F}^{-1} = (D_{\\mathsf{x}_0}\\mathsf{F})^{-1}. The proof of this theorem is non-trivial, and can be found in any good book on multivariable calculus.","title":"Properties of the Fr\u00e9chet derivative"},{"location":"nonlinear_maps/#directional-derivatives","text":"Given a nonlinear map \\mathsf{F}:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}^m , where U is open in \\mathbb{R}^n , it is convenient to define a weaker notion of a derivative than the Fr\u00e9chet derivative called the directional derivative of \\mathsf{F} . The basic idea is to study how \\mathsf{F} varies along a particular direction. The map \\mathsf{F}:U \\to \\mathbb{R}^m is said to be G\u00e2teaux differentiable at \\mathsf{x}_0 \\in U if there exists a map \\delta_{\\mathsf{x}_0} \\mathsf{F}:\\mathbb{R}^n \\to \\mathbb{R}^m such that \\delta_{\\mathsf{x}_0} \\mathsf{F}(\\mathsf{h}) = \\lim_{t \\to 0} \\frac{\\mathsf{F}(\\mathsf{x}_0 + t\\mathsf{h}) - \\mathsf{F}(\\mathsf{x}_0)}{t} = \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{F}(\\mathsf{x}_0 + t\\mathsf{h}), for any \\mathsf{h} \\in \\mathbb{R}^n . The map \\delta_{\\mathsf{x}_0}\\mathsf{F} is also called the directional derivative of \\mathsf{F} at \\mathsf{x}_0 along the direction \\mathsf{h} . If \\mathsf{F} is G\u00e2teaux differentiable at every \\mathsf{x}_0 \\in U , then the G\u00e2teaux differential \\delta \\mathsf{F}:U \\times \\mathbb{R}^n \\to \\mathbb{R}^m of \\mathsf{F} is defined as \\delta \\mathsf{F}(\\mathsf{x}_0, \\mathsf{h}) = \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{F}(\\mathsf{x}_0 + t\\mathsf{h}), where \\mathsf{h} \\in \\mathbb{R}^n . Remark It is to be emphasized that \\delta_{\\mathsf{x}_0} \\mathsf{F}:\\mathbb{R}^n \\to \\mathbb{R}^m is not necessarily a linear map. If it turns out that the G\u00e2teaux differential \\delta \\mathsf{F}:U \\times \\mathbb{R}^n \\to \\mathbb{R}^m is linear in its second argument, it is usually written using one of the following equivalent notations: \\delta \\mathsf{F}(\\mathsf{x}_0, \\mathsf{h}) = \\frac{\\delta \\mathsf{F}}{\\delta \\mathsf{x}}\\bigg\\vert_{\\mathsf{x}_0}\\mathsf{h} = \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0) \\mathsf{h}, where \\mathsf{x}_0 \\in U and \\mathsf{h} \\in \\mathbb{R}^n . The latter notation will be adopted more frequently in these notes. The linear map \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0):\\mathbb{R}^n \\to \\mathbb{R}^m is called the G\u00e2teaux derivative , or the functional derivative , of \\mathsf{F} at \\mathsf{x}_0 . If the map \\mathsf{F}:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}^m is Fr\u00e9chet differentiable, then it is necessarily G\u00e2teaux differentiable on U . Further, in this case, the G\u00e2teaux differential is linear in its second argument, and (D_{\\mathsf{x}_0}\\mathsf{F})\\mathsf{h} = \\delta \\mathsf{F}(\\mathsf{x}_0, \\mathsf{h}) = \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0) \\mathsf{h}, for any \\mathsf{x}_0 \\in U and \\mathsf{h} \\in \\mathbb{R}^n . This shows, in particular, that under the assumption of Fr\u00e9chet differentiability, the G\u00e2teaux differential can be used to compute the Fr\u00e9chet derivative. Remark The converse is not true: the existence of the G\u00e2teaux differential does not imply Fr\u00e9chet differntiability, except when the G\u00e2teaux differential satisfies additional conditions. In these notes, all maps between vector spaces are assumed to be Fr\u00e9chet differentiable, unless stated otherwise.","title":"Directional derivatives"},{"location":"nonlinear_maps/#gradient-of-nonlinear-maps","text":"We will now introduce an important notion called the gradient of a nonlinear map. Suppose that \\mathsf{F}:V \\to W is a nonlinear map between finite dimensional inner product spaces as before that is differentiable. In this case, the Fr\\'echet derivative of \\mathsf{F} is equal to its G\\^ateaux derivative, as we just discussed. To introduce the notion of the gradient of \\mathsf{F} , it is helpful to first introduce the notion of tensor product spaces . Given finite dimensional inner product spaces V, W , the tensor product space W \\otimes V is defined as follows: W \\otimes V = \\{\\mathsf{T}:W \\times V \\to \\mathbb{R} \\,|\\, \\mathsf{T} \\text{ is a multilinear map}\\}. This is best illustrated with the help of some examples. Example TO DO ... The gradient of \\mathsf{F} at \\mathsf{x}_0 \\in V is defined as the tensor \\text{grad }\\mathsf{F}(\\mathsf{x}_0) \\in W \\otimes V such that, for any \\mathsf{h} \\in V , \\text{grad }\\mathsf{F}(\\mathsf{x}_0) \\cdot \\mathsf{h} = \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0)\\mathsf{h}. Remark It is conventional to denote the gradient of \\mathsf{F} using the same symbol as the G\\^ateaux derivative of F . Thus, we will often write \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0) to denote the gradient of \\mathsf{F} at \\mathsf{x}_0 , \\text{grad }\\mathsf{F}(\\mathsf{x}_0) . The meaning of a term like \\partial_{\\mathsf{x}} \\mathsf{F}(\\mathsf{x}_0) should thus be carefully interpreted depending on the context.","title":"Gradient of nonlinear maps"},{"location":"set_theory/","text":"A review of a few basic ideas from set theory is provided here. Set theory forms the foundation for almost all of modern mathematics. Indeed, one can say that modern mathematics is the study of sets. We will not make any attempts to define rigorously the notion of a set. Certain elementary definitions are provided here for convenient reference. More definitions will be introduced later on as and when they are required. Describing a set Loosely speaking, a set is a collection of elements. These elements are said to belong to the set. Given a set S , the statement that an element a belongs to S is abbreviated as a \\in S. The negation of this statement, that a does not belong to S , is written as a \\notin S. The set consisting of n elements a_1, a_2, \\ldots, a_n is written as \\{a_1,a_2,\\ldots,a_n\\}. Example Let us collect together the list C of India\u2019s Twenty20 cricket captains so far (as of 2019): C = \\{\\text{Sehwag}, \\text{Dhoni}, \\text{Raina}, \\text{Rahane}, \\text{Kohli}, \\text{Sharma}\\}. (I have listed only the last names since the full names would make the list too long.) We denote the fact that \\text{Dhoni} was a captain of the Indian Twenty20 cricket team as \\text{Dhoni} \\in C. Similarly, we denote the fact that \\text{Tendulkar} was not a captain of the Indian Twenty20 cricket team using the notation \\text{Tendulkar} \\notin C. Notice how the set-theoretic notation \\text{Tendulkar} \\notin C succinctly represents the more verbose statement \u2018Tendulkar was not a captain of the Indian Twenty20 cricket team\u2019 . Remark Whenever you see a mathematical expression, keep in mind that it just summarizes a possibly long, or, complex idea - it is a good idea to read it out fully until you gain enough intuition to think abstractly. A more powerful notation to represent a set is as follows. Suppose we are given a property P(a) that any element a belonging to a set S may, or may not, satisfy. The set of all elements in S that satisfy the property P is written as \\{a \\in S \\,|\\, P(a)\\}. The above statement is read \u2018the set of all a in S for which P(a) is true\u2019 . Example Continuing the earlier example involving the set C of all Indian Twenty20 cricket team (as of 2019), notice that only \\text{Rahane} and \\text{Sharma} represented Mumbai, in the domestic matches. Using the notation just introduced, we see that the set \\{\\text{Rahane}, \\text{Sharma}\\} consisting of just those Indian Twenty20 captains until 2019 who played for Mumbai as \\{A \\in C \\,|\\, A\\text{ played for Mumbai}\\}. It is helpful to postulate the existence of a special set called the empty set that contains nothing. The standard notation for the empty set is \\emptyset . Example Returning to the set C of all Indian Twenty20 captains, it turns out that none of them were born in Mumbai. In set-theoretic notation, we write this as \\{A \\in C \\,|\\, A\\text{ was born in Mumbai}\\} = \\emptyset. Finally, we will always work within the confines of a universal set , which contains everything that we are dealing with in a particular context. Example In all the earlier examples involving the set C of all Indian Twenty20 captains, a convenient choice of universal set is the set P of all cricketers who represented India at the international level in cricket matches: P = \\{\\text{Indians who played for India in at least one international cricket match}\\}. Note that the set C can be obtained from P as follows: C = \\{A \\in P \\,|\\, A\\text{ captained India in a Twenty20 match}\\}. Remark Note that the choice of universal set is not unique. In the examples considered above, we might just as well have chosen the set I of all Indians who were born, say, after 1900, as our universal set. In practice, the universal set is often implicitly understood from the context; it is always a good idea to be aware of it. Numerical sets While set theory is very broad in its purview, we will limit our discussion henceforth to studying the properties of special kinds of sets. In particular, we will find the following numerical sets to be very useful: The set of all natural numbers : \\mathbb{N} = \\{1, 2, 3, \\ldots \\} . The set of all integers : \\mathbb{Z} = \\{\\ldots, -2, -1, 0, 1, 2, \\ldots\\} . The set of all rational numbers , which are numbers of the form p/q where p \\in \\mathbb{Z} and q \\in \\mathbb{N} , will be denoted by the symbol \\mathbb{Q} . Finally, the set of all real numbers will be denoted by the symbol \\mathbb{R} . We will sometimes use the symbol \\mathbb{R}_+ to denote the set of non-negative real numbers: \\mathbb{R}_+ = \\{a \\in \\mathbb{R} \\,|\\, a \\ge 0\\} . Remark In the modern mathematical formalism, all the numerical sets introduced above are constructed from the empty set \\emptyset . Understanding how this is done is beyond the scope of these notes. Any good book on real analysis should, however, provide the necessary details. All the discussions henceforth will be confined to numerical sets, or sets constructed from numerical sets, unless stated otherwise. Subsets, inclusion and equality A set S is said to be a subset of another set T , written S \\subseteq T , if, and only if, it is true that whenever a \\in S , it is also the case that a \\in T . Formally, S \\subseteq T \\,\\Leftrightarrow\\, a\\in S \\Rightarrow a \\in T. Remark We will use a variety of logical qualifiers in these notes. The symbol \\forall stands for the logical qualifier for all . We also use the logical qualifiers \\exists and \\exists! to represent the notions there exists , and there exists a unique , respectively. The symbol \\Rightarrow stands for implication . Thus, a \\Rightarrow b is read b if a , or a only if b . Finally, the symbol \\Leftrightarrow stands for equivalence . Thus, a \\Leftrightarrow b is read a if, and only if, b . We will henceforth abbreviate the phrase if, and only if as iff . Example We will use the following subsets of \\mathbb{R} to be very useful: for any a,b \\in \\mathbb{R} , \\begin{split} (a,b) &= \\{x \\in \\mathbb{R}\\,|\\, a < x < b\\},\\\\ (a,b] &= \\{x \\in \\mathbb{R}\\,|\\, a < x \\le b\\},\\\\ [a,b) &= \\{x \\in \\mathbb{R}\\,|\\, a \\le x < b\\},\\\\ [a,b] &= \\{x \\in \\mathbb{R}\\,|\\, a \\le x \\le b\\},\\\\ (a,\\infty) &= \\{x \\in \\mathbb{R}\\,|\\, x > a\\},\\\\ [a,\\infty) &= \\{x \\in \\mathbb{R}\\,|\\, x \\ge a\\},\\\\ (\\infty,a) &= \\{x \\in \\mathbb{R}\\,|\\, x < a\\},\\\\ (\\infty,a] &= \\{x \\in \\mathbb{R}\\,|\\, x \\le a\\}. \\end{split} The phrase S is contained in T is also used for S \\subseteq T . This can alternatively be stated as, T \\supseteq S , and we say that T is a superset of S , or that T contains S . S is said to be a strict subset of T , written S \\subsetneq T , if S is a subset of T , and there exists an element a \\in T such that a \\notin S . Formally, S \\subsetneq T \\,\\Leftrightarrow\\, S \\subseteq T, \\text{ and } \\exists\\, a \\in T \\text{ such that } a \\notin S. Example Note that the following inclusion relation holds among the numerical sets introduced earlier: \\emptyset \\subseteq \\mathbb{N} \\subseteq \\mathbb{Z} \\subseteq \\mathbb{Q} \\subseteq \\mathbb{R}. Finally, two sets S and T are said to be equal , written S = T , if both S \\subseteq T and T \\subseteq S . In other words, every element in S belongs to T , and vice versa. Formally, S = T \\,\\Leftrightarrow\\, S \\subseteq T \\text{ and } T \\subseteq S \\,\\Leftrightarrow\\, (a \\in S \\Leftrightarrow a \\in T). This is an important strategy to prove the equality of two sets. Example Let us consider the two sets S = [-1,1] \\subseteq \\mathbb{R}, and T = \\{ \\sin x \\,|\\, x \\in \\mathbb{R} \\} \\subseteq \\mathbb{R} For any y \\in S , note that it is possible to find a unique x \\in [-\\pi/2,\\pi/2] such that y = \\sin x \\in T . Thus, we see that S \\subseteq T . To prove the reverse inclusion, note that for any x \\in \\mathbb{R} , \\sin x \\in [-1,1] , and hence it is true that T \\subseteq S . We have thus shown that S = [-1,1] = \\{ \\sin x \\,|\\, x \\in \\mathbb{R} \\} = T. Families of sets We will now introduce a useful notation that will serve us well in the subsequent development of set theory. A family of sets indexed by the set I is a set S whose elements are sets S_i where i \\in I . In symbols, S = \\{S_i \\,|\\, i \\in I\\}. The set I is also called the index set for the family. It is also conventional to represent the family as \\{S_i\\}_{i\\in I}. Example Suppose that we have 4 sets S_1, S_2, S_3, S_4 defined as follows: \\begin{split} S_1 &= [0, 1) \\subseteq \\mathbb{R},\\\\ S_2 &= [0, 1/2) \\subseteq \\mathbb{R},\\\\ S_3 &= [0, 1/3) \\subseteq \\mathbb{R},\\\\ S_4 &= [0, 1/4) \\subseteq \\mathbb{R}. \\end{split} Let I_n \\subseteq \\mathbb{N} be the set consisting of the first n natural numbers: I_n = \\{1, 2, \\ldots, n\\}. We can now collect together all the 4 sets S_1, S_2, S_3, S_4 into a single set S , and index it using I_5 : S = \\{[0,1), [0,1/2), [0,1/3), [0,1/4)\\} = \\{S_i \\,|\\, i \\in I_4\\} = \\{S_i\\}_{i \\in I_4}. The indexing notation is particularly convenient when dealing with index sets which are subsets of real numbers. For instance, defining the sets T_a = \\{[0, 1/a) \\subseteq\\mathbb{R}\\,|\\, a \\in (0,1]\\}, we write the set T consisting of every such T_a as T = \\{T_a \\,|\\, a \\in (0,1]\\} = \\{T_a\\}_{a \\in (0,1]}. An important kind of family of sets is the power set \\mathcal{P}(S) of a given set S , consisting of all the subsets of S . Note that since \\emptyset \\subseteq S for every set S , \\emptyset \\in \\mathcal{P}(S) . Example Consider the set S = \\{1,2,3\\} . The power set of S is the set \\mathcal{P}(S) = \\{ \\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{2,3\\}, \\{1,3\\}, \\{1,2,3\\} \\}. Note that \\mathcal{P}(S) has 2^3 = 8 elements. In general, the power set of a finite set consisting of n elements has 2^n elements. Remark The power set of a given set is strictly larger than that set. Consider the set \\mathbb{N} of all natural numbers. Intuitively, we think of this as an infinite set. Consider the power set \\mathcal{P}(\\mathbb{N}) of \\mathbb{N} . Based on the comment just made, \\mathcal{P}(\\mathbb{N}) should have more elements in it than \\mathbb{N} , which by itself has infinite elements. Does it make sense to say that something is larger than infinity? We will unfortunately not have the pleasure of studying this very interesting question in these notes. Set operations Given an initial collection of sets, we can construct new sets from the existing ones using a variety of set operations . We will study a few of the more important one here. Union The union of two sets S and T , written as S \\cup T , is defined as the set of elements that belong to either S , or to T , or to both. Symbolically, S \\cup T = \\{a \\,|\\, a \\in S, \\text{ or } a \\in T\\}. Given a family of sets \\{ S_i \\}_{i \\in I} , the union of the family is defined similarly as \\cup_{i \\in I} S_i = \\{a \\,|\\, \\exists\\,j\\in I \\text{ such that }a \\in S_j\\}. Two special cases are worth noting here. When I = \\{1,2,\\ldots,N\\} , the union is written as \\cup_{i=1}^N S_i. When I = \\mathbb{N} , it is written as \\cup_{i=1}^\\infty S_i. Example Continuing the previous example involving the family \\{S_i \\,|\\, i\\in I_4\\} and \\{T_a \\,|\\, a \\in (0,1]\\} , we see that \\cup_{i \\in I_4} S_i = [0,1) \\cup [0,1/2) \\cup [0,1/3) \\cup [0,1/4) = [0,1). Similarly, it is easily checked that \\cup_{a \\in (0,1]} T_a = [0,1). Intersection The intersection of two sets S and T , written S \\cap T , is defined as the set of elements that belong to both S and T . In symbols, S \\cap T = \\{a \\,|\\, a \\in S, \\text{ and } a \\in T\\}. The extension of the notation to the case of families of sets is analogous to the case of unions described earlier. Two sets S and T are said to be disjoint if they have no element in common, S \\cap T = \\emptyset . A collection of sets is said to be pairwise disjoint if every pair of two sets in the collection is disjoint. Example With the sets \\{S_i \\,|\\, i \\in I_4\\} defined as earlier, note that \\cap_{i \\in I_4} S_i = [0,1) \\cap [0,1/2) \\cap [0,1/3) \\cap [0,1/4) = [0,1/4). Note also that S_i \\cap [1,2] = \\emptyset for every i \\in I_4 . We therefore say that S_i and [1,2] are disjoint for every i \\in I_4 . Set difference The set difference of two sets S and T , denoted as S \\setminus T , is defined as the set of all elements of S that do not belong to T : S \\setminus T = \\{a \\in S \\,|\\, a \\notin T\\}. If a universal set X is provided, the complement of a set S \\subset X , written as S^c , is defined as the set difference of X and S . In symbols, S^c = X \\setminus S . Example For any a \\in (0,1] , we defined the set T_a earlier as [0,1/a) . If I = [0,1] , then we see that I \\setminus T_a = [1/a,1]. Since T_a \\subseteq \\mathbb{R} , we can compute its complement in \\mathbb{R} as T_a^c = \\mathbb{R} \\setminus T_a = (-\\infty, 0) \\cup [1/a,\\infty). Cartesian products A set containing two elements \\{a,b\\} has no natural ordering among its elements. Thus, this set is exactly the same as the set \\{b,a\\} . We define an ordered pair (a,b) as a collection of two elements such that a is the first element , and b is the second element . Note that the ordered pair (a,b) is different from the ordered pair (b,a) . Remark The notion of an ordered pair can be defined entirely in terms of sets. Consider the set \\{a,\\{a,b\\}\\} . This prescribes a set \\{a,b\\} , and singles out a particular element, a in this case. If we define (a,b) = \\{a,\\{a,b\\}\\} , it is immediately obvious that the set (a,b) is not the same as the set (b,a) = \\{b,\\{a,b\\}\\} . While this shows how an order is fundamentally just a special set, it is almost never explicitly written out in practice. The Cartesian product of two sets S and T is defined as the set S \\times T that consists of all ordered pairs (s,t) such that s belongs to S and t belongs to T : S \\times T = \\{(s,t) \\,|\\, s \\in S, t \\in T\\}. Example Consider the sets S = \\{1,2,3\\} and T = \\{\\text{red}, \\text{blue}\\} . The Cartesian product of S and T in this case is computed as S \\times T = \\{ (1,\\text{red}), (1,\\text{blue}), (2,\\text{red}), (2,\\text{blue}, (3,\\text{red}), (3,\\text{blue} \\} Remark The word Cartesian derives from the the name of the French philosopher Rene Descartes. We will capitalize the first letter of the name of a term if that term has its origin in the name of an individual. The Cartesian product of a finite collection of sets S_1, S_2, \\ldots, S_n is defined similarly as S_1 \\times S_2 \\times \\ldots \\times S_n = \\{(s_1, s_2, \\ldots, s_n) \\,|\\, \\forall\\,i\\in\\{1,2,\\ldots,n\\},\\,s_i \\in S_i\\}. A similar extension holds for an infinite collection of sets. The Cartesian product of a set S with itself, S \\times S , is often written as S^2 . Similarly, the cartesian product of n copies of S is written as S^n . Example As an important example of the Cartesian product of a set with itself is the set \\mathbb{R}^n : \\mathbb{R}^n = \\underbrace{\\mathbb{R} \\times \\ldots \\times \\mathbb{R}}_{n \\text{ factors}}. In particular, the sets \\mathbb{R} , \\mathbb{R}^2 and \\mathbb{R}^3 will play a significant role in these notes. Binary relations A binary relation R on a set S is a subset of S \\times S . If (a,b) \\in R , it is conventional to represent this as a\\,R\\,b . Remark In the example presented earlier in the context of the Cartesian product, S = \\{1,2,3\\} and T = \\{\\text{red}, \\text{blue}\\} . One possible relation on S \\times T is the set R defined as R = \\{(1, \\text{red}), (1, \\text{blue}), (3, \\text{blue})\\}. Note that, in this case, 1 \\,R\\, \\text{blue} , while it is not true that 2 \\,R\\, \\text{blue} . A binary relation R \\subseteq S \\times S is said to be reflexive , if it is true that for each a \\in S , it is the case that a\\,R\\,a . symmetric , if for every a,b \\in S , a\\,R\\,b \\Rightarrow b\\,R\\,a . antisymmetric if for every a,b \\in S , (a\\,R\\,b\\text{ and }b\\,R\\,a)\\Rightarrow a = b . transitive if for every a,b,c \\in S , (a\\,R\\,b\\text{ and }b\\,R\\,c)\\Rightarrow a\\,R\\,c . Example Given any set S , let us consider the relation \\subseteq on the power set \\mathcal{P}(S) of S . Note that for any A, B, C \\in \\mathcal{P}(S) , A \\subseteq A shows us that the relation is reflexive; A \\subseteq B \\text{ and } B \\subseteq A \\Rightarrow A = B shows us that the relation is anti-symmetric; and, A \\subseteq B \\text{ and } B \\subseteq C \\Rightarrow A \\subseteq C shows us that the relation is transitive. Example Consider the set S = \\{1,2,3\\} . The relation R on S defined as R = \\{(1,1), (2,2), (1,2), (2,1), (2,3), (3,2), (1,3), (3,1)\\} is not reflexive since (3,3) \\notin R . It is, however, symmetric and transitive. An equivalence class on a set S is a binary relation on S that is reflexive, symmetric and transitive. An equivalence class is typically represented using symbols like \\sim , and (a,b)\\in\\sim is usually written as a \\sim b . Given an equivalence relation \\sim on S , we define the equivalence class of a \\in S as the set [a] consisting of all elements in S that are related to a through \\sim , [a] = \\{b \\in S \\,|\\, b \\sim a\\}. The notation [a]_\\sim is used if the particular equivalence relation used to define this class is to be emphasized. The collection of all equivalence classes [a] of S induced by the equivalence relation \\sim is called the factor space of S , and is denoted as S / \\sim . Thus, S / \\sim = \\{[a] \\,|\\, a \\in S\\}. Example Consider the following equivalence relation \\sim on the set \\mathbb{N} of all natural numbers: m \\sim n \\text{ iff } (m - n) \\text{ is divisible by } 5. Let us first verify that this is an equivalence relation. \\sim is reflexive since for any natural number n \\in \\mathbb{N} , n - n = 0 is divisible by 5 . For any m, n \\in \\mathbb{N} , if m \\sim n , then (m - n) is divisible by 5 . But this also means that (n - m) is divisible by 5 . In other words, n \\sim m . Thus, we see that \\sim is symmetric. Finally, for m,n,k \\in \\mathbb{N} , if m \\sim n and n \\sim k , then (m -n) = 5p for some p \\in \\mathbb{Z} , and (n - k) = 5q for some q \\in \\mathbb{Z} . Adding these two equations, we see that (m - k) = 5(p + q) , and hence that m \\sim k . We therefore see that \\sim is also a transitive relation. Together, these three results show that \\sim is an equivalence relation on \\mathbb{N} . The equivalence classes of \\mathbb{N} in this case are given by \\begin{split} [0] &= \\{0, 5, 10, 15, \\ldots \\},\\\\ [1] &= \\{1, 6, 11, 16, \\ldots \\},\\\\ [2] &= \\{2, 7, 12, 17, \\ldots \\},\\\\ [3] &= \\{3, 8, 13, 18, \\ldots \\},\\\\ [4] &= \\{4, 9, 14, 19, \\ldots \\}. \\end{split} There are only 5 distinct equivalence classes in this case. The factor space is thus seen to be \\mathbb{N}/\\sim = \\{[0], [1], [2], [3], [4]\\}. Equivalence classes offer a power means to partition a set. A partition of a set S is a pairwise disjoint family \\{T_i \\, |\\, i \\in I\\} of subsets of S such that the \\cup_{i \\in I} T_i = S . The sets \\{T_i\\}_{i \\in I} are then said to partition the set. The importance of equivalence relation lies in the fact that it partitions the set on which it is defined into disjoint equivalence classes. Proof A proof of the statement that an equivalence class \\sim on a set S partitions the set into disjoint equivalence classes is provided here. This can be skipped on a first reading. Consider the set P = \\{[a]\\,|\\,a \\in S\\} consisting of all the equivalence classes of S . It is easy to see that \\cup_{a \\in S} [a] = S . By construction, \\cup_{a \\in S} [a] \\subseteq S . Further, a \\in S \\Rightarrow a \\in [a] \\subseteq \\cup_{a \\in S} [a] , which gives the reverse inclusion, S \\subseteq \\cup_{a \\in S} [a] . Thus, \\cup_{a \\in S} [a] = S . We prove next that if a,b \\in S and a \\sim b , then [a] = [b] . Choose any c \\in [a] . This choice is always possible since a \\in [a] , and hence [a] \\neq \\emptyset . From the definition of an equivalence class, c \\in [a] \\Rightarrow c \\sim a . Using the transitivity property of the equivalence relation, (c \\sim a and a \\sim b) \\Rightarrow c \\sim b , and hence c \\in [b] . We have thus shown that [a] \\subseteq [b] . By a symmetric argument, it is evident that [b] \\subseteq [a] . This shows that if a \\sim b , then [a] = [b] . Finally, let us show that the equivalence classes that are not disjoint are identical. Consider any two equivalence classes [a] and [b] , where a, b \\in S . It is either the case that [a] \\cap [b] = \\emptyset , or [a] \\cap [b] \\neq \\emptyset . In the latter case, \\exists\\,c \\in S such that c\\in[a] and c\\in[b] . But this means that c \\sim a and c \\sim b , and hence, by symmetry and transitivity of the equivalence relation, a \\sim b . Using the argument given earlier, we see that [a] = [b] . We have thus shown that \\cup_{a \\in S} [a] = S is a pairwise disjoint cover of S , consisting of subsets of S . Example In the example considered earlier, notice how the set of equivalence classes \\{[0], [1], [2], [3], [4]\\} is pairwise disjoint, and further that \\cup_{i \\in I_5} [i-1] = \\mathbb{N}. The factor space S/\\sim = \\{[0], [1], [2], [3], [4]\\} thus provides a partition the set of natural numbers \\mathbb{N} . Maps and functions Given two sets S and T , a map , or a mapping , from S into T is a relation \\varphi \\subseteq S \\times T between S and T with the property that for every s \\in S , there exists a unique t \\in T such that (s,t) \\in \\varphi . In symbols, \\forall\\,s\\in S,\\,(s,t)\\in\\varphi\\text{ and }(s,t')\\in\\varphi \\,\\Rightarrow\\,t = t'. It is conventional to notate (s,t)\\in\\varphi as \\varphi(s) = t to make connection with the notation often employed in applications. We will also use the notation \\varphi:s \\mapsto t to denote the fact that \\varphi maps s \\in S to t \\in T . The set \\{(s,t)\\in S \\times T \\,|\\, (s,t) \\in \\varphi\\} is called the graph of \\varphi . We will reserve the word function to denote a map of the form \\varphi:S \\to \\mathbb{R} . Example Consider the sets S = \\{1,2,3\\} and T = \\{\\text{red},\\text{blue}\\} . The relation \\varphi \\subseteq S \\times T defined as \\varphi = \\{(1,\\text{red}),(1,\\text{blue}),(2,\\text{red}),(3,\\text{blue})\\} does not represent a map since 1 is related to both \\text{red} and \\text{blue} . One the other hand, the relation \\psi \\subseteq S \\times T defined as \\psi = \\{(1,\\text{red}),(2,\\text{red}),(3,\\text{blue})\\} represents a mapping between S and T : \\psi(1) = \\text{red} , \\psi(2) = \\text{red} , and \\psi(3) = \\text{blue} . The fact that the map \\varphi takes an element of S and returns an element of T is compactly written using the notation \\varphi:S \\to T . Here, the set S is called the domain of \\varphi , and T is called the codomain of \\varphi . The domain of \\varphi is sometimes abbreviated as \\text{dom }\\varphi . The set \\varphi(S) = \\{t \\in T \\,|\\, \\exists\\,s\\in S\\text{ such that }\\varphi(s) = t\\} is called the range of \\varphi . \\varphi(S) is also called the image of S under \\varphi , and is sometimes abbreviated as \\text{img }\\varphi . Remark It is important to always be aware of the domain and codomain of any map/function. For instance, the two functions f:\\mathbb{R}\\to\\mathbb{R} , defined as \\forall\\,x \\in \\mathbb{R}, \\quad f(x) = x^2, and the function g:\\mathbb{R}_+\\to\\mathbb{R} , defined as \\forall\\,x \\in \\mathbb{R}_+, \\quad g(x) = x^2, represent two different functions. The pre-image , or inverse image , of a subset V \\subseteq T of T under the mapping \\varphi:S \\to T is the subset of S consisting of all those elements that are mapped into V through \\varphi , \\varphi^{-1}(V) = \\{ s \\in S \\,|\\,\\varphi(s) \\in V\\}. The notation \\varphi^{-1} should not be confused with the inverse map , to be defined later. We also note that \\varphi^{-1}(V) = \\emptyset if there is no s \\in S such that \\varphi(s) \\in V . Thus, the inverse image is defined for all subsets of V . Example Consider the sets S = \\{1,2,3\\} , T = \\{\\text{red}, \\text{green}, \\text{blue}\\} and the map \\psi:S \\to T defined as \\psi = \\{(1,\\text{red}), (2,\\text{red}), (3,\\text{blue})\\}. In this case, \\psi^{-1}(\\text{red}) = \\{1,2\\} , \\psi^{-1}(\\text{blue}) = \\{3\\} and \\psi^{-1}(\\text{green}) = \\emptyset . Given sets S,T,V and maps \\varphi:S \\to T and \\psi:T\\to V , the composition map \\psi \\circ \\varphi:S \\to V is defined as \\psi\\circ\\varphi(s) = \\psi(\\varphi(s)) , for every s \\in S . The composition of maps is associative . What this means is that if \\varphi:S \\to T , \\psi: T \\to V , and \\xi:V \\to W are given maps, then \\xi\\circ(\\psi\\circ\\varphi) = (\\xi\\circ\\psi)\\circ\\varphi . It thus makes sense to write the composition map as \\xi\\circ\\psi\\circ\\phi:S \\to W . Example Let f:\\mathbb{R}\\to\\mathbb{R}_+ be the function defined as \\forall\\, x\\in \\mathbb{R}, \\quad f(x) = x^2. Let g:\\mathbb{R}_+\\to \\mathbb{R} be the function defined as \\forall\\, x\\in\\mathbb{R}, \\quad g(x) = \\log x. Then the composite function g \\circ f:\\mathbb{R}\\to\\mathbb{R} is given by \\forall\\,x\\in\\mathbb{R},\\quad (g\\circ f)(x) = g(f(x)) = \\log x^2. The map \\varphi:S \\to T is said to be a one-to-one map from S into T , or injective , or an injection , if for s,s' \\in S,\\,\\varphi(s) = \\varphi(s') \\Rightarrow s = s' . The map \\varphi:S\\to T is said to be a map from S onto T , or surjective , or a surjection , if \\varphi(S) = T . The map \\varphi:S\\to T that is both one-to-one and onto, or, equivalently, both injective and surjective, is said to be a one-to-one correspondence , or bijective , or a bijection from S onto T . A bijection thus allows us to identify elements of one set with that of another. Example We will now illustrate how the choice of the domain and codomain for the same rule can be critical in deciding the nature of the corresponding map: The function f:\\mathbb{R} \\to \\mathbb{R}_+ defined as \\forall\\, x \\in \\mathbb{R}, \\quad f(x) = x^2, is onto, but not one-to-one. The function g:\\mathbb{R}_+ \\to \\mathbb{R} defined as \\forall\\, x \\in \\mathbb{R}_+, \\quad g(x) = x^2, is one-to-one, but not onto. The function h:\\mathbb{R}_+ \\to\\mathbb{R}_+ defined as \\forall\\, x \\in \\mathbb{R}_+, \\quad h(x) = x^2, is both one-to-one and onto. If \\varphi:S \\to T is a bijection, the inverse map \\varphi^{-1}:T \\to S is defined as follows: \\forall\\,t\\in T,\\,\\varphi^{-1}(t) = s , where s \\in S is the unique element in S such that \\varphi(s) = t . Remark It is important to distinguish the inverse map from the inverse image. The latter is a mapping from subsets of T to subsets of S , and is defined even when the inverse map does not exist. Given any set S , we will use the notation \\text{id}_S:S \\to S to denote the identity map defined as \\text{id}_S(s) = s for every s \\in S . (Depending on the context, other notations will also be used for the identity map.) We thus see that if \\varphi:S \\to T is a bijection, then \\varphi^{-1} \\circ \\varphi = \\text{id}_S and \\varphi \\circ \\varphi^{-1} = \\text{id}_T .","title":"Appendix - Set Theory"},{"location":"set_theory/#describing-a-set","text":"Loosely speaking, a set is a collection of elements. These elements are said to belong to the set. Given a set S , the statement that an element a belongs to S is abbreviated as a \\in S. The negation of this statement, that a does not belong to S , is written as a \\notin S. The set consisting of n elements a_1, a_2, \\ldots, a_n is written as \\{a_1,a_2,\\ldots,a_n\\}. Example Let us collect together the list C of India\u2019s Twenty20 cricket captains so far (as of 2019): C = \\{\\text{Sehwag}, \\text{Dhoni}, \\text{Raina}, \\text{Rahane}, \\text{Kohli}, \\text{Sharma}\\}. (I have listed only the last names since the full names would make the list too long.) We denote the fact that \\text{Dhoni} was a captain of the Indian Twenty20 cricket team as \\text{Dhoni} \\in C. Similarly, we denote the fact that \\text{Tendulkar} was not a captain of the Indian Twenty20 cricket team using the notation \\text{Tendulkar} \\notin C. Notice how the set-theoretic notation \\text{Tendulkar} \\notin C succinctly represents the more verbose statement \u2018Tendulkar was not a captain of the Indian Twenty20 cricket team\u2019 . Remark Whenever you see a mathematical expression, keep in mind that it just summarizes a possibly long, or, complex idea - it is a good idea to read it out fully until you gain enough intuition to think abstractly. A more powerful notation to represent a set is as follows. Suppose we are given a property P(a) that any element a belonging to a set S may, or may not, satisfy. The set of all elements in S that satisfy the property P is written as \\{a \\in S \\,|\\, P(a)\\}. The above statement is read \u2018the set of all a in S for which P(a) is true\u2019 . Example Continuing the earlier example involving the set C of all Indian Twenty20 cricket team (as of 2019), notice that only \\text{Rahane} and \\text{Sharma} represented Mumbai, in the domestic matches. Using the notation just introduced, we see that the set \\{\\text{Rahane}, \\text{Sharma}\\} consisting of just those Indian Twenty20 captains until 2019 who played for Mumbai as \\{A \\in C \\,|\\, A\\text{ played for Mumbai}\\}. It is helpful to postulate the existence of a special set called the empty set that contains nothing. The standard notation for the empty set is \\emptyset . Example Returning to the set C of all Indian Twenty20 captains, it turns out that none of them were born in Mumbai. In set-theoretic notation, we write this as \\{A \\in C \\,|\\, A\\text{ was born in Mumbai}\\} = \\emptyset. Finally, we will always work within the confines of a universal set , which contains everything that we are dealing with in a particular context. Example In all the earlier examples involving the set C of all Indian Twenty20 captains, a convenient choice of universal set is the set P of all cricketers who represented India at the international level in cricket matches: P = \\{\\text{Indians who played for India in at least one international cricket match}\\}. Note that the set C can be obtained from P as follows: C = \\{A \\in P \\,|\\, A\\text{ captained India in a Twenty20 match}\\}. Remark Note that the choice of universal set is not unique. In the examples considered above, we might just as well have chosen the set I of all Indians who were born, say, after 1900, as our universal set. In practice, the universal set is often implicitly understood from the context; it is always a good idea to be aware of it.","title":"Describing a set"},{"location":"set_theory/#numerical-sets","text":"While set theory is very broad in its purview, we will limit our discussion henceforth to studying the properties of special kinds of sets. In particular, we will find the following numerical sets to be very useful: The set of all natural numbers : \\mathbb{N} = \\{1, 2, 3, \\ldots \\} . The set of all integers : \\mathbb{Z} = \\{\\ldots, -2, -1, 0, 1, 2, \\ldots\\} . The set of all rational numbers , which are numbers of the form p/q where p \\in \\mathbb{Z} and q \\in \\mathbb{N} , will be denoted by the symbol \\mathbb{Q} . Finally, the set of all real numbers will be denoted by the symbol \\mathbb{R} . We will sometimes use the symbol \\mathbb{R}_+ to denote the set of non-negative real numbers: \\mathbb{R}_+ = \\{a \\in \\mathbb{R} \\,|\\, a \\ge 0\\} . Remark In the modern mathematical formalism, all the numerical sets introduced above are constructed from the empty set \\emptyset . Understanding how this is done is beyond the scope of these notes. Any good book on real analysis should, however, provide the necessary details. All the discussions henceforth will be confined to numerical sets, or sets constructed from numerical sets, unless stated otherwise.","title":"Numerical sets"},{"location":"set_theory/#subsets-inclusion-and-equality","text":"A set S is said to be a subset of another set T , written S \\subseteq T , if, and only if, it is true that whenever a \\in S , it is also the case that a \\in T . Formally, S \\subseteq T \\,\\Leftrightarrow\\, a\\in S \\Rightarrow a \\in T. Remark We will use a variety of logical qualifiers in these notes. The symbol \\forall stands for the logical qualifier for all . We also use the logical qualifiers \\exists and \\exists! to represent the notions there exists , and there exists a unique , respectively. The symbol \\Rightarrow stands for implication . Thus, a \\Rightarrow b is read b if a , or a only if b . Finally, the symbol \\Leftrightarrow stands for equivalence . Thus, a \\Leftrightarrow b is read a if, and only if, b . We will henceforth abbreviate the phrase if, and only if as iff . Example We will use the following subsets of \\mathbb{R} to be very useful: for any a,b \\in \\mathbb{R} , \\begin{split} (a,b) &= \\{x \\in \\mathbb{R}\\,|\\, a < x < b\\},\\\\ (a,b] &= \\{x \\in \\mathbb{R}\\,|\\, a < x \\le b\\},\\\\ [a,b) &= \\{x \\in \\mathbb{R}\\,|\\, a \\le x < b\\},\\\\ [a,b] &= \\{x \\in \\mathbb{R}\\,|\\, a \\le x \\le b\\},\\\\ (a,\\infty) &= \\{x \\in \\mathbb{R}\\,|\\, x > a\\},\\\\ [a,\\infty) &= \\{x \\in \\mathbb{R}\\,|\\, x \\ge a\\},\\\\ (\\infty,a) &= \\{x \\in \\mathbb{R}\\,|\\, x < a\\},\\\\ (\\infty,a] &= \\{x \\in \\mathbb{R}\\,|\\, x \\le a\\}. \\end{split} The phrase S is contained in T is also used for S \\subseteq T . This can alternatively be stated as, T \\supseteq S , and we say that T is a superset of S , or that T contains S . S is said to be a strict subset of T , written S \\subsetneq T , if S is a subset of T , and there exists an element a \\in T such that a \\notin S . Formally, S \\subsetneq T \\,\\Leftrightarrow\\, S \\subseteq T, \\text{ and } \\exists\\, a \\in T \\text{ such that } a \\notin S. Example Note that the following inclusion relation holds among the numerical sets introduced earlier: \\emptyset \\subseteq \\mathbb{N} \\subseteq \\mathbb{Z} \\subseteq \\mathbb{Q} \\subseteq \\mathbb{R}. Finally, two sets S and T are said to be equal , written S = T , if both S \\subseteq T and T \\subseteq S . In other words, every element in S belongs to T , and vice versa. Formally, S = T \\,\\Leftrightarrow\\, S \\subseteq T \\text{ and } T \\subseteq S \\,\\Leftrightarrow\\, (a \\in S \\Leftrightarrow a \\in T). This is an important strategy to prove the equality of two sets. Example Let us consider the two sets S = [-1,1] \\subseteq \\mathbb{R}, and T = \\{ \\sin x \\,|\\, x \\in \\mathbb{R} \\} \\subseteq \\mathbb{R} For any y \\in S , note that it is possible to find a unique x \\in [-\\pi/2,\\pi/2] such that y = \\sin x \\in T . Thus, we see that S \\subseteq T . To prove the reverse inclusion, note that for any x \\in \\mathbb{R} , \\sin x \\in [-1,1] , and hence it is true that T \\subseteq S . We have thus shown that S = [-1,1] = \\{ \\sin x \\,|\\, x \\in \\mathbb{R} \\} = T.","title":"Subsets, inclusion and equality"},{"location":"set_theory/#families-of-sets","text":"We will now introduce a useful notation that will serve us well in the subsequent development of set theory. A family of sets indexed by the set I is a set S whose elements are sets S_i where i \\in I . In symbols, S = \\{S_i \\,|\\, i \\in I\\}. The set I is also called the index set for the family. It is also conventional to represent the family as \\{S_i\\}_{i\\in I}. Example Suppose that we have 4 sets S_1, S_2, S_3, S_4 defined as follows: \\begin{split} S_1 &= [0, 1) \\subseteq \\mathbb{R},\\\\ S_2 &= [0, 1/2) \\subseteq \\mathbb{R},\\\\ S_3 &= [0, 1/3) \\subseteq \\mathbb{R},\\\\ S_4 &= [0, 1/4) \\subseteq \\mathbb{R}. \\end{split} Let I_n \\subseteq \\mathbb{N} be the set consisting of the first n natural numbers: I_n = \\{1, 2, \\ldots, n\\}. We can now collect together all the 4 sets S_1, S_2, S_3, S_4 into a single set S , and index it using I_5 : S = \\{[0,1), [0,1/2), [0,1/3), [0,1/4)\\} = \\{S_i \\,|\\, i \\in I_4\\} = \\{S_i\\}_{i \\in I_4}. The indexing notation is particularly convenient when dealing with index sets which are subsets of real numbers. For instance, defining the sets T_a = \\{[0, 1/a) \\subseteq\\mathbb{R}\\,|\\, a \\in (0,1]\\}, we write the set T consisting of every such T_a as T = \\{T_a \\,|\\, a \\in (0,1]\\} = \\{T_a\\}_{a \\in (0,1]}. An important kind of family of sets is the power set \\mathcal{P}(S) of a given set S , consisting of all the subsets of S . Note that since \\emptyset \\subseteq S for every set S , \\emptyset \\in \\mathcal{P}(S) . Example Consider the set S = \\{1,2,3\\} . The power set of S is the set \\mathcal{P}(S) = \\{ \\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{2,3\\}, \\{1,3\\}, \\{1,2,3\\} \\}. Note that \\mathcal{P}(S) has 2^3 = 8 elements. In general, the power set of a finite set consisting of n elements has 2^n elements. Remark The power set of a given set is strictly larger than that set. Consider the set \\mathbb{N} of all natural numbers. Intuitively, we think of this as an infinite set. Consider the power set \\mathcal{P}(\\mathbb{N}) of \\mathbb{N} . Based on the comment just made, \\mathcal{P}(\\mathbb{N}) should have more elements in it than \\mathbb{N} , which by itself has infinite elements. Does it make sense to say that something is larger than infinity? We will unfortunately not have the pleasure of studying this very interesting question in these notes.","title":"Families of sets"},{"location":"set_theory/#set-operations","text":"Given an initial collection of sets, we can construct new sets from the existing ones using a variety of set operations . We will study a few of the more important one here.","title":"Set operations"},{"location":"set_theory/#union","text":"The union of two sets S and T , written as S \\cup T , is defined as the set of elements that belong to either S , or to T , or to both. Symbolically, S \\cup T = \\{a \\,|\\, a \\in S, \\text{ or } a \\in T\\}. Given a family of sets \\{ S_i \\}_{i \\in I} , the union of the family is defined similarly as \\cup_{i \\in I} S_i = \\{a \\,|\\, \\exists\\,j\\in I \\text{ such that }a \\in S_j\\}. Two special cases are worth noting here. When I = \\{1,2,\\ldots,N\\} , the union is written as \\cup_{i=1}^N S_i. When I = \\mathbb{N} , it is written as \\cup_{i=1}^\\infty S_i. Example Continuing the previous example involving the family \\{S_i \\,|\\, i\\in I_4\\} and \\{T_a \\,|\\, a \\in (0,1]\\} , we see that \\cup_{i \\in I_4} S_i = [0,1) \\cup [0,1/2) \\cup [0,1/3) \\cup [0,1/4) = [0,1). Similarly, it is easily checked that \\cup_{a \\in (0,1]} T_a = [0,1).","title":"Union"},{"location":"set_theory/#intersection","text":"The intersection of two sets S and T , written S \\cap T , is defined as the set of elements that belong to both S and T . In symbols, S \\cap T = \\{a \\,|\\, a \\in S, \\text{ and } a \\in T\\}. The extension of the notation to the case of families of sets is analogous to the case of unions described earlier. Two sets S and T are said to be disjoint if they have no element in common, S \\cap T = \\emptyset . A collection of sets is said to be pairwise disjoint if every pair of two sets in the collection is disjoint. Example With the sets \\{S_i \\,|\\, i \\in I_4\\} defined as earlier, note that \\cap_{i \\in I_4} S_i = [0,1) \\cap [0,1/2) \\cap [0,1/3) \\cap [0,1/4) = [0,1/4). Note also that S_i \\cap [1,2] = \\emptyset for every i \\in I_4 . We therefore say that S_i and [1,2] are disjoint for every i \\in I_4 .","title":"Intersection"},{"location":"set_theory/#set-difference","text":"The set difference of two sets S and T , denoted as S \\setminus T , is defined as the set of all elements of S that do not belong to T : S \\setminus T = \\{a \\in S \\,|\\, a \\notin T\\}. If a universal set X is provided, the complement of a set S \\subset X , written as S^c , is defined as the set difference of X and S . In symbols, S^c = X \\setminus S . Example For any a \\in (0,1] , we defined the set T_a earlier as [0,1/a) . If I = [0,1] , then we see that I \\setminus T_a = [1/a,1]. Since T_a \\subseteq \\mathbb{R} , we can compute its complement in \\mathbb{R} as T_a^c = \\mathbb{R} \\setminus T_a = (-\\infty, 0) \\cup [1/a,\\infty).","title":"Set difference"},{"location":"set_theory/#cartesian-products","text":"A set containing two elements \\{a,b\\} has no natural ordering among its elements. Thus, this set is exactly the same as the set \\{b,a\\} . We define an ordered pair (a,b) as a collection of two elements such that a is the first element , and b is the second element . Note that the ordered pair (a,b) is different from the ordered pair (b,a) . Remark The notion of an ordered pair can be defined entirely in terms of sets. Consider the set \\{a,\\{a,b\\}\\} . This prescribes a set \\{a,b\\} , and singles out a particular element, a in this case. If we define (a,b) = \\{a,\\{a,b\\}\\} , it is immediately obvious that the set (a,b) is not the same as the set (b,a) = \\{b,\\{a,b\\}\\} . While this shows how an order is fundamentally just a special set, it is almost never explicitly written out in practice. The Cartesian product of two sets S and T is defined as the set S \\times T that consists of all ordered pairs (s,t) such that s belongs to S and t belongs to T : S \\times T = \\{(s,t) \\,|\\, s \\in S, t \\in T\\}. Example Consider the sets S = \\{1,2,3\\} and T = \\{\\text{red}, \\text{blue}\\} . The Cartesian product of S and T in this case is computed as S \\times T = \\{ (1,\\text{red}), (1,\\text{blue}), (2,\\text{red}), (2,\\text{blue}, (3,\\text{red}), (3,\\text{blue} \\} Remark The word Cartesian derives from the the name of the French philosopher Rene Descartes. We will capitalize the first letter of the name of a term if that term has its origin in the name of an individual. The Cartesian product of a finite collection of sets S_1, S_2, \\ldots, S_n is defined similarly as S_1 \\times S_2 \\times \\ldots \\times S_n = \\{(s_1, s_2, \\ldots, s_n) \\,|\\, \\forall\\,i\\in\\{1,2,\\ldots,n\\},\\,s_i \\in S_i\\}. A similar extension holds for an infinite collection of sets. The Cartesian product of a set S with itself, S \\times S , is often written as S^2 . Similarly, the cartesian product of n copies of S is written as S^n . Example As an important example of the Cartesian product of a set with itself is the set \\mathbb{R}^n : \\mathbb{R}^n = \\underbrace{\\mathbb{R} \\times \\ldots \\times \\mathbb{R}}_{n \\text{ factors}}. In particular, the sets \\mathbb{R} , \\mathbb{R}^2 and \\mathbb{R}^3 will play a significant role in these notes.","title":"Cartesian products"},{"location":"set_theory/#binary-relations","text":"A binary relation R on a set S is a subset of S \\times S . If (a,b) \\in R , it is conventional to represent this as a\\,R\\,b . Remark In the example presented earlier in the context of the Cartesian product, S = \\{1,2,3\\} and T = \\{\\text{red}, \\text{blue}\\} . One possible relation on S \\times T is the set R defined as R = \\{(1, \\text{red}), (1, \\text{blue}), (3, \\text{blue})\\}. Note that, in this case, 1 \\,R\\, \\text{blue} , while it is not true that 2 \\,R\\, \\text{blue} . A binary relation R \\subseteq S \\times S is said to be reflexive , if it is true that for each a \\in S , it is the case that a\\,R\\,a . symmetric , if for every a,b \\in S , a\\,R\\,b \\Rightarrow b\\,R\\,a . antisymmetric if for every a,b \\in S , (a\\,R\\,b\\text{ and }b\\,R\\,a)\\Rightarrow a = b . transitive if for every a,b,c \\in S , (a\\,R\\,b\\text{ and }b\\,R\\,c)\\Rightarrow a\\,R\\,c . Example Given any set S , let us consider the relation \\subseteq on the power set \\mathcal{P}(S) of S . Note that for any A, B, C \\in \\mathcal{P}(S) , A \\subseteq A shows us that the relation is reflexive; A \\subseteq B \\text{ and } B \\subseteq A \\Rightarrow A = B shows us that the relation is anti-symmetric; and, A \\subseteq B \\text{ and } B \\subseteq C \\Rightarrow A \\subseteq C shows us that the relation is transitive. Example Consider the set S = \\{1,2,3\\} . The relation R on S defined as R = \\{(1,1), (2,2), (1,2), (2,1), (2,3), (3,2), (1,3), (3,1)\\} is not reflexive since (3,3) \\notin R . It is, however, symmetric and transitive. An equivalence class on a set S is a binary relation on S that is reflexive, symmetric and transitive. An equivalence class is typically represented using symbols like \\sim , and (a,b)\\in\\sim is usually written as a \\sim b . Given an equivalence relation \\sim on S , we define the equivalence class of a \\in S as the set [a] consisting of all elements in S that are related to a through \\sim , [a] = \\{b \\in S \\,|\\, b \\sim a\\}. The notation [a]_\\sim is used if the particular equivalence relation used to define this class is to be emphasized. The collection of all equivalence classes [a] of S induced by the equivalence relation \\sim is called the factor space of S , and is denoted as S / \\sim . Thus, S / \\sim = \\{[a] \\,|\\, a \\in S\\}. Example Consider the following equivalence relation \\sim on the set \\mathbb{N} of all natural numbers: m \\sim n \\text{ iff } (m - n) \\text{ is divisible by } 5. Let us first verify that this is an equivalence relation. \\sim is reflexive since for any natural number n \\in \\mathbb{N} , n - n = 0 is divisible by 5 . For any m, n \\in \\mathbb{N} , if m \\sim n , then (m - n) is divisible by 5 . But this also means that (n - m) is divisible by 5 . In other words, n \\sim m . Thus, we see that \\sim is symmetric. Finally, for m,n,k \\in \\mathbb{N} , if m \\sim n and n \\sim k , then (m -n) = 5p for some p \\in \\mathbb{Z} , and (n - k) = 5q for some q \\in \\mathbb{Z} . Adding these two equations, we see that (m - k) = 5(p + q) , and hence that m \\sim k . We therefore see that \\sim is also a transitive relation. Together, these three results show that \\sim is an equivalence relation on \\mathbb{N} . The equivalence classes of \\mathbb{N} in this case are given by \\begin{split} [0] &= \\{0, 5, 10, 15, \\ldots \\},\\\\ [1] &= \\{1, 6, 11, 16, \\ldots \\},\\\\ [2] &= \\{2, 7, 12, 17, \\ldots \\},\\\\ [3] &= \\{3, 8, 13, 18, \\ldots \\},\\\\ [4] &= \\{4, 9, 14, 19, \\ldots \\}. \\end{split} There are only 5 distinct equivalence classes in this case. The factor space is thus seen to be \\mathbb{N}/\\sim = \\{[0], [1], [2], [3], [4]\\}. Equivalence classes offer a power means to partition a set. A partition of a set S is a pairwise disjoint family \\{T_i \\, |\\, i \\in I\\} of subsets of S such that the \\cup_{i \\in I} T_i = S . The sets \\{T_i\\}_{i \\in I} are then said to partition the set. The importance of equivalence relation lies in the fact that it partitions the set on which it is defined into disjoint equivalence classes. Proof A proof of the statement that an equivalence class \\sim on a set S partitions the set into disjoint equivalence classes is provided here. This can be skipped on a first reading. Consider the set P = \\{[a]\\,|\\,a \\in S\\} consisting of all the equivalence classes of S . It is easy to see that \\cup_{a \\in S} [a] = S . By construction, \\cup_{a \\in S} [a] \\subseteq S . Further, a \\in S \\Rightarrow a \\in [a] \\subseteq \\cup_{a \\in S} [a] , which gives the reverse inclusion, S \\subseteq \\cup_{a \\in S} [a] . Thus, \\cup_{a \\in S} [a] = S . We prove next that if a,b \\in S and a \\sim b , then [a] = [b] . Choose any c \\in [a] . This choice is always possible since a \\in [a] , and hence [a] \\neq \\emptyset . From the definition of an equivalence class, c \\in [a] \\Rightarrow c \\sim a . Using the transitivity property of the equivalence relation, (c \\sim a and a \\sim b) \\Rightarrow c \\sim b , and hence c \\in [b] . We have thus shown that [a] \\subseteq [b] . By a symmetric argument, it is evident that [b] \\subseteq [a] . This shows that if a \\sim b , then [a] = [b] . Finally, let us show that the equivalence classes that are not disjoint are identical. Consider any two equivalence classes [a] and [b] , where a, b \\in S . It is either the case that [a] \\cap [b] = \\emptyset , or [a] \\cap [b] \\neq \\emptyset . In the latter case, \\exists\\,c \\in S such that c\\in[a] and c\\in[b] . But this means that c \\sim a and c \\sim b , and hence, by symmetry and transitivity of the equivalence relation, a \\sim b . Using the argument given earlier, we see that [a] = [b] . We have thus shown that \\cup_{a \\in S} [a] = S is a pairwise disjoint cover of S , consisting of subsets of S . Example In the example considered earlier, notice how the set of equivalence classes \\{[0], [1], [2], [3], [4]\\} is pairwise disjoint, and further that \\cup_{i \\in I_5} [i-1] = \\mathbb{N}. The factor space S/\\sim = \\{[0], [1], [2], [3], [4]\\} thus provides a partition the set of natural numbers \\mathbb{N} .","title":"Binary relations"},{"location":"set_theory/#maps-and-functions","text":"Given two sets S and T , a map , or a mapping , from S into T is a relation \\varphi \\subseteq S \\times T between S and T with the property that for every s \\in S , there exists a unique t \\in T such that (s,t) \\in \\varphi . In symbols, \\forall\\,s\\in S,\\,(s,t)\\in\\varphi\\text{ and }(s,t')\\in\\varphi \\,\\Rightarrow\\,t = t'. It is conventional to notate (s,t)\\in\\varphi as \\varphi(s) = t to make connection with the notation often employed in applications. We will also use the notation \\varphi:s \\mapsto t to denote the fact that \\varphi maps s \\in S to t \\in T . The set \\{(s,t)\\in S \\times T \\,|\\, (s,t) \\in \\varphi\\} is called the graph of \\varphi . We will reserve the word function to denote a map of the form \\varphi:S \\to \\mathbb{R} . Example Consider the sets S = \\{1,2,3\\} and T = \\{\\text{red},\\text{blue}\\} . The relation \\varphi \\subseteq S \\times T defined as \\varphi = \\{(1,\\text{red}),(1,\\text{blue}),(2,\\text{red}),(3,\\text{blue})\\} does not represent a map since 1 is related to both \\text{red} and \\text{blue} . One the other hand, the relation \\psi \\subseteq S \\times T defined as \\psi = \\{(1,\\text{red}),(2,\\text{red}),(3,\\text{blue})\\} represents a mapping between S and T : \\psi(1) = \\text{red} , \\psi(2) = \\text{red} , and \\psi(3) = \\text{blue} . The fact that the map \\varphi takes an element of S and returns an element of T is compactly written using the notation \\varphi:S \\to T . Here, the set S is called the domain of \\varphi , and T is called the codomain of \\varphi . The domain of \\varphi is sometimes abbreviated as \\text{dom }\\varphi . The set \\varphi(S) = \\{t \\in T \\,|\\, \\exists\\,s\\in S\\text{ such that }\\varphi(s) = t\\} is called the range of \\varphi . \\varphi(S) is also called the image of S under \\varphi , and is sometimes abbreviated as \\text{img }\\varphi . Remark It is important to always be aware of the domain and codomain of any map/function. For instance, the two functions f:\\mathbb{R}\\to\\mathbb{R} , defined as \\forall\\,x \\in \\mathbb{R}, \\quad f(x) = x^2, and the function g:\\mathbb{R}_+\\to\\mathbb{R} , defined as \\forall\\,x \\in \\mathbb{R}_+, \\quad g(x) = x^2, represent two different functions. The pre-image , or inverse image , of a subset V \\subseteq T of T under the mapping \\varphi:S \\to T is the subset of S consisting of all those elements that are mapped into V through \\varphi , \\varphi^{-1}(V) = \\{ s \\in S \\,|\\,\\varphi(s) \\in V\\}. The notation \\varphi^{-1} should not be confused with the inverse map , to be defined later. We also note that \\varphi^{-1}(V) = \\emptyset if there is no s \\in S such that \\varphi(s) \\in V . Thus, the inverse image is defined for all subsets of V . Example Consider the sets S = \\{1,2,3\\} , T = \\{\\text{red}, \\text{green}, \\text{blue}\\} and the map \\psi:S \\to T defined as \\psi = \\{(1,\\text{red}), (2,\\text{red}), (3,\\text{blue})\\}. In this case, \\psi^{-1}(\\text{red}) = \\{1,2\\} , \\psi^{-1}(\\text{blue}) = \\{3\\} and \\psi^{-1}(\\text{green}) = \\emptyset . Given sets S,T,V and maps \\varphi:S \\to T and \\psi:T\\to V , the composition map \\psi \\circ \\varphi:S \\to V is defined as \\psi\\circ\\varphi(s) = \\psi(\\varphi(s)) , for every s \\in S . The composition of maps is associative . What this means is that if \\varphi:S \\to T , \\psi: T \\to V , and \\xi:V \\to W are given maps, then \\xi\\circ(\\psi\\circ\\varphi) = (\\xi\\circ\\psi)\\circ\\varphi . It thus makes sense to write the composition map as \\xi\\circ\\psi\\circ\\phi:S \\to W . Example Let f:\\mathbb{R}\\to\\mathbb{R}_+ be the function defined as \\forall\\, x\\in \\mathbb{R}, \\quad f(x) = x^2. Let g:\\mathbb{R}_+\\to \\mathbb{R} be the function defined as \\forall\\, x\\in\\mathbb{R}, \\quad g(x) = \\log x. Then the composite function g \\circ f:\\mathbb{R}\\to\\mathbb{R} is given by \\forall\\,x\\in\\mathbb{R},\\quad (g\\circ f)(x) = g(f(x)) = \\log x^2. The map \\varphi:S \\to T is said to be a one-to-one map from S into T , or injective , or an injection , if for s,s' \\in S,\\,\\varphi(s) = \\varphi(s') \\Rightarrow s = s' . The map \\varphi:S\\to T is said to be a map from S onto T , or surjective , or a surjection , if \\varphi(S) = T . The map \\varphi:S\\to T that is both one-to-one and onto, or, equivalently, both injective and surjective, is said to be a one-to-one correspondence , or bijective , or a bijection from S onto T . A bijection thus allows us to identify elements of one set with that of another. Example We will now illustrate how the choice of the domain and codomain for the same rule can be critical in deciding the nature of the corresponding map: The function f:\\mathbb{R} \\to \\mathbb{R}_+ defined as \\forall\\, x \\in \\mathbb{R}, \\quad f(x) = x^2, is onto, but not one-to-one. The function g:\\mathbb{R}_+ \\to \\mathbb{R} defined as \\forall\\, x \\in \\mathbb{R}_+, \\quad g(x) = x^2, is one-to-one, but not onto. The function h:\\mathbb{R}_+ \\to\\mathbb{R}_+ defined as \\forall\\, x \\in \\mathbb{R}_+, \\quad h(x) = x^2, is both one-to-one and onto. If \\varphi:S \\to T is a bijection, the inverse map \\varphi^{-1}:T \\to S is defined as follows: \\forall\\,t\\in T,\\,\\varphi^{-1}(t) = s , where s \\in S is the unique element in S such that \\varphi(s) = t . Remark It is important to distinguish the inverse map from the inverse image. The latter is a mapping from subsets of T to subsets of S , and is defined even when the inverse map does not exist. Given any set S , we will use the notation \\text{id}_S:S \\to S to denote the identity map defined as \\text{id}_S(s) = s for every s \\in S . (Depending on the context, other notations will also be used for the identity map.) We thus see that if \\varphi:S \\to T is a bijection, then \\varphi^{-1} \\circ \\varphi = \\text{id}_S and \\varphi \\circ \\varphi^{-1} = \\text{id}_T .","title":"Maps and functions"},{"location":"tensor_algebra/","text":"This section introduces certain key topics related to the algebra of tensors . The choice of topics is not exhaustive, but provides a good starting point for the study of Euclidean tensor analysis. Multilinear functions and tensors Let us extend the definition of bilinear functions introduced in the preceding section to multilinear functions. Given inner product spaces V_1, \\ldots, V_n , consider a map \\mathsf{T}:V_1 \\times \\ldots \\times V_n \\to \\mathbb{R} such that for any \\mathsf{u}_i, \\mathsf{v}_i \\in V_i , 1 \\le i \\le n , and a \\in \\mathbb{R} , \\mathsf{T}(\\mathsf{v}_1, \\ldots, \\mathsf{u}_i + a\\mathsf{v}_i, \\ldots, \\mathsf{v}_n) = \\mathsf{T}(\\mathsf{v}_1, \\ldots, \\mathsf{u}_i, \\ldots, \\mathsf{v}_n) + a\\,\\mathsf{T}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_i, \\ldots, \\mathsf{v}_n). Such functions, which are linear separately in each of their arguments, are said to be multilinear . We will denote the set of all multilinear functions of this form as \\mathcal{T}(V_1 \\times \\ldots \\times V_n,\\mathbb{R}) . Note that multilinearity and linearity are distinct concepts, as the following example illustrates. Example Consider the functions \\mathsf{S}, \\mathsf{T}:\\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R} defined as follows: for any x, y \\in \\mathbb{R} , \\mathsf{S}(x, y) = x + y, \\qquad \\mathsf{T}(x, y) = xy. It follows that given a, u, v \\in \\mathbb{R} , \\begin{split} \\mathsf{S}((x,y) + c(u,v)) &= (x + y) + c(u + v) = \\mathsf{S}(x,y) + a\\mathsf{S}(u,v),\\\\ \\mathsf{T}((x,y) + a(u,v)) &= (x + au)(y + av) = xy + a^2uv + axv + ayu \\neq \\mathsf{T}(x,y) + a\\mathsf{T}(u,v). \\end{split} We thus see that \\mathsf{S} is linear, but \\mathsf{T} is not. On the other hand, note that \\begin{split} \\mathsf{S}(x + au, y) &= x + y + au \\neq \\mathsf{S}(x,y) + a\\mathsf{S}(u,y),\\\\ \\mathsf{T}(x + au, y) &= (x + au)y = xy + auy = \\mathsf{T}(x,y) + a\\mathsf{T}(u,y). \\end{split} We thus see that \\mathsf{S} is not multilinear, even though it is linear, and \\mathsf{T} is multilinear and not linear. Let us now focus attention on a single inner product space V of dimension n and multilinear functions defined on finite Cartesian products of V . A tensor of order k on V is defined as multilinear function of the form \\mathsf{A}:\\underbrace{V \\times \\ldots \\times V}_{k \\text{ terms}} \\to \\mathbb{R}, The set of all multilinear maps of the form \\mathsf{A}:\\times^k V \\to \\mathbb{R} is denoted by \\mathcal{T}^k(V) . Thus \\mathcal{T}^k(V) denotes the set of all tensors of order k on V . Defining maps +:\\mathcal{T}^k(V) \\times \\mathcal{T}^k(V) \\to \\mathcal{T}^k(V) and \\cdot:\\mathbb{R} \\times \\mathcal{T}^k(V) \\to \\mathcal{T}^k(V) as \\begin{split} (\\mathsf{A} + \\mathsf{B})(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k) &= \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k) + \\mathsf{B}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k),\\\\ (a\\mathsf{A})(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k) &= a \\, \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k), \\end{split} for any \\mathsf{A},\\mathsf{B} \\in \\mathcal{T}^k(V) , a \\in \\mathbb{R} and \\mathsf{u}_1, \\ldots, \\mathsf{u}_k \\in V , it is easy to verify that \\mathcal{T}^k(V) is a real linear space. The set \\mathcal{T}^1(V) = \\{\\mathsf{T}:V \\to \\mathbb{R} \\,|\\, \\mathsf{T} \\text{ is linear}\\} , defined as the set of all linear functions on V , is called the (algebraic) dual space of V , and is often written as V^* . It turns out that if V is a finite dimensional inner product space, then V^* is canonically isomorphic to V . This means that V and V^* are identical for all practical purposes. For this reason, a vector is often called a tensor of order 1 . Further, in this case, the action of any \\mathsf{v} \\in \\mathcal{T}^1(V) on any \\mathsf{u} \\in V is defined as follows: \\mathsf{v}(\\mathsf{u}) = \\mathsf{v} \\cdot \\mathsf{u}. This is a simple instance of what is known as the Riesz representation theorem . It is conventional to define \\mathcal{T}^0(V) = \\mathbb{R} . Thus, tensors of order 0 are scalars, tensors of order 1 are vectors, and tensors of order 2 can be identified with linear maps. The present definition thus unifies the various kinds of linear spaces on V studied earlier. The set of all \\mathcal{T}^k(V) is said to constitute a tensor algebra on V . Example As a simple but important example of tensors, consider the second order tensor \\hat{\\mathsf{I}} \\in \\mathcal{T}^2(V) on V defined as follows: for any \\mathsf{u},\\mathsf{v} \\in V , \\hat{\\mathsf{I}}(\\mathsf{u},\\mathsf{v}) = \\mathsf{u} \\cdot \\mathsf{v}. The multilinearity of \\hat{\\mathsf{I}} , in this case just its bilinearity, follows from the bilinearity of the inner product. More generally, given any linear map \\mathsf{T} \\in L(V,V) , we can define a second order tensor \\hat{\\mathsf{T}} \\in \\mathcal{T}^2(V) as follows: for any \\mathsf{u},\\mathsf{v} \\in V , \\hat{\\mathsf{T}}(\\mathsf{u},\\mathsf{v}) = \\mathsf{u} \\cdot \\mathsf{T}\\mathsf{v}. It is left as a simple exercise to verify that the map \\hat{\\mathsf{T}} is a second order tensor on V . Remark Defining the identity map on V as the map \\mathsf{I} \\in L(V,V) such that \\mathsf{I}\\mathsf{v} = \\mathsf{v} for any \\mathsf{v} \\in V , note that the corresponding second order tensor is precisely the tensor \\hat{\\mathsf{I}} \\in \\mathcal{T}^2(V) introduced earlier. Tensor products Given two tensors \\mathsf{A} \\in \\mathcal{T}^k(V) and \\mathsf{B} \\in \\mathcal{T}^l(V) , it is possible to combine them to obtain a tensor of higher order. Specifically, the tensor product of \\mathsf{A} and \\mathsf{B} is defined as the tensor \\mathsf{A}\\otimes\\mathsf{B} \\in \\mathcal{T}^{k + l}(V) such that for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_k, \\mathsf{u}_{k+1}, \\ldots, \\mathsf{u}_{k + l} \\in V , \\mathsf{A} \\otimes \\mathsf{B}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k, \\mathsf{u}_{k+1}, \\ldots, \\mathsf{u}_{k + l}) = \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k)\\mathsf{B}(\\mathsf{u}_{k+1}, \\ldots, \\mathsf{u}_{k + l}). As a special case given vectors \\mathsf{v}, \\mathsf{w} \\in V , their tensor product yields a second order tensor \\mathsf{v} \\otimes \\mathsf{w} \\in \\mathcal{T}^2(V) : for any \\mathsf{u}_1, \\mathsf{u}_2 \\in V , \\mathsf{v} \\otimes \\mathsf{w} (\\mathsf{u}_1, \\mathsf{u}_2) = (\\mathsf{v} \\cdot \\mathsf{u}_1)(\\mathsf{w} \\cdot \\mathsf{u}_2). The foregoing definition can be extended to define the tensor product of a finite number of tensors. Suppose that \\mathsf{u}_1, \\ldots, \\mathsf{u}_k are k vectors in V . The tensor product of these vectors is defined as the multilinear map \\mathsf{u}_1 \\otimes \\ldots \\otimes \\mathsf{u}_k: V^k \\to \\mathbb{R}, such that for any set of k vectors \\mathsf{v}_1, \\ldots, \\mathsf{v}_k in V , \\mathsf{u}_1 \\otimes \\ldots \\otimes \\mathsf{u}_k(\\mathsf{v}_1, \\ldots, \\mathsf{v}_k) = (\\mathsf{u}_1\\cdot \\mathsf{v}_1) \\ldots (\\mathsf{u}_k\\cdot \\mathsf{v}_k). Thus, the tensor product allows us to construct higher order tensors from lower order tensors. A higher order tensor that is constructed from vectors, like the one shown above, is called a rank- 1 tensor. Important Note! There is an ambiguity in the tensor product notation that warrants clarification. Given vectors \\mathsf{v}, \\mathsf{w} \\in V , the quantity \\mathsf{v}\\otimes\\mathsf{w} can stand for either a linear map, i.e. an element of L(V,V) , or a tensor of order 2 , i.e. an element of \\mathcal{T}^2(V) . The vector spaces L(V,V) and \\mathcal{T}^2(V) are isomorphic - this means that they can be naturally identified with each other. Indeed, given any \\mathsf{T} \\in L(V,V) , the bilinear function \\hat{\\mathsf{T}} \\in \\mathcal{T}^2(V) defined as \\hat{\\mathsf{T}}(\\mathsf{u}_1,\\mathsf{u}_2) = \\mathsf{u}_1\\cdot\\mathsf{T}\\mathsf{u}_2, for any \\mathsf{u}_1,\\mathsf{u}_2 \\in V , can be used to uniquely associate an element of L(V,V) with \\mathcal{T}^2(V) , and vice versa. This isomorphism is often abused to represent both \\mathsf{T} \\in L(V,V) and \\hat{\\mathsf{T}} \\in \\mathcal{T}^2(V) using the same symbol. In practice, such an ambiguity is averted by specifying the domain and codomain of every function that is used. A similar ambiguity arises in the case of tensors of higher order - the appropriate meaning is to be inferred from the context. Basis representation Recall that the set \\mathcal{T}^k(V) of all tensors of order k on V is a linear space. This means that we can legitimately ask how we can construct a basis for it, and represent any k^{\\text{th}} order tensor in terms of this basis. This question will be explored in this section. Let us first consider the special case when an orthonormal basis (\\mathsf{e}_i) of V is provided. Then, the representation of a k^{\\text{th}} order tensor \\mathsf{A} \\in \\mathcal{T}^k(V) is easily computed by considering the action of \\mathsf{A} on k vectors \\mathsf{v}_i = \\sum v_{ij} \\mathsf{e}_j in V , where 1 \\le i \\le k : \\begin{split} \\mathsf{A}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_k) &= \\mathsf{A}\\left(\\sum v_{1i_1} \\mathsf{e}_{i_1}, \\ldots, \\sum v_{ki_k} \\mathsf{e}_{i_k}\\right)\\\\ &= \\sum \\mathsf{A}_{i_1\\ldots i_k} v_{1i_1} \\ldots v_{ki_k}, \\end{split} where \\mathsf{A}_{i_1\\ldots i_k} = \\mathsf{A}(\\mathsf{e}_{i_1}, \\ldots, \\mathsf{e}_{i_k}). It follows from the orthonormality of (\\mathsf{e}_i) that \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_k) = v_{1i_1} \\ldots v_{ki_k}. Putting these together, we get \\mathsf{A}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_k) = \\sum A_{i_1\\ldots i_k} \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_k). Since this is true for any choice of vectors \\mathsf{v}_1, \\ldots, \\mathsf{v}_k \\in V , we get the representation of \\mathsf{A} with respect to the orthonormal basis (\\mathsf{e}_i) of V as \\mathsf{A} = \\sum A_{i_1\\ldots i_k} \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}. The constants (A_{i_1\\ldots i_k}) are called the components of \\mathsf{A} with respect to the basis (\\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}) of V . The representation of \\mathsf{A} just derived also informs us that the set of n^k multilinear functions \\{\\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k} \\} spans \\mathcal{T}^k(V) . By studying the actin of \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k} on a set of k basis vectors, it is easy to show that these n^k multilinear functions are also linearly independent. This shows us that the set of n^k multilinear maps \\{\\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k} \\} constitute a basis of \\mathcal{T}(V^k,\\mathbb{R}) , and that \\text{dim}(\\mathcal{T}(V^k,\\mathbb{R})) = (\\text{dim}(V))^k . Example Let us consider the second order tensor \\hat{I}\\in\\mathcal{T}^2(V) defined earlier. Suppose that (\\mathsf{e}_i) is an orthonormal basis of V . The representation of \\hat{\\mathsf{I}} can be computed easily as follows: \\hat{\\mathsf{I}} = \\sum \\hat{I}_{ij} \\mathsf{e}_i \\otimes \\mathsf{e}_j, where \\hat{I}_{ij} = \\hat{\\mathsf{I}}(\\mathsf{e}_i,\\mathsf{e}_j) = \\mathsf{e}_i\\cdot\\mathsf{e}_j = \\delta_{ij}. We thus see that the second order tensor \\hat{\\mathsf{I}} \\in \\mathcal{T}^2(V) has the following representation: \\hat{\\mathsf{I}} = \\sum \\delta_{ij} \\mathsf{e}_i \\otimes \\mathsf{e}_j. Notice how the Kr\u00f6necker delta symbols defined earlier naturally figure as the components of the tensor \\hat{\\mathsf{I}} . It is convenient to represent the components of second order tensor as a matrix. For instance, the components of \\hat{\\mathsf{I}} just computed can be arranged as follows: = \\begin{bmatrix} 1 & 0 & \\ldots & 0\\\\ 0 & 1 & \\ldots & 0\\\\ \\vdots & & \\ddots & \\vdots\\\\ 0 & \\ldots & \\ldots & 1 \\end{bmatrix}. Note that any second order tensor can be represented as a matrix, but such a representation is not possible for higher order tensors. Let us quickly consider the representation of \\mathsf{A} \\in \\mathcal{T}^k(V) with respect to a general basis (\\mathsf{g}_i) of V . As before, the multilinearity of \\mathsf{A} can be used to express it in terms of the basis (\\mathsf{g}_i) as follows: for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_k \\in V , \\begin{split} \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k) &= \\mathsf{A}\\left(\\sum u_{1i_1}\\mathsf{g}_{i_1}, \\ldots, \\sum u_{ki_k}\\mathsf{g}_{i_k}\\right)\\\\ &= \\sum \\mathsf{A}(\\mathsf{g}_{i_1}, \\ldots, \\mathsf{g}_{i_k}) u_{1i_1} \\ldots u_{ki_k},\\\\ &= \\sum A^*_{i_1 \\ldots i_k} (\\mathsf{g}^{i_1} \\otimes \\ldots \\otimes \\mathsf{g}^{i_k})(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k), \\end{split} where A^*_{i_1\\ldots i_k} = \\mathsf{A}(\\mathsf{g}_{i_1}, \\ldots, \\mathsf{g}_{i_k}) . Since this holds for any choice of \\mathsf{u}_1, \\ldots, \\mathsf{u}_k \\in V , it follows that \\mathsf{A} = \\sum A^*_{i_1 \\ldots i_k} \\mathsf{g}^{i_1} \\otimes \\ldots \\otimes \\mathsf{g}^{i_k}. It is straightforward to check that the set of n^k tensors (\\mathsf{g}^{i_1} \\otimes \\ldots \\otimes \\mathsf{g}^{i_k}) form a basis of \\mathcal{T}^k(V) . This also shows that the dimension of \\mathcal{T}^k(\\mathbb{R}^3) is n^k . The coefficients A^*_{i_1\\ldots i_k} are called the c ovariant components of \\mathsf{A} \\in \\mathcal{T}^k(V) with respect to the basis (\\mathsf{g}^{i_1} \\otimes \\ldots \\mathsf{g}^{i_k}) of \\mathcal{T}^k(V) . An alternative means to represent the basis representation of the tensor \\mathsf{A} \\in \\mathcal{T}^k(V) can be obtained using the relations \\mathsf{g}^i = \\sum g^{ij}\\mathsf{g}_j in the basis expansion derived earlier. This yields the following representation: \\mathsf{A} = \\sum A_{i_1\\ldots i_k} \\mathsf{g}_{i_1} \\otimes \\ldots \\otimes \\mathsf{g}_{i_k}, where A_{i_1\\ldots i_k} = \\sum g^{i_1j_1}\\ldots g^{i_kj_k}A^*_{j_1\\ldots j_k}. It is left as an easy exercise to prove that the set of n^k multilinear maps (\\mathsf{g}_{i_1} \\otimes \\ldots \\otimes \\mathsf{g}_{i_k}) also form a basis of \\mathcal{T}^k(V) . The corresponding components of the tensor \\mathsf{A} \\in \\mathcal{T}^k(V) are the constants A_{i_1 \\ldots i_k} , and are called the contravariant components of \\mathsf{A} . Remark It is also possible to defined mixed components of a given tensor by having few of the indices as covariant and the remaining as contravariant, but such generalizations are not considered here in the interest of simplicity. Change of basis Note that the representation of a tensor on V is always with respect to some choice of basis on V . Let us now study how this representation changes when we change the basis of V . As before, let us first consider the simple case of orthonormal bases. Let (\\mathsf{e}_i) and (\\mathsf{f}_i) be two orthonormal bases of V . Let the representation of a k^{\\text{th}} order tensor \\mathsf{A} \\in \\mathcal{T}^k(V) with respect to these bases be given by \\begin{split} \\mathsf{A} &= \\sum A_{i_1\\ldots i_k} \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}, \\quad A_{i_1\\ldots i_k} = \\mathsf{A}(\\mathsf{e}_{i_1}, \\ldots, \\mathsf{e}_{i_k}),\\\\ \\mathsf{A} &= \\sum \\tilde{A}_{i_1\\ldots i_k} \\mathsf{f}_{i_1} \\otimes \\ldots \\otimes \\mathsf{f}_{i_k}, \\quad \\tilde{A}_{i_1\\ldots i_k} = \\mathsf{A}(\\mathsf{f}_{i_1}, \\ldots, \\mathsf{f}_{i_k}). \\end{split} If \\mathsf{f}_i = \\sum Q_{ji}\\mathsf{e}_j , then we see that \\begin{split} \\sum A_{i_1\\ldots i_k} \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k} &= \\sum \\tilde{A}_{i_1\\ldots i_k} \\mathsf{f}_{i_1} \\otimes \\ldots \\otimes \\mathsf{f}_{i_k}\\\\ &= \\sum \\tilde{A}_{i_1\\ldots i_k} \\left(\\sum Q_{j_1i_1}\\mathsf{e}_{j_1}\\right) \\otimes \\ldots \\otimes \\left(\\sum Q_{j_ki_k}\\mathsf{e}_{j_k}\\right)\\\\ &= \\sum Q_{j_1i_1} \\ldots Q_{j_ki_k} \\tilde{A}_{i_1\\ldots i_k} \\mathsf{e}_{j_1} \\otimes \\ldots \\otimes \\mathsf{e}_{j_k}, \\end{split} which shows that \\begin{split} A_{j_1\\ldots j_k} &= \\sum Q_{j_1i_1} \\ldots Q_{j_ki_k} \\tilde{A}_{i_1\\ldots i_k}\\\\ \\Rightarrow \\tilde{A}_{i_1\\ldots i_k} &= \\sum Q^{-1}_{i_1j_1} \\ldots Q^{-1}_{i_kj_k} A_{j_1\\ldots j_k}\\\\ &= \\sum Q_{j_1i_1} \\ldots Q_{j_ki_k} A_{j_1\\ldots j_k}. \\end{split} !!! info \"Remark\" We can equivalently derive this transformation rule as follows \\begin{split} \\tilde{A}_{i_1\\ldots i_k} &= \\mathsf{A}(\\mathsf{f}_{i_1}, \\ldots, \\mathsf{f}_{i_k})\\\\ &= \\mathsf{A}\\left(\\sum Q_{j_1i_1}\\mathsf{e}_{j_1}, \\ldots, \\sum Q_{j_ki_k}\\mathsf{e}_{j_k}\\right)\\\\ &= \\sum Q_{j_1i_1}\\ldots Q_{j_1i_1} A_{j_1 \\ldots j_k}. \\end{split} Notice how the orthonormality of the two bases (\\mathsf{e}_i) and (\\mathsf{f}_i) of V are implicitly used in this derivation. Example Let us revisit the second order tensor \\hat{\\mathsf{I}} \\in \\mathcal{T}^2(V) that we studied earlier. Suppose that (\\mathsf{e}_i) and (\\mathsf{f}_i) are orthonormal bases of V such that \\mathsf{f}_i = \\sum Q_{ji}\\mathsf{e}_j . Let us briefly consider the case when general bases are employed. Suppose that the tensor \\mathsf{A} \\in \\mathcal{T}^k(V) has components A^*_{i_1 \\ldots i_k} and \\tilde{A}^*_{i_1 \\ldots i_k} with respect to bases (\\mathsf{g}^{i_1} \\otimes \\ldots \\otimes \\mathsf{g}^{i_k}) and (\\tilde{\\mathsf{g}}^{i_1} \\otimes \\ldots \\otimes \\tilde{\\mathsf{g}}^{i_k}) , respectively. The relationship between these components is easily computed as follows: \\begin{split} \\tilde{A}^*_{i_1 \\ldots i_k} &= \\mathsf{A}(\\tilde{\\mathsf{g}}_{i_1}, \\ldots, \\tilde{\\mathsf{g}}_{i_k})\\\\ &= \\mathsf{A}\\left(\\sum (\\tilde{\\mathsf{g}}_{i_1}\\cdot\\mathsf{g}^{j_1})\\mathsf{g}_{j_1}, \\ldots, \\sum (\\tilde{\\mathsf{g}}_{i_k}\\cdot\\mathsf{g}^{j_k})\\mathsf{g}_{j_k}\\right)\\\\ &= \\sum (\\tilde{\\mathsf{g}}_{i_1}\\cdot\\mathsf{g}^{j_1})\\ldots(\\tilde{\\mathsf{g}}_{i_k}\\cdot\\mathsf{g}^{j_k}) A^*_{j_1 \\ldots j_k}. \\end{split} The inverse of this relation can also be computed similarly. Contraction Let V be a finite dimensional inner product space, and let (\\mathsf{g}_i) be a general basis of V . Given a tensor \\mathsf{A} \\in \\mathcal{T}^k(V) of order k , where k \\ge 2 , the (i,j) -contraction of \\mathsf{A} is the tensor \\mathcal{C}_{i,j}\\mathsf{A} \\in \\mathcal{T}^{k-2}(V) of order (k - 2) defined as follows: for any \\mathsf{v}_1, \\ldots, \\mathsf{v}_k \\in V , \\begin{split} & \\mathcal{C}_{i,j}\\mathsf{A}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_{i-1}, \\mathsf{v}_{i+1}, \\ldots, \\mathsf{v}_{j-1},\\mathsf{v}_{j+1}, \\ldots, \\mathsf{v}_k)\\\\ = & \\sum_a \\mathsf{A}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_{i-1}, \\mathsf{g}_a, \\mathsf{v}_{i+1}, \\ldots, \\mathsf{v}_{j-1}, \\mathsf{g}^a, \\mathsf{v}_{j+1}, \\ldots, \\mathsf{v}_k). \\end{split} Remark It can be verified that \\mathsf{g}_a and \\mathsf{g}^a in the definition of the contraction can be swapped: \\sum \\mathsf{A}(\\ldots, \\mathsf{g}_a, \\ldots, \\mathsf{g}^a, \\ldots) = \\sum \\mathsf{A}(\\ldots, \\mathsf{g}^a, \\ldots, \\mathsf{g}_a, \\ldots). Thus, the order in which \\mathsf{g}_a and \\mathsf{g}^a appears is irrelevant. Unlike the various definitions provided earlier, note that this definition seemingly depends on the choice of a basis (\\mathsf{g}_i) . It is left as a simple exercise to verify that if another basis (\\mathsf{f}_i) of V is chosen instead, the contraction operation yields the same tensor \\mathcal{C}_{i,j}\\mathsf{A} \\in \\mathcal{T}^{k-2}(V) as when the bases (\\mathsf{g}_i) are chosen. The contraction operation is thus well-defined . The basis representation of the contracted tensor can be easily computed. With respect to a basis (\\mathsf{g}_i) of V , recall that any \\mathsf{A} \\in \\mathcal{T}^k(V) can be written as \\mathsf{A} = \\sum A_{a_1 \\ldots a_k} \\mathsf{g}_{a_1} \\otimes \\ldots \\otimes \\mathsf{g}_{a_k}. The (i,j) contraction of \\mathsf{A} is easily seen to have the following representation: \\begin{split}\\mathcal{C}_{i,j}\\mathsf{A} &= \\sum g_{a_ia_j}A_{a_1\\ldots a_{i-1} a_i a_{i+1} \\ldots a_{j-1} a_j a_{j+1}\\ldots a_k}\\\\ & \\qquad\\quad\\mathsf{g}_{a_1} \\otimes \\mathsf{g}_{a_{i-1}} \\otimes \\mathsf{g}_{a_{i+1}} \\otimes \\ldots \\otimes \\mathsf{g}_{a_{j-1}}\\otimes\\mathsf{g}_{a_{j+1}}\\otimes \\ldots \\otimes \\mathsf{g}_{a_k}. \\end{split} In the special case of a second order tensor \\mathsf{B} \\in \\mathcal{T}^2(V) , there is only one possible contraction, \\mathcal{C}_{1,2}\\mathsf{B} \\in \\mathbb{R} , which is a tensor of order 0 , or just a real number. The contraction operation, in this case, is called the trace . It is customary to write the trace of \\mathsf{B} as \\text{tr}(\\mathsf{B}) . With respect to a basis (\\mathsf{g}_i) of V , \\mathsf{B} = \\sum B_{ij} \\, \\mathsf{g}_i \\otimes \\mathsf{g}_j \\quad\\Rightarrow\\quad \\text{tr}(\\mathsf{B}) = \\sum g_{ab}B_{ab}. It can be easily checked that \\mathsf{tr}(\\mathsf{B}) is independent of the choice of basis. Note that in the special case when (\\mathsf{g}_i) is an orthonormal basis of V , the expression for the trace of \\mathsf{B} reduces to the familiar form \\text{tr}(\\mathsf{B}) = \\sum B_{aa} . An alternative definition of the trace of a linear map will be provided in a later section. Generalized dot product of tensors Note! This terminology is not standard, but it is adequate for the purposes of this course. Certain special operations called generalized dot products , or simply dot products , are now introduced between tensors of different orders. These are introduced on account of their prevalence in the continuum mechanics literature. Throughout this section, V denotes a finite dimensional inner product space. To motivate the definition of the generalized dot product of tensors on V , it is helpful to first consider a few important special cases. In the simplest case, given two vectors \\mathsf{u}, \\mathsf{v} \\in V , note that the inner product of these two vectors can be expressed in terms of the contraction operation as follows: \\mathsf{u} \\cdot \\mathsf{v} = \\mathcal{C}_{1,2}(\\mathsf{u} \\otimes \\mathsf{v}). This restatement of the inner product in terms of the contraction operation will serve as the starting point for its generalization to the dot product of two arbitrary tensors. Suppose that \\mathsf{u}, \\mathsf{v}, \\mathsf{w} \\in V are any three vectors in V . Then the vector (\\mathsf{u} \\otimes \\mathsf{v}) \\cdot \\mathsf{w} \\in V is defined as follows: (\\mathsf{u} \\otimes \\mathsf{v}) \\cdot \\mathsf{w} = \\mathcal{C}_{2,3}(\\mathsf{u} \\otimes \\mathsf{v} \\otimes \\mathsf{w}). To understand what this means, consider the action of the vector \\mathcal{C}_{2,3}(\\mathsf{u} \\otimes \\mathsf{v} \\otimes \\mathsf{w}) on an arbitrary vector \\mathsf{z} \\in V : if (\\mathsf{g}_i) is a general basis of V , then \\begin{split} \\mathcal{C}_{2,3}(\\mathsf{u} \\otimes \\mathsf{v} \\otimes \\mathsf{w})(\\mathsf{z}) &= \\sum (\\mathsf{u} \\otimes \\mathsf{v} \\otimes \\mathsf{w})(\\mathsf{z}, \\mathsf{g}_a, \\mathsf{g}^a)\\\\ &= \\sum u_i v_j w_k (\\mathsf{g}_i \\otimes \\mathsf{g}_j \\otimes \\mathsf{g}_k)\\left(\\sum z_b \\mathsf{g}_b, \\mathsf{g}_a, \\mathsf{g}^a\\right)\\\\ &= \\sum g_{ib} u_i v_j w_k z_b g_{ja} \\delta_{ka}\\\\ &= \\sum g_{jk} v_j w_k \\, \\sum g_{ia} u_a z_a\\\\ &= (\\mathsf{v} \\cdot \\mathsf{w})(\\mathsf{u} \\cdot \\mathsf{z})\\\\ &= (\\mathsf{v} \\cdot \\mathsf{w})\\mathsf{u}(\\mathsf{z}). \\end{split} Since this is true for any \\mathsf{z} \\in V , it follows that (\\mathsf{u} \\otimes \\mathsf{v}) \\cdot \\mathsf{w} = (\\mathsf{v} \\cdot \\mathsf{w}) \\mathsf{u}. This result also provides a means to compute the dot product \\mathsf{A} \\cdot \\mathsf{v} of a second order tensor \\mathsf{A} \\in \\mathcal{T}^2(V) and a vector \\mathsf{v} \\in V : if (\\mathsf{g}_i) is any basis of V , then \\begin{split} \\mathsf{A} \\cdot \\mathsf{v} &= \\left(\\sum A_{ij} \\mathsf{g}_i \\otimes \\mathsf{g}_j\\right)\\cdot\\left(\\sum v_k \\mathsf{g}_k\\right)\\\\ &= \\sum A_{ij} v_k g_{jk} \\mathsf{g}_i \\in V. \\end{split} Note that \\tilde{\\mathsf{A}} \\in L(V,V) denote the linear map corresponding to the second order tensor \\mathsf{A} \\in \\mathcal{T}^2(V) , then, for any \\mathsf{v} \\in V , \\mathsf{A} \\cdot \\mathsf{v} = \\tilde{\\mathsf{A}}\\mathsf{v} . This is most easily seen by representing both sides this equation with respect to an orthonormal basis of V . This shows that the dot product defined here is consistent with the theory of linear maps developed earlier. Remark Note that given an arbitrary second order tensor \\mathsf{A} \\in \\mathcal{T}^2(V) and an arbitrary vector \\mathsf{v} \\in V , \\mathsf{A} \\cdot \\mathsf{v} \\in V and \\mathsf{v} \\cdot \\mathsf{A} \\in V are, in general, different vectors. The generalized dot product of tensors is thus not necessarily symmetric. As an extension of the foregoing ideas, we now define dot products for tensors of second order. Given vectors \\mathsf{u}, \\mathsf{v}, \\mathsf{w}, \\mathsf{z} \\in V , the dot product (\\mathsf{u} \\otimes \\mathsf{v}) \\cdot (\\mathsf{w} \\otimes \\mathsf{z}) \\in \\mathbb{R} between the second order tensors \\mathsf{u} \\otimes \\mathsf{v} \\in \\mathcal{T}^2(V) and \\mathsf{w} \\otimes \\mathsf{z} \\in \\mathcal{T}^2(V) is defined as follows: (\\mathsf{u} \\otimes \\mathsf{v}) \\cdot (\\mathsf{w} \\otimes \\mathsf{z}) = \\mathcal{C}_{1,2}\\left(\\mathcal{C}_{1,3}\\left(\\mathsf{u} \\otimes \\mathsf{v} \\otimes \\mathsf{w} \\otimes \\mathsf{z}\\right)\\right) = (\\mathsf{u} \\cdot \\mathsf{w})(\\mathsf{v} \\cdot \\mathsf{z}). The simplest means to understand this by considering the dot products of two second order tensors \\mathsf{A}, \\mathsf{B} \\in \\mathcal{T}^2(V) . If (\\mathsf{g}_i) denotes an orthonormal basis of V , and \\mathsf{A} = \\sum A_{ij} \\mathsf{g}_i \\otimes \\mathsf{g}_j and \\mathsf{B} = \\sum B_{ij} \\mathsf{g}_i \\otimes \\mathsf{g}_j are the representations of \\mathsf{A} and \\mathsf{B} , respectively, with respect to this basis, then note that \\mathsf{A} \\cdot \\mathsf{B} = \\sum A_{ij} B_{ij}. Remark It is important to note that different authors follow different conventions regarding this. For instance, it common in the Continuum Mechanics literature to use the following notations: for any two second order tensors \\mathsf{A},\\mathsf{B} \\in \\mathcal{T}^2(V) , \\begin{split} \\mathsf{A} \\cdot \\mathsf{B} &= \\sum A_{ij}B_{ji},\\\\ \\mathsf{A} : \\mathsf{B} &= \\sum A_{ij}B_{ij}, \\end{split} where (A_{ij}) and (B_{ij}) are the components of \\mathsf{A} and \\mathsf{B} , respectively, with respect to some orthonormal basis of V . Note that the dot product \\mathsf{A} \\cdot \\mathsf{B} , according to our definition, is \\mathsf{A} : \\mathsf{B} according to this definition. Care must be therefore exercised when reading the literature to understand the appropriate meaning of a quantity like \\mathsf{A} \\cdot \\mathsf{B} . The reason for defining the generalized dot product the way we have done is to ensure a uniform and simple notation for many differential and integral identities that we will encounter later on. It is important, however, to keep in mind that this is essentially a matter of convention. As a final and useful example, note that if \\mathsf{A} \\in \\mathcal{T}^k(V) is a tensor of order k , where k > 2 , and \\mathsf{B} \\in \\mathcal{T}^2(V) is a second order tensor, then, with respect to an orthonormal basis (\\mathsf{g}_i) of V , \\mathsf{A} \\cdot \\mathsf{B} = \\sum A_{i_1 \\ldots i_{k-2} a b} B_{ab}. The extension of this notion of dot products can be similarly extended to tensors of higher orders. Example Suppose that \\mathsf{A}, \\mathsf{B} \\in \\mathcal{T}^2(V) are second order tensors on V . Then, it is true that \\mathsf{A} \\cdot \\mathsf{B} = \\mathsf{B} \\cdot \\mathsf{A}. This is most easily seen with the help of the basis representation with respect to an orthonormal basis of V : in this case, \\mathsf{A} \\cdot \\mathsf{B} = \\sum A_{ij} B_{ij} = \\sum B_{ij} A_{ij} = \\mathsf{B} \\cdot \\mathsf{A}, thereby establishing the claim. Example Suppose that \\mathsf{A} \\in \\mathcal{T}^2(V) is a second order tensor on V and \\mathsf{u},\\mathsf{v} \\in V are two vectors in V , then \\mathsf{A} \\cdot (\\mathsf{u} \\otimes \\mathsf{v}) = \\mathsf{u} \\cdot \\mathsf{A}\\mathsf{v}. This is also easily established by choosing an orthonormal (\\mathsf{g}_i) basis of $V$. The component form of \\mathsf{A} \\cdot (\\mathsf{u} \\otimes \\mathsf{v}) then reads \\begin{split}\\mathsf{A} \\cdot (\\mathsf{u} \\otimes \\mathsf{v}) &= \\sum A_{ij} u_i v_j\\\\ &= \\left(\\sum u_k \\mathsf{g}_k\\right) \\cdot \\left(\\sum A_{ij} v_j \\mathsf{g}_i\\right)\\\\ &= \\mathsf{u} \\cdot \\mathsf{A}\\mathsf{v}.\\end{split} It can be shown similarly that (\\mathsf{u} \\otimes \\mathsf{v})\\cdot\\mathsf{A} = \\mathsf{A}^T\\mathsf{u} \\cdot \\mathsf{v} . Volume forms Let V be an inner product space of dimension n . A tensor \\mathsf{A} \\in \\mathcal{T}^k(V) is said to be symmetric if, for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_k \\in V , \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_i, \\ldots, \\mathsf{u}_j, \\ldots, \\mathsf{u}_n) = \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_j, \\ldots, \\mathsf{u}_i, \\ldots, \\mathsf{u}_n), where 1 \\le i < j \\le n . If the sign reverses when any two arguments are interchanged, then the tensor is said to be skew-symmetric . Remark It turns out that the set of all antisymmetric tensors of a given order have a rich algebraic structure, called exterior algebra . A volume form on V is a skew-symmetric tensor \\mathsf{\\epsilon}:\\times^n V \\to \\mathbb{R} of order n . It is customary to denote the set of all volume forms on V using the notation \\Omega^n(V) : \\Omega^n(V) = \\{ \\mathsf\\epsilon \\in \\mathcal{T}^n(V) \\,|\\, \\mathsf\\epsilon \\text{ is skew-symmetric}\\}. It is important to note that the trivial volume form \\mathsf{0} \\in \\mathcal{T}^n(V) that maps every set of n vectors in V to zero is excluded from this discussion. A volume form that is not trivial is said to be non-trivial . All volume forms considered here are assumed to be non-trivial. Given two volume forms \\mathsf{\\epsilon}, \\mathsf{\\omega} \\in \\Omega^n(V) , it can be shown that there exists a scalar a \\in \\mathbb{R} such that \\mathsf\\omega = a\\mathsf{\\epsilon} . An equivalent way of stating this is that the set of all antisymmetric tensors of order n over an n -dimensional vector space is a vector space of dimension 1 : \\text{dim}(\\Omega^n(V)) = 1 . Picking a particular volume form \\mathsf{\\epsilon} \\in \\Omega^n(V) , the set of all volume forms \\mathsf{\\omega} \\in \\Omega^n(V) such that \\mathsf\\omega = a\\mathsf{\\epsilon} for some a > 0 are said to constitute an orientation of V . Given this orientation on V , a basis (\\mathsf{g}_i) of V is said to be right-handed if \\epsilon(\\mathsf{g}_1, \\ldots, \\mathsf{g}_n) > 0 , and left-handed otherwise. Remark It is emphasized that the choice of orientation is arbitrary. Typically, a special volume form that has mathematical or physical relevance is chosen and the positive orientation is defined with respect to this choice. With a particular choice of orientation, it is customary to focus only on only right-handed bases. Rather than discussing the general theory of volume forms, it is useful to look at volume forms on the three dimensional Euclidean space \\mathbb{R}^3 . The standard volume form on \\mathbb{R}^3 , written \\mathsf{\\epsilon} \\in \\Omega^3(\\mathbb{R}^3) , sometimes called the Levi-Civita tensor , is defined as follows: if (\\mathsf{e}_i) denotes the standard basis of \\mathbb{R}^3 , \\mathsf{\\epsilon} = \\sum \\epsilon_{ijk} \\mathsf{e}_i \\otimes \\mathsf{e}_j \\otimes \\mathsf{e}_k, where \\epsilon_{ijk} is the Levi-Civita symbol introduced earlier in the context of the cross product. Note that \\mathsf\\epsilon(\\mathsf{e}_1, \\mathsf{e}_2, \\mathsf{e}_3) = \\epsilon_{123} = 1 - the standard basis of \\mathbb{R}^3 is thus right handed with respect to the standard volume form of \\mathbb{R}^3 . Cross product in $\\mathbb{R}^3$ The three dimensional Euclidean space \\mathbb{R}^3 admits another special algebraic operation. Given any two vectors \\mathsf{u} = \\sum u_i \\mathsf{e}_i \\in \\mathbb{R}^3 and \\mathsf{v} = \\sum v_i \\mathsf{e}_i \\in \\mathbb{R}^3 , where (\\mathsf{e}_i) is the standard basis of \\mathbb{R}^3 , their cross product is defined as the vector \\mathsf{u} \\times \\mathsf{v} \\in \\mathbb{R}^3 given by \\mathsf{u} \\times \\mathsf{v} = (u_2 v_3 - u_3 v_2)\\mathsf{e}_1 + (u_3 v_1 - u_1 v_3)\\mathsf{e}_2 + (u_1 v_2 - u_2 v_1)\\mathsf{e}_3. This can be written compactly by introducing the Levi-Civita symbol \\epsilon_{ijk} , \\epsilon_{ijk} = \\begin{cases} 1, & \\{i,j,k\\}\\text{ is an even permutation of $\\{1,2,3\\}$},\\\\ -1, & \\{i,j,k\\}\\text{ is an odd permutation of $\\{1,2,3\\}$},\\\\ 0, & \\text{otherwise}, \\end{cases} as follows: \\mathsf{u} \\times \\mathsf{v} = \\sum \\epsilon_{ijk} u_j v_k \\mathsf{e}_i, as can be easily checked. The reciprocal basis can be computed easily in the special case of the three dimensional Euclidean space \\mathbb{R}^3 using the cross product. Given any basis (\\mathsf{g}_i) of \\mathbb{R}^3 , the corresponding reciprocal basis (\\mathsf{g}^i) of \\mathbb{R}^3 can be computed as \\mathsf{g}^1 = \\frac{\\mathsf{g}_2 \\times \\mathsf{g}_3}{\\mathsf{g}_1 \\cdot \\mathsf{g}_2 \\times \\mathsf{g}_3}, \\quad \\mathsf{g}^2 = \\frac{\\mathsf{g}_3 \\times \\mathsf{g}_1}{\\mathsf{g}_1 \\cdot \\mathsf{g}_2 \\times \\mathsf{g}_3}, \\quad \\mathsf{g}^3 = \\frac{\\mathsf{g}_1 \\times \\mathsf{g}_2}{\\mathsf{g}_1 \\cdot \\mathsf{g}_2 \\times \\mathsf{g}_3}. It is a simple exercise to check these formulae satisfy the defining condition of the reciprocal basis: \\mathsf{g}^i \\cdot \\mathsf{g}_j = \\delta_{ij} . An alternative definition of the cross product of two vectors in \\mathbb{R}^3 can be given in terms of the standard volume form. Given vectors \\mathsf{v}, \\mathsf{w} \\in \\mathbb{R}^3 , their cross product \\mathsf{v} \\times \\mathsf{w} \\in \\mathbb{R}^3 is defined as follows: for any \\mathsf{u} \\in \\mathbb{R}^3 , \\mathsf{u} \\cdot (\\mathsf{v} \\times \\mathsf{w}) = \\mathsf\\epsilon(\\mathsf{u}, \\mathsf{v}, \\mathsf{w}). It is left as an exercise to verify this. ( Hint. Choose \\mathsf{u} to be the standard basis \\mathsf{e}_i . This immediately yields: (\\mathsf{v}\\times\\mathsf{w})_i = \\sum \\epsilon_{ijk} v_j w_k .)","title":"Tensor Algebra"},{"location":"tensor_algebra/#multilinear-functions-and-tensors","text":"Let us extend the definition of bilinear functions introduced in the preceding section to multilinear functions. Given inner product spaces V_1, \\ldots, V_n , consider a map \\mathsf{T}:V_1 \\times \\ldots \\times V_n \\to \\mathbb{R} such that for any \\mathsf{u}_i, \\mathsf{v}_i \\in V_i , 1 \\le i \\le n , and a \\in \\mathbb{R} , \\mathsf{T}(\\mathsf{v}_1, \\ldots, \\mathsf{u}_i + a\\mathsf{v}_i, \\ldots, \\mathsf{v}_n) = \\mathsf{T}(\\mathsf{v}_1, \\ldots, \\mathsf{u}_i, \\ldots, \\mathsf{v}_n) + a\\,\\mathsf{T}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_i, \\ldots, \\mathsf{v}_n). Such functions, which are linear separately in each of their arguments, are said to be multilinear . We will denote the set of all multilinear functions of this form as \\mathcal{T}(V_1 \\times \\ldots \\times V_n,\\mathbb{R}) . Note that multilinearity and linearity are distinct concepts, as the following example illustrates. Example Consider the functions \\mathsf{S}, \\mathsf{T}:\\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R} defined as follows: for any x, y \\in \\mathbb{R} , \\mathsf{S}(x, y) = x + y, \\qquad \\mathsf{T}(x, y) = xy. It follows that given a, u, v \\in \\mathbb{R} , \\begin{split} \\mathsf{S}((x,y) + c(u,v)) &= (x + y) + c(u + v) = \\mathsf{S}(x,y) + a\\mathsf{S}(u,v),\\\\ \\mathsf{T}((x,y) + a(u,v)) &= (x + au)(y + av) = xy + a^2uv + axv + ayu \\neq \\mathsf{T}(x,y) + a\\mathsf{T}(u,v). \\end{split} We thus see that \\mathsf{S} is linear, but \\mathsf{T} is not. On the other hand, note that \\begin{split} \\mathsf{S}(x + au, y) &= x + y + au \\neq \\mathsf{S}(x,y) + a\\mathsf{S}(u,y),\\\\ \\mathsf{T}(x + au, y) &= (x + au)y = xy + auy = \\mathsf{T}(x,y) + a\\mathsf{T}(u,y). \\end{split} We thus see that \\mathsf{S} is not multilinear, even though it is linear, and \\mathsf{T} is multilinear and not linear. Let us now focus attention on a single inner product space V of dimension n and multilinear functions defined on finite Cartesian products of V . A tensor of order k on V is defined as multilinear function of the form \\mathsf{A}:\\underbrace{V \\times \\ldots \\times V}_{k \\text{ terms}} \\to \\mathbb{R}, The set of all multilinear maps of the form \\mathsf{A}:\\times^k V \\to \\mathbb{R} is denoted by \\mathcal{T}^k(V) . Thus \\mathcal{T}^k(V) denotes the set of all tensors of order k on V . Defining maps +:\\mathcal{T}^k(V) \\times \\mathcal{T}^k(V) \\to \\mathcal{T}^k(V) and \\cdot:\\mathbb{R} \\times \\mathcal{T}^k(V) \\to \\mathcal{T}^k(V) as \\begin{split} (\\mathsf{A} + \\mathsf{B})(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k) &= \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k) + \\mathsf{B}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k),\\\\ (a\\mathsf{A})(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k) &= a \\, \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k), \\end{split} for any \\mathsf{A},\\mathsf{B} \\in \\mathcal{T}^k(V) , a \\in \\mathbb{R} and \\mathsf{u}_1, \\ldots, \\mathsf{u}_k \\in V , it is easy to verify that \\mathcal{T}^k(V) is a real linear space. The set \\mathcal{T}^1(V) = \\{\\mathsf{T}:V \\to \\mathbb{R} \\,|\\, \\mathsf{T} \\text{ is linear}\\} , defined as the set of all linear functions on V , is called the (algebraic) dual space of V , and is often written as V^* . It turns out that if V is a finite dimensional inner product space, then V^* is canonically isomorphic to V . This means that V and V^* are identical for all practical purposes. For this reason, a vector is often called a tensor of order 1 . Further, in this case, the action of any \\mathsf{v} \\in \\mathcal{T}^1(V) on any \\mathsf{u} \\in V is defined as follows: \\mathsf{v}(\\mathsf{u}) = \\mathsf{v} \\cdot \\mathsf{u}. This is a simple instance of what is known as the Riesz representation theorem . It is conventional to define \\mathcal{T}^0(V) = \\mathbb{R} . Thus, tensors of order 0 are scalars, tensors of order 1 are vectors, and tensors of order 2 can be identified with linear maps. The present definition thus unifies the various kinds of linear spaces on V studied earlier. The set of all \\mathcal{T}^k(V) is said to constitute a tensor algebra on V . Example As a simple but important example of tensors, consider the second order tensor \\hat{\\mathsf{I}} \\in \\mathcal{T}^2(V) on V defined as follows: for any \\mathsf{u},\\mathsf{v} \\in V , \\hat{\\mathsf{I}}(\\mathsf{u},\\mathsf{v}) = \\mathsf{u} \\cdot \\mathsf{v}. The multilinearity of \\hat{\\mathsf{I}} , in this case just its bilinearity, follows from the bilinearity of the inner product. More generally, given any linear map \\mathsf{T} \\in L(V,V) , we can define a second order tensor \\hat{\\mathsf{T}} \\in \\mathcal{T}^2(V) as follows: for any \\mathsf{u},\\mathsf{v} \\in V , \\hat{\\mathsf{T}}(\\mathsf{u},\\mathsf{v}) = \\mathsf{u} \\cdot \\mathsf{T}\\mathsf{v}. It is left as a simple exercise to verify that the map \\hat{\\mathsf{T}} is a second order tensor on V . Remark Defining the identity map on V as the map \\mathsf{I} \\in L(V,V) such that \\mathsf{I}\\mathsf{v} = \\mathsf{v} for any \\mathsf{v} \\in V , note that the corresponding second order tensor is precisely the tensor \\hat{\\mathsf{I}} \\in \\mathcal{T}^2(V) introduced earlier.","title":"Multilinear functions and tensors"},{"location":"tensor_algebra/#tensor-products","text":"Given two tensors \\mathsf{A} \\in \\mathcal{T}^k(V) and \\mathsf{B} \\in \\mathcal{T}^l(V) , it is possible to combine them to obtain a tensor of higher order. Specifically, the tensor product of \\mathsf{A} and \\mathsf{B} is defined as the tensor \\mathsf{A}\\otimes\\mathsf{B} \\in \\mathcal{T}^{k + l}(V) such that for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_k, \\mathsf{u}_{k+1}, \\ldots, \\mathsf{u}_{k + l} \\in V , \\mathsf{A} \\otimes \\mathsf{B}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k, \\mathsf{u}_{k+1}, \\ldots, \\mathsf{u}_{k + l}) = \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k)\\mathsf{B}(\\mathsf{u}_{k+1}, \\ldots, \\mathsf{u}_{k + l}). As a special case given vectors \\mathsf{v}, \\mathsf{w} \\in V , their tensor product yields a second order tensor \\mathsf{v} \\otimes \\mathsf{w} \\in \\mathcal{T}^2(V) : for any \\mathsf{u}_1, \\mathsf{u}_2 \\in V , \\mathsf{v} \\otimes \\mathsf{w} (\\mathsf{u}_1, \\mathsf{u}_2) = (\\mathsf{v} \\cdot \\mathsf{u}_1)(\\mathsf{w} \\cdot \\mathsf{u}_2). The foregoing definition can be extended to define the tensor product of a finite number of tensors. Suppose that \\mathsf{u}_1, \\ldots, \\mathsf{u}_k are k vectors in V . The tensor product of these vectors is defined as the multilinear map \\mathsf{u}_1 \\otimes \\ldots \\otimes \\mathsf{u}_k: V^k \\to \\mathbb{R}, such that for any set of k vectors \\mathsf{v}_1, \\ldots, \\mathsf{v}_k in V , \\mathsf{u}_1 \\otimes \\ldots \\otimes \\mathsf{u}_k(\\mathsf{v}_1, \\ldots, \\mathsf{v}_k) = (\\mathsf{u}_1\\cdot \\mathsf{v}_1) \\ldots (\\mathsf{u}_k\\cdot \\mathsf{v}_k). Thus, the tensor product allows us to construct higher order tensors from lower order tensors. A higher order tensor that is constructed from vectors, like the one shown above, is called a rank- 1 tensor. Important Note! There is an ambiguity in the tensor product notation that warrants clarification. Given vectors \\mathsf{v}, \\mathsf{w} \\in V , the quantity \\mathsf{v}\\otimes\\mathsf{w} can stand for either a linear map, i.e. an element of L(V,V) , or a tensor of order 2 , i.e. an element of \\mathcal{T}^2(V) . The vector spaces L(V,V) and \\mathcal{T}^2(V) are isomorphic - this means that they can be naturally identified with each other. Indeed, given any \\mathsf{T} \\in L(V,V) , the bilinear function \\hat{\\mathsf{T}} \\in \\mathcal{T}^2(V) defined as \\hat{\\mathsf{T}}(\\mathsf{u}_1,\\mathsf{u}_2) = \\mathsf{u}_1\\cdot\\mathsf{T}\\mathsf{u}_2, for any \\mathsf{u}_1,\\mathsf{u}_2 \\in V , can be used to uniquely associate an element of L(V,V) with \\mathcal{T}^2(V) , and vice versa. This isomorphism is often abused to represent both \\mathsf{T} \\in L(V,V) and \\hat{\\mathsf{T}} \\in \\mathcal{T}^2(V) using the same symbol. In practice, such an ambiguity is averted by specifying the domain and codomain of every function that is used. A similar ambiguity arises in the case of tensors of higher order - the appropriate meaning is to be inferred from the context.","title":"Tensor products"},{"location":"tensor_algebra/#basis-representation","text":"Recall that the set \\mathcal{T}^k(V) of all tensors of order k on V is a linear space. This means that we can legitimately ask how we can construct a basis for it, and represent any k^{\\text{th}} order tensor in terms of this basis. This question will be explored in this section. Let us first consider the special case when an orthonormal basis (\\mathsf{e}_i) of V is provided. Then, the representation of a k^{\\text{th}} order tensor \\mathsf{A} \\in \\mathcal{T}^k(V) is easily computed by considering the action of \\mathsf{A} on k vectors \\mathsf{v}_i = \\sum v_{ij} \\mathsf{e}_j in V , where 1 \\le i \\le k : \\begin{split} \\mathsf{A}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_k) &= \\mathsf{A}\\left(\\sum v_{1i_1} \\mathsf{e}_{i_1}, \\ldots, \\sum v_{ki_k} \\mathsf{e}_{i_k}\\right)\\\\ &= \\sum \\mathsf{A}_{i_1\\ldots i_k} v_{1i_1} \\ldots v_{ki_k}, \\end{split} where \\mathsf{A}_{i_1\\ldots i_k} = \\mathsf{A}(\\mathsf{e}_{i_1}, \\ldots, \\mathsf{e}_{i_k}). It follows from the orthonormality of (\\mathsf{e}_i) that \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_k) = v_{1i_1} \\ldots v_{ki_k}. Putting these together, we get \\mathsf{A}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_k) = \\sum A_{i_1\\ldots i_k} \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_k). Since this is true for any choice of vectors \\mathsf{v}_1, \\ldots, \\mathsf{v}_k \\in V , we get the representation of \\mathsf{A} with respect to the orthonormal basis (\\mathsf{e}_i) of V as \\mathsf{A} = \\sum A_{i_1\\ldots i_k} \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}. The constants (A_{i_1\\ldots i_k}) are called the components of \\mathsf{A} with respect to the basis (\\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}) of V . The representation of \\mathsf{A} just derived also informs us that the set of n^k multilinear functions \\{\\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k} \\} spans \\mathcal{T}^k(V) . By studying the actin of \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k} on a set of k basis vectors, it is easy to show that these n^k multilinear functions are also linearly independent. This shows us that the set of n^k multilinear maps \\{\\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k} \\} constitute a basis of \\mathcal{T}(V^k,\\mathbb{R}) , and that \\text{dim}(\\mathcal{T}(V^k,\\mathbb{R})) = (\\text{dim}(V))^k . Example Let us consider the second order tensor \\hat{I}\\in\\mathcal{T}^2(V) defined earlier. Suppose that (\\mathsf{e}_i) is an orthonormal basis of V . The representation of \\hat{\\mathsf{I}} can be computed easily as follows: \\hat{\\mathsf{I}} = \\sum \\hat{I}_{ij} \\mathsf{e}_i \\otimes \\mathsf{e}_j, where \\hat{I}_{ij} = \\hat{\\mathsf{I}}(\\mathsf{e}_i,\\mathsf{e}_j) = \\mathsf{e}_i\\cdot\\mathsf{e}_j = \\delta_{ij}. We thus see that the second order tensor \\hat{\\mathsf{I}} \\in \\mathcal{T}^2(V) has the following representation: \\hat{\\mathsf{I}} = \\sum \\delta_{ij} \\mathsf{e}_i \\otimes \\mathsf{e}_j. Notice how the Kr\u00f6necker delta symbols defined earlier naturally figure as the components of the tensor \\hat{\\mathsf{I}} . It is convenient to represent the components of second order tensor as a matrix. For instance, the components of \\hat{\\mathsf{I}} just computed can be arranged as follows: = \\begin{bmatrix} 1 & 0 & \\ldots & 0\\\\ 0 & 1 & \\ldots & 0\\\\ \\vdots & & \\ddots & \\vdots\\\\ 0 & \\ldots & \\ldots & 1 \\end{bmatrix}. Note that any second order tensor can be represented as a matrix, but such a representation is not possible for higher order tensors. Let us quickly consider the representation of \\mathsf{A} \\in \\mathcal{T}^k(V) with respect to a general basis (\\mathsf{g}_i) of V . As before, the multilinearity of \\mathsf{A} can be used to express it in terms of the basis (\\mathsf{g}_i) as follows: for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_k \\in V , \\begin{split} \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k) &= \\mathsf{A}\\left(\\sum u_{1i_1}\\mathsf{g}_{i_1}, \\ldots, \\sum u_{ki_k}\\mathsf{g}_{i_k}\\right)\\\\ &= \\sum \\mathsf{A}(\\mathsf{g}_{i_1}, \\ldots, \\mathsf{g}_{i_k}) u_{1i_1} \\ldots u_{ki_k},\\\\ &= \\sum A^*_{i_1 \\ldots i_k} (\\mathsf{g}^{i_1} \\otimes \\ldots \\otimes \\mathsf{g}^{i_k})(\\mathsf{u}_1, \\ldots, \\mathsf{u}_k), \\end{split} where A^*_{i_1\\ldots i_k} = \\mathsf{A}(\\mathsf{g}_{i_1}, \\ldots, \\mathsf{g}_{i_k}) . Since this holds for any choice of \\mathsf{u}_1, \\ldots, \\mathsf{u}_k \\in V , it follows that \\mathsf{A} = \\sum A^*_{i_1 \\ldots i_k} \\mathsf{g}^{i_1} \\otimes \\ldots \\otimes \\mathsf{g}^{i_k}. It is straightforward to check that the set of n^k tensors (\\mathsf{g}^{i_1} \\otimes \\ldots \\otimes \\mathsf{g}^{i_k}) form a basis of \\mathcal{T}^k(V) . This also shows that the dimension of \\mathcal{T}^k(\\mathbb{R}^3) is n^k . The coefficients A^*_{i_1\\ldots i_k} are called the c ovariant components of \\mathsf{A} \\in \\mathcal{T}^k(V) with respect to the basis (\\mathsf{g}^{i_1} \\otimes \\ldots \\mathsf{g}^{i_k}) of \\mathcal{T}^k(V) . An alternative means to represent the basis representation of the tensor \\mathsf{A} \\in \\mathcal{T}^k(V) can be obtained using the relations \\mathsf{g}^i = \\sum g^{ij}\\mathsf{g}_j in the basis expansion derived earlier. This yields the following representation: \\mathsf{A} = \\sum A_{i_1\\ldots i_k} \\mathsf{g}_{i_1} \\otimes \\ldots \\otimes \\mathsf{g}_{i_k}, where A_{i_1\\ldots i_k} = \\sum g^{i_1j_1}\\ldots g^{i_kj_k}A^*_{j_1\\ldots j_k}. It is left as an easy exercise to prove that the set of n^k multilinear maps (\\mathsf{g}_{i_1} \\otimes \\ldots \\otimes \\mathsf{g}_{i_k}) also form a basis of \\mathcal{T}^k(V) . The corresponding components of the tensor \\mathsf{A} \\in \\mathcal{T}^k(V) are the constants A_{i_1 \\ldots i_k} , and are called the contravariant components of \\mathsf{A} . Remark It is also possible to defined mixed components of a given tensor by having few of the indices as covariant and the remaining as contravariant, but such generalizations are not considered here in the interest of simplicity.","title":"Basis representation"},{"location":"tensor_algebra/#change-of-basis","text":"Note that the representation of a tensor on V is always with respect to some choice of basis on V . Let us now study how this representation changes when we change the basis of V . As before, let us first consider the simple case of orthonormal bases. Let (\\mathsf{e}_i) and (\\mathsf{f}_i) be two orthonormal bases of V . Let the representation of a k^{\\text{th}} order tensor \\mathsf{A} \\in \\mathcal{T}^k(V) with respect to these bases be given by \\begin{split} \\mathsf{A} &= \\sum A_{i_1\\ldots i_k} \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}, \\quad A_{i_1\\ldots i_k} = \\mathsf{A}(\\mathsf{e}_{i_1}, \\ldots, \\mathsf{e}_{i_k}),\\\\ \\mathsf{A} &= \\sum \\tilde{A}_{i_1\\ldots i_k} \\mathsf{f}_{i_1} \\otimes \\ldots \\otimes \\mathsf{f}_{i_k}, \\quad \\tilde{A}_{i_1\\ldots i_k} = \\mathsf{A}(\\mathsf{f}_{i_1}, \\ldots, \\mathsf{f}_{i_k}). \\end{split} If \\mathsf{f}_i = \\sum Q_{ji}\\mathsf{e}_j , then we see that \\begin{split} \\sum A_{i_1\\ldots i_k} \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k} &= \\sum \\tilde{A}_{i_1\\ldots i_k} \\mathsf{f}_{i_1} \\otimes \\ldots \\otimes \\mathsf{f}_{i_k}\\\\ &= \\sum \\tilde{A}_{i_1\\ldots i_k} \\left(\\sum Q_{j_1i_1}\\mathsf{e}_{j_1}\\right) \\otimes \\ldots \\otimes \\left(\\sum Q_{j_ki_k}\\mathsf{e}_{j_k}\\right)\\\\ &= \\sum Q_{j_1i_1} \\ldots Q_{j_ki_k} \\tilde{A}_{i_1\\ldots i_k} \\mathsf{e}_{j_1} \\otimes \\ldots \\otimes \\mathsf{e}_{j_k}, \\end{split} which shows that \\begin{split} A_{j_1\\ldots j_k} &= \\sum Q_{j_1i_1} \\ldots Q_{j_ki_k} \\tilde{A}_{i_1\\ldots i_k}\\\\ \\Rightarrow \\tilde{A}_{i_1\\ldots i_k} &= \\sum Q^{-1}_{i_1j_1} \\ldots Q^{-1}_{i_kj_k} A_{j_1\\ldots j_k}\\\\ &= \\sum Q_{j_1i_1} \\ldots Q_{j_ki_k} A_{j_1\\ldots j_k}. \\end{split} !!! info \"Remark\" We can equivalently derive this transformation rule as follows \\begin{split} \\tilde{A}_{i_1\\ldots i_k} &= \\mathsf{A}(\\mathsf{f}_{i_1}, \\ldots, \\mathsf{f}_{i_k})\\\\ &= \\mathsf{A}\\left(\\sum Q_{j_1i_1}\\mathsf{e}_{j_1}, \\ldots, \\sum Q_{j_ki_k}\\mathsf{e}_{j_k}\\right)\\\\ &= \\sum Q_{j_1i_1}\\ldots Q_{j_1i_1} A_{j_1 \\ldots j_k}. \\end{split} Notice how the orthonormality of the two bases (\\mathsf{e}_i) and (\\mathsf{f}_i) of V are implicitly used in this derivation. Example Let us revisit the second order tensor \\hat{\\mathsf{I}} \\in \\mathcal{T}^2(V) that we studied earlier. Suppose that (\\mathsf{e}_i) and (\\mathsf{f}_i) are orthonormal bases of V such that \\mathsf{f}_i = \\sum Q_{ji}\\mathsf{e}_j . Let us briefly consider the case when general bases are employed. Suppose that the tensor \\mathsf{A} \\in \\mathcal{T}^k(V) has components A^*_{i_1 \\ldots i_k} and \\tilde{A}^*_{i_1 \\ldots i_k} with respect to bases (\\mathsf{g}^{i_1} \\otimes \\ldots \\otimes \\mathsf{g}^{i_k}) and (\\tilde{\\mathsf{g}}^{i_1} \\otimes \\ldots \\otimes \\tilde{\\mathsf{g}}^{i_k}) , respectively. The relationship between these components is easily computed as follows: \\begin{split} \\tilde{A}^*_{i_1 \\ldots i_k} &= \\mathsf{A}(\\tilde{\\mathsf{g}}_{i_1}, \\ldots, \\tilde{\\mathsf{g}}_{i_k})\\\\ &= \\mathsf{A}\\left(\\sum (\\tilde{\\mathsf{g}}_{i_1}\\cdot\\mathsf{g}^{j_1})\\mathsf{g}_{j_1}, \\ldots, \\sum (\\tilde{\\mathsf{g}}_{i_k}\\cdot\\mathsf{g}^{j_k})\\mathsf{g}_{j_k}\\right)\\\\ &= \\sum (\\tilde{\\mathsf{g}}_{i_1}\\cdot\\mathsf{g}^{j_1})\\ldots(\\tilde{\\mathsf{g}}_{i_k}\\cdot\\mathsf{g}^{j_k}) A^*_{j_1 \\ldots j_k}. \\end{split} The inverse of this relation can also be computed similarly.","title":"Change of basis"},{"location":"tensor_algebra/#contraction","text":"Let V be a finite dimensional inner product space, and let (\\mathsf{g}_i) be a general basis of V . Given a tensor \\mathsf{A} \\in \\mathcal{T}^k(V) of order k , where k \\ge 2 , the (i,j) -contraction of \\mathsf{A} is the tensor \\mathcal{C}_{i,j}\\mathsf{A} \\in \\mathcal{T}^{k-2}(V) of order (k - 2) defined as follows: for any \\mathsf{v}_1, \\ldots, \\mathsf{v}_k \\in V , \\begin{split} & \\mathcal{C}_{i,j}\\mathsf{A}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_{i-1}, \\mathsf{v}_{i+1}, \\ldots, \\mathsf{v}_{j-1},\\mathsf{v}_{j+1}, \\ldots, \\mathsf{v}_k)\\\\ = & \\sum_a \\mathsf{A}(\\mathsf{v}_1, \\ldots, \\mathsf{v}_{i-1}, \\mathsf{g}_a, \\mathsf{v}_{i+1}, \\ldots, \\mathsf{v}_{j-1}, \\mathsf{g}^a, \\mathsf{v}_{j+1}, \\ldots, \\mathsf{v}_k). \\end{split} Remark It can be verified that \\mathsf{g}_a and \\mathsf{g}^a in the definition of the contraction can be swapped: \\sum \\mathsf{A}(\\ldots, \\mathsf{g}_a, \\ldots, \\mathsf{g}^a, \\ldots) = \\sum \\mathsf{A}(\\ldots, \\mathsf{g}^a, \\ldots, \\mathsf{g}_a, \\ldots). Thus, the order in which \\mathsf{g}_a and \\mathsf{g}^a appears is irrelevant. Unlike the various definitions provided earlier, note that this definition seemingly depends on the choice of a basis (\\mathsf{g}_i) . It is left as a simple exercise to verify that if another basis (\\mathsf{f}_i) of V is chosen instead, the contraction operation yields the same tensor \\mathcal{C}_{i,j}\\mathsf{A} \\in \\mathcal{T}^{k-2}(V) as when the bases (\\mathsf{g}_i) are chosen. The contraction operation is thus well-defined . The basis representation of the contracted tensor can be easily computed. With respect to a basis (\\mathsf{g}_i) of V , recall that any \\mathsf{A} \\in \\mathcal{T}^k(V) can be written as \\mathsf{A} = \\sum A_{a_1 \\ldots a_k} \\mathsf{g}_{a_1} \\otimes \\ldots \\otimes \\mathsf{g}_{a_k}. The (i,j) contraction of \\mathsf{A} is easily seen to have the following representation: \\begin{split}\\mathcal{C}_{i,j}\\mathsf{A} &= \\sum g_{a_ia_j}A_{a_1\\ldots a_{i-1} a_i a_{i+1} \\ldots a_{j-1} a_j a_{j+1}\\ldots a_k}\\\\ & \\qquad\\quad\\mathsf{g}_{a_1} \\otimes \\mathsf{g}_{a_{i-1}} \\otimes \\mathsf{g}_{a_{i+1}} \\otimes \\ldots \\otimes \\mathsf{g}_{a_{j-1}}\\otimes\\mathsf{g}_{a_{j+1}}\\otimes \\ldots \\otimes \\mathsf{g}_{a_k}. \\end{split} In the special case of a second order tensor \\mathsf{B} \\in \\mathcal{T}^2(V) , there is only one possible contraction, \\mathcal{C}_{1,2}\\mathsf{B} \\in \\mathbb{R} , which is a tensor of order 0 , or just a real number. The contraction operation, in this case, is called the trace . It is customary to write the trace of \\mathsf{B} as \\text{tr}(\\mathsf{B}) . With respect to a basis (\\mathsf{g}_i) of V , \\mathsf{B} = \\sum B_{ij} \\, \\mathsf{g}_i \\otimes \\mathsf{g}_j \\quad\\Rightarrow\\quad \\text{tr}(\\mathsf{B}) = \\sum g_{ab}B_{ab}. It can be easily checked that \\mathsf{tr}(\\mathsf{B}) is independent of the choice of basis. Note that in the special case when (\\mathsf{g}_i) is an orthonormal basis of V , the expression for the trace of \\mathsf{B} reduces to the familiar form \\text{tr}(\\mathsf{B}) = \\sum B_{aa} . An alternative definition of the trace of a linear map will be provided in a later section.","title":"Contraction"},{"location":"tensor_algebra/#generalized-dot-product-of-tensors","text":"Note! This terminology is not standard, but it is adequate for the purposes of this course. Certain special operations called generalized dot products , or simply dot products , are now introduced between tensors of different orders. These are introduced on account of their prevalence in the continuum mechanics literature. Throughout this section, V denotes a finite dimensional inner product space. To motivate the definition of the generalized dot product of tensors on V , it is helpful to first consider a few important special cases. In the simplest case, given two vectors \\mathsf{u}, \\mathsf{v} \\in V , note that the inner product of these two vectors can be expressed in terms of the contraction operation as follows: \\mathsf{u} \\cdot \\mathsf{v} = \\mathcal{C}_{1,2}(\\mathsf{u} \\otimes \\mathsf{v}). This restatement of the inner product in terms of the contraction operation will serve as the starting point for its generalization to the dot product of two arbitrary tensors. Suppose that \\mathsf{u}, \\mathsf{v}, \\mathsf{w} \\in V are any three vectors in V . Then the vector (\\mathsf{u} \\otimes \\mathsf{v}) \\cdot \\mathsf{w} \\in V is defined as follows: (\\mathsf{u} \\otimes \\mathsf{v}) \\cdot \\mathsf{w} = \\mathcal{C}_{2,3}(\\mathsf{u} \\otimes \\mathsf{v} \\otimes \\mathsf{w}). To understand what this means, consider the action of the vector \\mathcal{C}_{2,3}(\\mathsf{u} \\otimes \\mathsf{v} \\otimes \\mathsf{w}) on an arbitrary vector \\mathsf{z} \\in V : if (\\mathsf{g}_i) is a general basis of V , then \\begin{split} \\mathcal{C}_{2,3}(\\mathsf{u} \\otimes \\mathsf{v} \\otimes \\mathsf{w})(\\mathsf{z}) &= \\sum (\\mathsf{u} \\otimes \\mathsf{v} \\otimes \\mathsf{w})(\\mathsf{z}, \\mathsf{g}_a, \\mathsf{g}^a)\\\\ &= \\sum u_i v_j w_k (\\mathsf{g}_i \\otimes \\mathsf{g}_j \\otimes \\mathsf{g}_k)\\left(\\sum z_b \\mathsf{g}_b, \\mathsf{g}_a, \\mathsf{g}^a\\right)\\\\ &= \\sum g_{ib} u_i v_j w_k z_b g_{ja} \\delta_{ka}\\\\ &= \\sum g_{jk} v_j w_k \\, \\sum g_{ia} u_a z_a\\\\ &= (\\mathsf{v} \\cdot \\mathsf{w})(\\mathsf{u} \\cdot \\mathsf{z})\\\\ &= (\\mathsf{v} \\cdot \\mathsf{w})\\mathsf{u}(\\mathsf{z}). \\end{split} Since this is true for any \\mathsf{z} \\in V , it follows that (\\mathsf{u} \\otimes \\mathsf{v}) \\cdot \\mathsf{w} = (\\mathsf{v} \\cdot \\mathsf{w}) \\mathsf{u}. This result also provides a means to compute the dot product \\mathsf{A} \\cdot \\mathsf{v} of a second order tensor \\mathsf{A} \\in \\mathcal{T}^2(V) and a vector \\mathsf{v} \\in V : if (\\mathsf{g}_i) is any basis of V , then \\begin{split} \\mathsf{A} \\cdot \\mathsf{v} &= \\left(\\sum A_{ij} \\mathsf{g}_i \\otimes \\mathsf{g}_j\\right)\\cdot\\left(\\sum v_k \\mathsf{g}_k\\right)\\\\ &= \\sum A_{ij} v_k g_{jk} \\mathsf{g}_i \\in V. \\end{split} Note that \\tilde{\\mathsf{A}} \\in L(V,V) denote the linear map corresponding to the second order tensor \\mathsf{A} \\in \\mathcal{T}^2(V) , then, for any \\mathsf{v} \\in V , \\mathsf{A} \\cdot \\mathsf{v} = \\tilde{\\mathsf{A}}\\mathsf{v} . This is most easily seen by representing both sides this equation with respect to an orthonormal basis of V . This shows that the dot product defined here is consistent with the theory of linear maps developed earlier. Remark Note that given an arbitrary second order tensor \\mathsf{A} \\in \\mathcal{T}^2(V) and an arbitrary vector \\mathsf{v} \\in V , \\mathsf{A} \\cdot \\mathsf{v} \\in V and \\mathsf{v} \\cdot \\mathsf{A} \\in V are, in general, different vectors. The generalized dot product of tensors is thus not necessarily symmetric. As an extension of the foregoing ideas, we now define dot products for tensors of second order. Given vectors \\mathsf{u}, \\mathsf{v}, \\mathsf{w}, \\mathsf{z} \\in V , the dot product (\\mathsf{u} \\otimes \\mathsf{v}) \\cdot (\\mathsf{w} \\otimes \\mathsf{z}) \\in \\mathbb{R} between the second order tensors \\mathsf{u} \\otimes \\mathsf{v} \\in \\mathcal{T}^2(V) and \\mathsf{w} \\otimes \\mathsf{z} \\in \\mathcal{T}^2(V) is defined as follows: (\\mathsf{u} \\otimes \\mathsf{v}) \\cdot (\\mathsf{w} \\otimes \\mathsf{z}) = \\mathcal{C}_{1,2}\\left(\\mathcal{C}_{1,3}\\left(\\mathsf{u} \\otimes \\mathsf{v} \\otimes \\mathsf{w} \\otimes \\mathsf{z}\\right)\\right) = (\\mathsf{u} \\cdot \\mathsf{w})(\\mathsf{v} \\cdot \\mathsf{z}). The simplest means to understand this by considering the dot products of two second order tensors \\mathsf{A}, \\mathsf{B} \\in \\mathcal{T}^2(V) . If (\\mathsf{g}_i) denotes an orthonormal basis of V , and \\mathsf{A} = \\sum A_{ij} \\mathsf{g}_i \\otimes \\mathsf{g}_j and \\mathsf{B} = \\sum B_{ij} \\mathsf{g}_i \\otimes \\mathsf{g}_j are the representations of \\mathsf{A} and \\mathsf{B} , respectively, with respect to this basis, then note that \\mathsf{A} \\cdot \\mathsf{B} = \\sum A_{ij} B_{ij}. Remark It is important to note that different authors follow different conventions regarding this. For instance, it common in the Continuum Mechanics literature to use the following notations: for any two second order tensors \\mathsf{A},\\mathsf{B} \\in \\mathcal{T}^2(V) , \\begin{split} \\mathsf{A} \\cdot \\mathsf{B} &= \\sum A_{ij}B_{ji},\\\\ \\mathsf{A} : \\mathsf{B} &= \\sum A_{ij}B_{ij}, \\end{split} where (A_{ij}) and (B_{ij}) are the components of \\mathsf{A} and \\mathsf{B} , respectively, with respect to some orthonormal basis of V . Note that the dot product \\mathsf{A} \\cdot \\mathsf{B} , according to our definition, is \\mathsf{A} : \\mathsf{B} according to this definition. Care must be therefore exercised when reading the literature to understand the appropriate meaning of a quantity like \\mathsf{A} \\cdot \\mathsf{B} . The reason for defining the generalized dot product the way we have done is to ensure a uniform and simple notation for many differential and integral identities that we will encounter later on. It is important, however, to keep in mind that this is essentially a matter of convention. As a final and useful example, note that if \\mathsf{A} \\in \\mathcal{T}^k(V) is a tensor of order k , where k > 2 , and \\mathsf{B} \\in \\mathcal{T}^2(V) is a second order tensor, then, with respect to an orthonormal basis (\\mathsf{g}_i) of V , \\mathsf{A} \\cdot \\mathsf{B} = \\sum A_{i_1 \\ldots i_{k-2} a b} B_{ab}. The extension of this notion of dot products can be similarly extended to tensors of higher orders. Example Suppose that \\mathsf{A}, \\mathsf{B} \\in \\mathcal{T}^2(V) are second order tensors on V . Then, it is true that \\mathsf{A} \\cdot \\mathsf{B} = \\mathsf{B} \\cdot \\mathsf{A}. This is most easily seen with the help of the basis representation with respect to an orthonormal basis of V : in this case, \\mathsf{A} \\cdot \\mathsf{B} = \\sum A_{ij} B_{ij} = \\sum B_{ij} A_{ij} = \\mathsf{B} \\cdot \\mathsf{A}, thereby establishing the claim. Example Suppose that \\mathsf{A} \\in \\mathcal{T}^2(V) is a second order tensor on V and \\mathsf{u},\\mathsf{v} \\in V are two vectors in V , then \\mathsf{A} \\cdot (\\mathsf{u} \\otimes \\mathsf{v}) = \\mathsf{u} \\cdot \\mathsf{A}\\mathsf{v}. This is also easily established by choosing an orthonormal (\\mathsf{g}_i) basis of $V$. The component form of \\mathsf{A} \\cdot (\\mathsf{u} \\otimes \\mathsf{v}) then reads \\begin{split}\\mathsf{A} \\cdot (\\mathsf{u} \\otimes \\mathsf{v}) &= \\sum A_{ij} u_i v_j\\\\ &= \\left(\\sum u_k \\mathsf{g}_k\\right) \\cdot \\left(\\sum A_{ij} v_j \\mathsf{g}_i\\right)\\\\ &= \\mathsf{u} \\cdot \\mathsf{A}\\mathsf{v}.\\end{split} It can be shown similarly that (\\mathsf{u} \\otimes \\mathsf{v})\\cdot\\mathsf{A} = \\mathsf{A}^T\\mathsf{u} \\cdot \\mathsf{v} .","title":"Generalized dot product of tensors"},{"location":"tensor_algebra/#volume-forms","text":"Let V be an inner product space of dimension n . A tensor \\mathsf{A} \\in \\mathcal{T}^k(V) is said to be symmetric if, for any \\mathsf{u}_1, \\ldots, \\mathsf{u}_k \\in V , \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_i, \\ldots, \\mathsf{u}_j, \\ldots, \\mathsf{u}_n) = \\mathsf{A}(\\mathsf{u}_1, \\ldots, \\mathsf{u}_j, \\ldots, \\mathsf{u}_i, \\ldots, \\mathsf{u}_n), where 1 \\le i < j \\le n . If the sign reverses when any two arguments are interchanged, then the tensor is said to be skew-symmetric . Remark It turns out that the set of all antisymmetric tensors of a given order have a rich algebraic structure, called exterior algebra . A volume form on V is a skew-symmetric tensor \\mathsf{\\epsilon}:\\times^n V \\to \\mathbb{R} of order n . It is customary to denote the set of all volume forms on V using the notation \\Omega^n(V) : \\Omega^n(V) = \\{ \\mathsf\\epsilon \\in \\mathcal{T}^n(V) \\,|\\, \\mathsf\\epsilon \\text{ is skew-symmetric}\\}. It is important to note that the trivial volume form \\mathsf{0} \\in \\mathcal{T}^n(V) that maps every set of n vectors in V to zero is excluded from this discussion. A volume form that is not trivial is said to be non-trivial . All volume forms considered here are assumed to be non-trivial. Given two volume forms \\mathsf{\\epsilon}, \\mathsf{\\omega} \\in \\Omega^n(V) , it can be shown that there exists a scalar a \\in \\mathbb{R} such that \\mathsf\\omega = a\\mathsf{\\epsilon} . An equivalent way of stating this is that the set of all antisymmetric tensors of order n over an n -dimensional vector space is a vector space of dimension 1 : \\text{dim}(\\Omega^n(V)) = 1 . Picking a particular volume form \\mathsf{\\epsilon} \\in \\Omega^n(V) , the set of all volume forms \\mathsf{\\omega} \\in \\Omega^n(V) such that \\mathsf\\omega = a\\mathsf{\\epsilon} for some a > 0 are said to constitute an orientation of V . Given this orientation on V , a basis (\\mathsf{g}_i) of V is said to be right-handed if \\epsilon(\\mathsf{g}_1, \\ldots, \\mathsf{g}_n) > 0 , and left-handed otherwise. Remark It is emphasized that the choice of orientation is arbitrary. Typically, a special volume form that has mathematical or physical relevance is chosen and the positive orientation is defined with respect to this choice. With a particular choice of orientation, it is customary to focus only on only right-handed bases. Rather than discussing the general theory of volume forms, it is useful to look at volume forms on the three dimensional Euclidean space \\mathbb{R}^3 . The standard volume form on \\mathbb{R}^3 , written \\mathsf{\\epsilon} \\in \\Omega^3(\\mathbb{R}^3) , sometimes called the Levi-Civita tensor , is defined as follows: if (\\mathsf{e}_i) denotes the standard basis of \\mathbb{R}^3 , \\mathsf{\\epsilon} = \\sum \\epsilon_{ijk} \\mathsf{e}_i \\otimes \\mathsf{e}_j \\otimes \\mathsf{e}_k, where \\epsilon_{ijk} is the Levi-Civita symbol introduced earlier in the context of the cross product. Note that \\mathsf\\epsilon(\\mathsf{e}_1, \\mathsf{e}_2, \\mathsf{e}_3) = \\epsilon_{123} = 1 - the standard basis of \\mathbb{R}^3 is thus right handed with respect to the standard volume form of \\mathbb{R}^3 .","title":"Volume forms"},{"location":"tensor_algebra/#cross-product-in-mathbbr3","text":"The three dimensional Euclidean space \\mathbb{R}^3 admits another special algebraic operation. Given any two vectors \\mathsf{u} = \\sum u_i \\mathsf{e}_i \\in \\mathbb{R}^3 and \\mathsf{v} = \\sum v_i \\mathsf{e}_i \\in \\mathbb{R}^3 , where (\\mathsf{e}_i) is the standard basis of \\mathbb{R}^3 , their cross product is defined as the vector \\mathsf{u} \\times \\mathsf{v} \\in \\mathbb{R}^3 given by \\mathsf{u} \\times \\mathsf{v} = (u_2 v_3 - u_3 v_2)\\mathsf{e}_1 + (u_3 v_1 - u_1 v_3)\\mathsf{e}_2 + (u_1 v_2 - u_2 v_1)\\mathsf{e}_3. This can be written compactly by introducing the Levi-Civita symbol \\epsilon_{ijk} , \\epsilon_{ijk} = \\begin{cases} 1, & \\{i,j,k\\}\\text{ is an even permutation of $\\{1,2,3\\}$},\\\\ -1, & \\{i,j,k\\}\\text{ is an odd permutation of $\\{1,2,3\\}$},\\\\ 0, & \\text{otherwise}, \\end{cases} as follows: \\mathsf{u} \\times \\mathsf{v} = \\sum \\epsilon_{ijk} u_j v_k \\mathsf{e}_i, as can be easily checked. The reciprocal basis can be computed easily in the special case of the three dimensional Euclidean space \\mathbb{R}^3 using the cross product. Given any basis (\\mathsf{g}_i) of \\mathbb{R}^3 , the corresponding reciprocal basis (\\mathsf{g}^i) of \\mathbb{R}^3 can be computed as \\mathsf{g}^1 = \\frac{\\mathsf{g}_2 \\times \\mathsf{g}_3}{\\mathsf{g}_1 \\cdot \\mathsf{g}_2 \\times \\mathsf{g}_3}, \\quad \\mathsf{g}^2 = \\frac{\\mathsf{g}_3 \\times \\mathsf{g}_1}{\\mathsf{g}_1 \\cdot \\mathsf{g}_2 \\times \\mathsf{g}_3}, \\quad \\mathsf{g}^3 = \\frac{\\mathsf{g}_1 \\times \\mathsf{g}_2}{\\mathsf{g}_1 \\cdot \\mathsf{g}_2 \\times \\mathsf{g}_3}. It is a simple exercise to check these formulae satisfy the defining condition of the reciprocal basis: \\mathsf{g}^i \\cdot \\mathsf{g}_j = \\delta_{ij} . An alternative definition of the cross product of two vectors in \\mathbb{R}^3 can be given in terms of the standard volume form. Given vectors \\mathsf{v}, \\mathsf{w} \\in \\mathbb{R}^3 , their cross product \\mathsf{v} \\times \\mathsf{w} \\in \\mathbb{R}^3 is defined as follows: for any \\mathsf{u} \\in \\mathbb{R}^3 , \\mathsf{u} \\cdot (\\mathsf{v} \\times \\mathsf{w}) = \\mathsf\\epsilon(\\mathsf{u}, \\mathsf{v}, \\mathsf{w}). It is left as an exercise to verify this. ( Hint. Choose \\mathsf{u} to be the standard basis \\mathsf{e}_i . This immediately yields: (\\mathsf{v}\\times\\mathsf{w})_i = \\sum \\epsilon_{ijk} v_j w_k .)","title":"Cross product in $\\mathbb{R}^3$"},{"location":"tensor_analysis_R3/","text":"The key ideas of tensor calculus on \\mathbb{R}^3 are now outlined. For the sake of conceptual clarity, the presentation is first carried out in the Cartesian coordinate setting. Finally, orthogonal curvilinear coordinate systems are briefly discussed. Coordinate systems A coordinate system on \\mathbb{R}^3 is an open subset U \\subseteq \\mathbb{R}^3 , together with a smooth injective map \\mathsf\\phi:\\mathbb{R}^3 \\to \\mathbb{R}^3 such that \\mathsf\\phi^{-1}:\\mathsf\\phi(U) \\to U is also smooth; such a map is called a diffeomorphism .. The map \\mathsf\\phi is called a coordinate chart and the open set U is called a coordinate patch on \\mathbb{R}^3 . Given any \\mathsf{x} \\in U , the triple of real numbers \\mathsf\\phi(\\mathsf{x}) \\in \\mathbb{R}^3 are called the curvilinear coordinates of \\mathsf{x} . What is special about \\mathbb{R}^3 (, and this is true for \\mathbb{R}^n too,) is that it admits a global coordinate system (\\mathbb{R}^3, \\mathsf\\iota) , also called the Cartesian coordinate system , where \\mathsf\\iota:\\mathbb{R}^3 \\to \\mathbb{R}^3 is the identity map defined as follows: for any \\mathsf{x} \\in \\mathbb{R}^3 , \\mathsf\\iota(\\mathsf{x}) = \\mathsf{x} . The coordinates of \\mathsf{x} with respect to this global coordinate system are called the Cartesian coordinates of \\mathsf{x} . Remark A few terminologies inspired by the more general case of a manifold are introduced here. A coordinate system (U,\\mathsf\\phi) on \\mathbb{R}^3 is said to be compatible with the Cartesian coordinate system on \\mathbb{R}^3 if the maps \\mathsf\\phi:U \\to \\mathsf\\phi(U) and \\mathsf\\phi^{-1}:\\mathsf\\phi(U) \\to U are smooth and invertible. Note that all coordinate systems considered hereafter are considered to be compatible with the Cartesian coordinate system on \\mathbb{R}^3 . An atlas on \\mathbb{R}^3 is a collection of compatible coordinate systems ((U_\\alpha,\\mathsf\\phi_\\alpha))_{\\alpha \\in A} , where A is some index set, such that \\cup_{\\alpha \\in A} U_\\alpha = \\mathbb{R}^3 . Note that adding a compatible coordinate system to an atlas yields a larger atlas. Consider now the Cartesian coordinate system on \\mathbb{R}^3 and construct the maximal atlas compatible with it; this amounts to adding every compatible coordinate system to the atlas consisting of the global Cartesian coordinate system. This maximal atlas is called the standard differentiable structure on \\mathbb{R}^3 . Remark The fact that \\mathbb{R}^3 admits a global coordinate system significantly simplifies the development of calculus on \\mathbb{R}^3 . There, however, exist many sets of practical importance that cannot be covered using a global coordinate system. The modern theory of differentiable manifolds was developed precisely to address these issues, and develop an appropriate extension of the tools of calculus to these sets. Suppose that (U,\\mathsf\\phi) is a coordinate system on \\mathbb{R}^3 that is compatible with the Cartesian coordinate system on \\mathbb{R}^3 . The curvilinear coordinates of \\mathsf{x} \\in U with respect to the coordinate system (U,\\mathsf\\phi) is given by the triple of real numbers \\mathsf{y} \\in \\mathbb{R}^3 , where \\mathsf{y} = \\mathsf\\phi(\\mathsf{x}). The inverse map \\mathsf\\phi^{-1}:\\mathsf\\phi(U) \\to U permits the computation of \\mathsf{x} given \\mathsf{y} : \\mathsf{x} = \\mathsf\\phi^{-1}(\\mathsf{y}). A useful simplified notation that will be adopted in the following development is the following: the foregoing relations will often be written as \\mathsf{y} = \\mathsf{y}(\\mathsf{x}), \\qquad \\mathsf{x} = \\mathsf{x}(\\mathsf{y}). Notice that this is a deliberate abuse of notation: \\mathsf{x} stands for both a point in U and a function that takes a point in \\mathsf\\phi(U) and returns a real number. Despite the ambiguity this introduces, this notation is often adopted in practice since it significantly simplifies the appearance of many equations. The ambiguity will not cause any problems as long as the appropriate meaning of the symbols are inferred from the context. To quantify precisely the notion that the coordinate system (U,\\mathsf\\phi) is compatible with the Cartesian coordinate system, note that the map \\mathsf\\phi:U \\to \\mathbb{R}^3 is smooth, and hence, its Fr\u00e9chet derivative exists at each \\mathsf{x} \\in U . This implies, in particular that the determinant of the Jacobian matrix is non-zero: \\text{det} \\begin{bmatrix} \\partial_1 y_1(\\mathsf{x}) & \\partial_2 y_1(\\mathsf{x}) & \\partial_3 y_1(\\mathsf{x})\\\\ \\partial_1 y_2(\\mathsf{x}) & \\partial_2 y_2(\\mathsf{x}) & \\partial_3 y_2(\\mathsf{x})\\\\ \\partial_1 y_3(\\mathsf{x}) & \\partial_2 y_3(\\mathsf{x}) & \\partial_3 y_3(\\mathsf{x})\\end{bmatrix} \\neq 0. Here, \\mathsf{y}(\\mathsf{x}) = (y_1(\\mathsf{x}), y_2(\\mathsf{x}), y_3(\\mathsf{x})) . A similar condition holds for the inverse map \\mathsf{x}:\\mathsf\\phi(U) \\to \\mathbb{R}^3 . Tangent spaces A concept that will prove to be very useful later on is that of a tangent space . Rather than defining this precisely, it suffices for the purposes of this course to present a qualitative definition that provides some intuition about what tangent spaces are. Let U \\subseteq \\mathbb{R}^3 be an open subset of \\mathbb{R}^3 . At each \\mathsf{x} \\in U , consider the set T_{\\mathsf{x}}U defined as follows: T_{\\mathsf{x}}U = \\{\\mathsf{v} \\in \\mathbb{R}^3 \\,|\\, \\text{there exists a (smooth) curve passing through $\\mathsf{x}$ with tangent $\\mathsf{v}$ at $\\mathsf{x}$}\\}. This set is called the tangent space at \\mathsf{x} to \\mathbb{R}^3 . The point \\mathsf{x} is called the base point of the tangent space. In the special case of \\mathbb{R}^3 , given any \\mathsf{v} \\in \\mathbb{R}^3 , the straight line \\{\\mathsf{x} + t\\mathsf{v}\\,|\\,t \\in \\mathbb{R}\\} is a smooth curve that passes through \\mathsf{x} with tangent \\mathsf{v} at \\mathsf{x} . This shows that T_{\\mathsf{x}}\\mathbb{R}^3 \\cong \\mathbb{R}^3 , and is hence a linear space. Thus the tangent space at any point in \\mathsf{x} \\in \\mathbb{R}^3 is a local copy of the whole of \\mathbb{R}^3 attached to \\mathsf{x} , and hence need not be distinguished from \\mathbb{R}^3 . For conceptual clarity, however, the tangent space will be explicitly indicated whenever appropriate. Further, each tangent space is assumed to be an inner product space, with the inner product being identical to the standard inner product on \\mathbb{R}^3 . The union of all tangent spaces to U is called the tangent bundle of U , and is written as TU : TU = \\cup_{\\mathsf{x} \\in U} T_{\\mathsf{x}}U. Notice that this is a disjoint union. Remark For the special case of U \\subseteq \\mathbb{R}^3 , where U is open, the tangent space T_{\\mathsf{x}}U to U at \\mathsf{x} \\in U can be equivalently characterized as follows: T_{\\mathsf{x}}U = \\{(\\mathsf{x},\\mathsf{v}) \\,|\\, \\mathsf{v} \\in \\mathbb{R}^3\\}. The set T_{\\mathsf{x}}U defined above acquires a linear structure with addition +:T_{\\mathsf{x}}U \\times T_{\\mathsf{x}} \\to T_{\\mathsf{x}}U and scalar multiplication \\cdot:\\mathbb{R} \\times T_{\\mathsf{x}} \\to T_{\\mathsf{x}}U defined as follows: for any \\mathsf{v}, \\mathsf{w} \\in \\mathbb{R}^3 and a \\in \\mathbb{R} , (\\mathsf{x},\\mathsf{v}) + (\\mathsf{x},\\mathsf{w}) = (\\mathsf{x}, \\mathsf{v} + \\mathsf{w}), \\qquad a(\\mathsf{x}, \\mathsf{v}) = (\\mathsf{x}, a\\mathsf{v}). Notice now this definition does not affect the base point \\mathsf{x} : this is necessary to ensure that vector addition and scalar multiplication satisfy the closure property. With this definition of the tangent space, it is easy to check that the tangent bundle is given by TU = U \\times \\mathbb{R}^3 . It follows from the the foregoing discussion that each T_{\\mathsf{x}}U can be thought as a local copy of \\mathbb{R}^3 . Thus, the tangent bundle TU can be envisaged as a copy of \\mathbb{R}^3 attached to each point \\mathsf{x} \\in U . Each of the tangent spaces T_{\\mathsf{x}}U , where \\mathsf{x} \\in U , can be equipped with a basis of \\mathbb{R}^3 that could, in general, vary as \\mathsf{x} varies over U . The simplest choice is to use the same standard basis (\\mathsf{e}_i) of \\mathbb{R}^3 as the basis for every tangent space T_{\\mathsf{x}}U . With this choice, any \\mathsf{h} \\in T_{\\mathsf{x}}\\mathbb{R}^3 admits a representation of the form \\mathsf{h} = \\sum h_i \\mathsf{e}_i . Note that other choices of bases are possible for each tangent space T_{\\mathsf{x}}\\mathbb{R}^3 ; this will in fact be the case when curvilinear coordinate systems are considered in a later section. Tensor fields Having defined the basic concepts of coordinate systems and tangent spaces, the stage is set to introduce the most important quantities from the point of view of continuum mechanics: tensor fields . In a loose sense, the term field is used here in the sense of a quantity that varies smoothly over a region of \\mathbb{R}^3 . The basic idea behind a tensor field is to associate a tensor to every point in some region of \\mathbb{R}^3 , and to do so in a manner that is smooth . These ideas are developed precisely in this section. Scalar fields, which are tensor fields of order 0 are discussed first, followed by vector fields, which are tensor fields of order 1 , and finally tensor fields of order k are discussed. Throughout this section U \\subseteq \\mathbb{R}^3 denotes an open subset of \\mathbb{R}^3 . For the coordinate representation of various fields, the global Cartesian coordinate system is used for \\mathbb{R}^3 and the standard basis of \\mathbb{R}^3 is used for each T_{\\mathsf{x}}U for every \\mathsf{x} \\in U . Remark The fact that \\mathbb{R}^3 has a linear structure will turn out be very convenient for the ensuing development and simplify many of the calculations. Some of the terminology associated with the more general case will however be used when appropriate. A scalar field is a function of the form f:U\\subseteq\\mathbb{R}^3 \\to \\mathbb{R} . Thus, the scalar field f associates with each \\mathsf{x} \\in U the real number f(\\mathsf{x}) . Note that the continuity, differentiability and smoothness of f can be defined according to the definitions given in the previous section in the context of nonlinear maps between vector spaces. A vector field on U \\subseteq \\mathbb{R}^3 is a map of the form \\mathsf{v}:U \\to TU such that \\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U for every \\mathsf{x} \\in U . The vector field \\mathsf{v} can be expressed with respect to the Cartesian coordinate system as follows: \\mathsf{v}(\\mathsf{x}) = \\sum v_i(\\mathsf{x}) \\mathsf{e}_i, where each v_i:U \\to \\mathbb{R} is a scalar field. The continuity/differentiability/smoothness of the vector field \\mathsf{v} is defined in terms of the corresponding property of the scalar fields v_i . A tensor field of order k on U \\subseteq \\mathbb{R}^3 is defined similarly as a map of the form \\mathsf{A}:U \\to \\otimes^k \\, TU, such that \\mathsf{A}(\\mathsf{x}) \\in \\otimes^k \\, T_{\\mathsf{x}}U for every \\mathsf{x} \\in U . In writing these expressions, the following notations have been introduced: \\begin{split} \\otimes^k T_{\\mathsf{x}}U &= \\{ \\mathsf{T}:\\times^k \\, T_{\\mathsf{x}}U \\to \\mathbb{R} \\,|\\, \\mathsf{T}\\text{ is multilinear }\\} = \\mathcal{T}^k(T_{\\mathsf{x}}U),\\\\ \\otimes^k TU &= \\cup_{\\mathsf{x} \\in U} \\left( \\otimes^k T_{\\mathsf{x}}U \\right) = \\cup_{\\mathsf{x} \\in U} \\mathcal{T}^k(T_{\\mathsf{x}}U). \\end{split} Thus, a tensor field of order k associates with each point in U a tensor of order k over the tangent space at that point. In terms of the Cartesian coordinate system, it is evident that \\mathsf{A}(\\mathsf{x}) = \\sum A_{i_1\\ldots i_k}(\\mathsf{x}) \\, \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}, where each A_{i_1 \\ldots i_k}:U \\to \\mathbb{R} is a scalar field. As in the case of vector fields, the continuity/differentiability/smoothness of tensor fields are naturally defined in terms of the corresponding properties of their component scalar fields. Remark Note that a tensor field of order 0 is a scalar field since, for any \\mathsf{x} \\in U , \\mathcal{T}^0(T_{\\mathsf{x}}U) \\cong \\mathbb{R} . Similarly, a tensor field of order 1 is a vector field since, for any \\mathsf{x} \\in U , \\mathcal{T}^1(T_{\\mathsf{x}}U) \\cong T_{\\mathsf{x}}U . Notice that this is true since each tangent space T_{\\mathsf{x}}U is a finite dimensional inner product space. Thus, it suffices to study tensor fields of order k on U , where k \\ge 0 , since this includes scalar and vector fields too. For pedagogical reasons, however, scalar, vector and tensor fields will be considered successively in these notes. Covariant derivative and Gradient How does a tensor field vary across its domain of definition, and, in particular, how can this change be quantified precisely? This is the fundamental question that is addressed in this and the next few subsections. The fundamental quantity, towards this end, is what is called the covariant derivative of the tensor field. This is a generalization of the familiar notion of the directional derivative encountered earlier. Let f:U \\subseteq \\mathbb{R}^3 \\to \\mathbb{R} , where U is an open subset of \\mathbb{R}^3 be a smooth scalar field on U . The directional derivative , also called the covariant derivative , of f at \\mathsf{x} \\in U along any \\mathsf{v} \\in T_{\\mathsf{x}}U is defined as the scalar \\nabla_{\\mathsf{v}}f(\\mathsf{x}) \\in \\mathbb{R} such that \\nabla_{\\mathsf{v}}f(\\mathsf{x}) = \\frac{d}{dt}\\bigg\\vert_{t=0} f(\\mathsf{x} + t\\mathsf{v}). Note that in the term f(\\mathsf{x} + t\\mathsf{v}) in the definition above, \\mathsf{x} \\in U and \\mathsf{v} \\in T_{\\mathsf{x}}U . Since U and T_{\\mathsf{x}}U are not the same spaces, the quantity \\mathsf{x} + t\\mathsf{v} is ill-defined, strictly speaking. The saving grace, in this occasion, is the fact that U is an open subset of \\mathbb{R}^3 , which as a linear structure, and since T_{\\mathsf{x}}U \\cong \\mathbb{R}^3 . Thus, \\mathsf{x} + t\\mathsf{v} is to be understood as the point in \\mathbb{R}^3 whose coordinates are obtained by summing the triples of real numbers \\mathsf{x} and t\\mathsf{v} . Note that this point lies inside U when t is sufficiently small, as noted earlier. Remark In the more general case when U is not a linear set, the foregoing definition breaks down. The modern theory of differentiable manifolds provides a more rigorous definition of the covariant derivative in more general cases where the argument just given breaks down by introducing an additional structure called a connection on a manifold. The gradient of f is defined as the vector field \\nabla f:U \\to TU such that, at every \\mathsf{x} \\in U , \\nabla f(\\mathsf{x}) \\cdot \\mathsf{v} = \\nabla_{\\mathsf{v}}f(\\mathsf{x}), for any \\mathsf{v} \\in T_{\\mathsf{x}}U . Note that \\nabla f(\\mathsf{x}) \\in T_{\\mathsf{x}}U by definition. Since \\mathsf{v} \\in T_{\\mathsf{x}}U , the dot product \\nabla f(\\mathsf{x}) \\cdot \\mathsf{v} is indeed well-defined. Remark In this course, the alternative notation \\text{grad }f(\\mathsf{x}) will often be used to indicate \\nabla f(\\mathsf{x}) . The directional derivative of f at \\mathsf{x} \\in U along \\mathsf{v} \\in T_{\\mathsf{x}}U can be written in terms of the Cartesian coordinate system on \\mathbb{R}^3 as follows: \\begin{split} \\nabla_{\\mathsf{v}}f(\\mathsf{x}) &= \\frac{d}{dt}\\bigg\\vert_{t=0} f(x_1 + tv_1, x_2 + tv_2, x_3 + tv_3)\\\\ &= \\sum \\partial_i f(\\mathsf{x}) v_i. \\end{split} Noting that \\sum \\partial_i f(\\mathsf{x})v_i = \\left(\\sum \\partial_i f(\\mathsf{x}) \\, \\mathsf{e}_i\\right)\\cdot\\left(\\sum v_j \\mathsf{e}_j\\right), it follows that the Cartesian coordinate representation of the gradient of f is given by \\nabla f(\\mathsf{x}) = \\sum \\partial_i f(\\mathsf{x})\\,\\mathsf{e}_i . The covariant derivative and gradient of a smooth vector field \\mathsf{v}:U \\to TU are defined similarly. The covariant derivative of \\mathsf{v} at \\mathsf{x} \\in U along \\mathsf{w} \\in T_{\\mathsf{x}}U is defined as the vector \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U such that \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) = \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{v}(\\mathsf{x} + t\\mathsf{w}). Note that \\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U , whereas \\mathsf{v}(\\mathsf{x} + t\\mathsf{w}) \\in T_{\\mathsf{x} + t\\mathsf{w}}U . Strictly speaking, the vectors \\mathsf{v}(\\mathsf{x}) and \\mathsf{v}(\\mathsf{x} + t\\mathsf{w}) cannot be compared since they belong to different vector spaces. However, the fact that both T_{\\mathsf{x}}U and T_{\\mathsf{x} + t\\mathsf{w}}U are isomorphic to \\mathbb{R}^3 comes to the rescue again. What is actually compared are the corresponding images of these vectors in \\mathbb{R}^3 , which is a valid algebraic operation. !!! info \"Remark\" Using the characterization of \\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U as the pair (\\mathsf{x}, \\mathsf{v}(\\mathsf{x})) , the derivative in the definition of the covariant derivative of \\mathsf{v} can be understood in the following sense: \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) = \\lim_{t \\to 0} \\frac{(\\mathsf{x},\\mathsf{v}(\\mathsf{x} + t\\mathsf{w})) - (\\mathsf{x}, \\mathsf{v}(\\mathsf{x}))}{t} = \\left(\\mathsf{x}, \\lim_{t \\to 0} \\frac{\\mathsf{v}(\\mathsf{x} + t\\mathsf{w}) - \\mathsf{v}(\\mathsf{x})}{t}\\right) \\in T_{\\mathsf{x}}U. Notice how the vector \\mathsf{v}(\\mathsf{x} + t\\mathsf{w}) has been parallely shifted from the tangent space T_{\\mathsf{x} + t\\mathsf{w}}U to T_{\\mathsf{x}}U . Such a shift is possible because of the fact that the Euclidean space \\mathbb{R}^3 is flat . The gradient of the vector field \\mathsf{v} is defined as the second order tensor field \\nabla \\mathsf{v}:U \\to \\otimes^2 TU such that, at every \\mathsf{x} \\in U , \\nabla \\mathsf{v}(\\mathsf{x}) \\cdot \\mathsf{w} = \\nabla_{\\mathsf{w}} \\mathsf{v}(\\mathsf{x}), for every \\mathsf{w} \\in T_{\\mathsf{x}}U . Recall that the term \\nabla \\mathsf{v}(\\mathsf{x}) \\cdot \\mathsf{w} in the definition of the gradient above stands for \\mathcal{C}_{2,3}(\\nabla \\mathsf{v}(\\mathsf{x}) \\otimes \\mathsf{w}) . The Cartesian coordinate representation of the covariant derivative of the vector field \\mathsf{v}:U \\to TU at \\mathsf{x} \\in U along \\mathsf{w} \\in T_{\\mathsf{x}}U is computed as follows: \\begin{split} \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\sum v_i(x_1 + tw_1, x_2 + tw_2, x_3 + tw_3) \\mathsf{e}_i\\\\ &= \\sum \\partial_j v_i(\\mathsf{x}) w_j \\mathsf{e}_i\\\\ &= \\left(\\sum \\partial_j v_i(\\mathsf{x}) \\, \\mathsf{e}_i \\otimes \\mathsf{e}_j\\right)\\cdot\\left(\\sum w_k \\mathsf{e}_k\\right). \\end{split} In deriving this expression, use has been made of the fact that the standard basis \\mathsf{e}_i of T_{\\mathsf{x}}U does not change as \\mathsf{x} varies over U . This calculation thus shows that the gradient of the vector field \\mathsf{v} has the following Cartesian coordinate representation: \\nabla \\mathsf{v}(\\mathsf{x}) = \\sum \\partial_j v_i(\\mathsf{x}) \\, \\mathsf{e}_i \\otimes \\mathsf{e}_j. This calculation also shows that the components of the gradient of a vector field with respect to the Cartesian coordinate system can be represented in matrix form as follows: = \\begin{bmatrix} \\partial_1 v_1(\\mathsf{x}) & \\partial_2 v_1(\\mathsf{x}) & \\partial_3 v_1(\\mathsf{x})\\\\ \\partial_1 v_2(\\mathsf{x}) & \\partial_2 v_2(\\mathsf{x}) & \\partial_3 v_2(\\mathsf{x})\\\\ \\partial_1 v_3(\\mathsf{x}) & \\partial_2 v_3(\\mathsf{x}) & \\partial_3 v_3(\\mathsf{x})\\end{bmatrix}. As remarked earlier, the alternative notation \\text{grad }\\mathsf{v}(\\mathsf{x}) will often be used for \\nabla \\mathsf{v}(\\mathsf{x}) . The definition of the covariant derivative of a vector field can be readily extended to the covariant derivative of tensor fields. Suppose that \\mathsf{A}:U \\to \\otimes^k TU is a smooth tensor field of order k on U . The covariant derivative of \\mathsf{A} at \\mathsf{x} \\in U along \\mathsf{v} \\in T_{\\mathsf{x}}U is defined as the k^{\\text{th}} order tensor \\nabla_{\\mathsf{v}} \\mathsf{A}(\\mathsf{x}) \\in \\otimes^k T_{\\mathsf{x}}U such that \\nabla_{\\mathsf{v}} \\mathsf{A}(\\mathsf{x}) = \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{A}(\\mathsf{x} + t\\mathsf{v}). The gradient of the tensor field \\mathsf{A} is defined as the tensor field \\nabla \\mathsf{A}:U \\to \\otimes^{k + 1} TU of order k + 1 on U such that, at every \\mathsf{x} \\in U , \\nabla \\mathsf{A}(\\mathsf{x}) \\cdot \\mathsf{v} = \\nabla_{\\mathsf{v}} \\mathsf{A}(\\mathsf{x}), for every \\mathsf{v} \\in T_{\\mathsf{x}}U . As before, the Cartesian coordinate representation of \\nabla \\mathsf{A}(\\mathsf{x}) can be worked out to be \\nabla \\mathsf{A}(\\mathsf{x}) = \\sum \\partial_j A_{i_1 \\ldots i_k}(\\mathsf{x}) \\, \\mathsf{e}_{i_1} \\otimes \\ldots \\mathsf{e}_{i_k} \\otimes \\mathsf{e}_j. Notice how the special structure of \\mathbb{R}^3 , and the special choice of the same basis for every tangent space of U have resulted in the simple expressions for gradient of tensor fields. Divergence Suppose that \\mathsf{A}:U \\to \\otimes^k TU is a smooth tensor field of order k , where k \\ge 1 , over an open subset U \\subseteq \\mathbb{R}^3 . The divergence of \\mathsf{A} is defined as the smooth tensor field \\nabla \\cdot \\mathsf{A}:U \\to \\otimes^{k-1}TU of order k - 1 on U such that, for any \\mathsf{x} \\in U , \\nabla \\cdot \\mathsf{A} (\\mathsf{x}) = \\mathcal{C}_{k,k+1} \\nabla \\mathsf{A}(\\mathsf{x}). Note that the gradient \\nabla \\mathsf{A}:U \\to \\otimes^{k+1} TU of \\mathsf{A} is a tensor field of order k + 1 on U , whence its contraction is a tensor field of order k - 1 , as required. Note also since \\mathsf{A} is a tensor field of order k , there are k possible contractions of \\nabla \\mathsf{A} . Each of these defines a distinct notion of divergence, and the convention adopted here to call the (k,k+1) contraction of \\nabla \\mathsf{A} as the divergence of \\mathsf{A} is just a matter of convention that will turn out to be helpful later on. Remark The notation \\text{div }\\mathsf{A} will also be used in this course to denote the divergence \\nabla \\cdot \\mathsf{A} of the tensor field \\mathsf{A} . In terms of the Cartesian coordinate system on \\mathbb{R}^3 , it is straightforward to check that \\nabla \\cdot \\mathsf{A}(\\mathsf{x}) = \\sum \\partial_a A_{i_1 \\ldots i_{k - 1}a}(\\mathsf{x}) \\, \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_{k-1}}. It is helpful to work out a few examples to illustrate the definition of the divergence of a tensor field. Suppose that \\mathsf{v}:U \\to TU is a smooth vector field on U . The divergence of \\mathsf{v} is a scalar field on U that is easily computed as follows: for any \\mathsf{x} \\in U , \\nabla \\cdot \\mathsf{v}(\\mathsf{x}) = \\mathcal{C}_{1,2} \\nabla \\mathsf{v}(\\mathsf{x}) = \\sum \\partial_i v_i(\\mathsf{x}). Similarly, given a smooth second order tensor field \\mathsf{A}:U \\to \\otimes^2 TU on U , its divergence is a vector field on U that is obtained as follows: for any \\mathsf{x} \\in U , \\nabla \\cdot \\mathsf{A}(\\mathsf{x}) = \\mathcal{C}_{2,3} \\nabla \\mathsf{A}(\\mathsf{x}) = \\sum \\partial_j A_{ij}(\\mathsf{x}) \\mathsf{e}_i. These expressions agree with the familiar expressions for divergence from elementary calculus. Curl The curl operation, specific to tensor fields over \\mathbb{R}^3 , is introduced now. The curl of a tensor field of order k , where k > 1 is defined recursively in terms of the curl of a tensor field of order k - 1 . Hence, the curl of a vector field is discussed first, followed by the general definition and an illustration of how to compute the curl of a second order tensor field. Suppose that \\mathsf{v}:U \\to \\otimes^k TU is a smooth vector field over an open subset U \\subseteq \\mathbb{R}^3 . The curl of \\mathsf{v} is defined as the smooth vector field \\nabla \\times \\mathsf{A}:U \\to \\otimes TU such that, for any \\mathsf{x} \\in U , \\mathsf{w} \\cdot \\nabla \\times \\mathsf{v}(\\mathsf{x}) = \\nabla \\cdot (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}). In the definition above, \\mathsf{w}:U \\to TU is a constant vector field on U : this can be thought of as the set \\cup_{\\mathsf{x} \\in U} (\\mathsf{x}, \\mathsf{w}) . Since \\mathsf{w} is a constant vector field, its dependence on \\mathsf{x} will be suppressed. The vector field \\mathsf{v} \\times \\mathsf{w}:U \\to \\otimes TU is defined as follows: for any \\mathsf{x} \\in U , \\mathsf{v} \\times \\mathsf{w}(\\mathsf{x}) = \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} . Remark The alternative notation \\text{curl }\\mathsf{v} will often be used to denote \\nabla \\times \\mathsf{v} . To see how this definition of the curl of a vector field matches up with the familiar expression from undergraduate calculus, it is helpful to work out the Cartesian coordinate represention of \\nabla \\times \\mathsf{v} . Since the curl of \\mathsf{v} is computed from the divergence of \\mathsf{v} , which, in turn, is computed from the gradient of \\mathsf{v} , the gradient of the vector field \\mathsf{v} \\times \\mathsf{w}:U \\to TU is computed first: for any \\mathsf{x} \\in U and \\mathsf{z} \\in T_{\\mathsf{x}}U , \\begin{split} \\nabla (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) \\cdot \\mathsf{z} &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{v}(\\mathsf{x} + t\\mathsf{z}) \\times \\mathsf{w}\\\\ &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\sum \\epsilon_{ijk} v_j(x_1 + tz_1, x_2 + tz_2, x_3 + tz_3) \\, w_k \\mathsf{e}_i\\\\ &= \\left(\\sum \\epsilon_{ijk} \\partial_a v_j(\\mathsf{x}) w_k \\mathsf{e}_i \\otimes \\mathsf{e}_a\\right) \\cdot \\left(\\sum z_b \\mathsf{e}_b\\right). \\end{split} This calculation thus yields the Cartesian coordinate representation of \\nabla (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) as \\nabla (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_a v_j(\\mathsf{x}) w_k \\mathsf{e}_i \\otimes \\mathsf{e}_a. The Cartesian coordinate representation of the divergence of the vector field \\mathsf{v} \\times \\mathsf{w} immediately follows as \\begin{split} \\nabla \\cdot (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) &= \\sum \\epsilon_{ijk} \\partial_i v_j(\\mathsf{x}) w_k\\\\ &= \\left(\\sum \\epsilon_{kij} \\partial_i v_j(\\mathsf{x}) \\, \\mathsf{e}_k\\right) \\cdot \\left(\\sum w_l \\mathsf{e}_l\\right). \\end{split} Comparing the final expression just obtained with \\mathsf{w} \\cdot \\nabla \\times \\mathsf{v}(\\mathsf{x}) yields the following expression for the Cartesian coordinate representation of the curl of the vector field \\mathsf{v} : \\nabla \\times \\mathsf{v}(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_j v_k(\\mathsf{x}) \\, \\mathsf{e}_i, which is the familiar expression for the curl of a vector field from undergraduate calculus. Remark The foregoing result can more easily be obtained by using the Cartesian coordinate expression for the divergence of a vector field. The reason for illustrating it in the longer version is to show the following dependence tree of the various differential quantities introduced in this section: \\text{Covariant derivative} \\to \\text{Gradient} \\to \\text{Divergence} \\to \\text{Curl}. The covariant derivative, which is a generalization of the directional derivative, is thus the fundamental starting point for all these computations. Before introducing the definition of the curl of a tensor field on U , it is pertinent to point out two useful identities. First, given any smooth vector field \\mathsf{v}:U \\to TU , the following result holds: \\nabla \\cdot \\nabla \\times \\mathsf{v} = \\mathsf{0} , where \\mathsf{0}:U \\to TU is the vector field that takes every \\mathsf{x} \\in U to the zero vector \\mathsf{0} \\in T_{\\mathsf{x}}U . This is most easily seen by using Cartesian coordinates: for any \\mathsf{x} \\in U , \\nabla \\cdot \\nabla \\times \\mathsf{v}(\\mathsf{x}) = \\sum \\partial_i \\epsilon_{ijk} \\partial_j v_k(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_i \\partial_j v_k(\\mathsf{x}) = 0. The last step follows from the equality of mixed partial derivatives and the properties of the Levi-Civita symbols. A second identity that is useful in practice is the following: if f:U \\to \\mathbb{R} is a smooth scalar field, then \\nabla \\times \\nabla f = \\mathsf{0} . This is also easily proved in a Cartesian coordinate setting: for any \\mathsf{x} \\in U , \\nabla \\times \\nabla f(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_j \\partial_k f(\\mathsf{x}) \\mathsf{e}_i = 0. Remark Despite the fact that a special coordinate system, namely the Cartesian coordinate system, was used to prove the identities \\text{div curl }\\mathsf{v} = \\mathsf{0} and \\text{curl grad }f = \\mathsf{0} , it should be noted that the final expression obtained in the coordinate setting, when cast back into the coordinate-free form, is still valid in any coordinate system. This trick will be used throughout this course to simplify a variety of calculations since it is significantly easier to work in a Cartesian coordinate setting. The curl of a smooth tensor field \\mathsf{A}:U \\to \\otimes^k TU of order k over U , is defined as the smooth tensor field \\nabla \\times \\mathsf{A}:U \\to \\otimes^k TU of order k on U such that, for any \\mathsf{x} \\in U , \\mathsf{w} \\cdot \\nabla \\times \\mathsf{A}(\\mathsf{x}) = \\nabla \\times (\\mathsf{w} \\cdot \\mathsf{A})(\\mathsf{x}). Here, \\mathsf{w}:U \\to TU is a constant vector field on U , as before. Notice that this is a recursive definition. The quantity \\mathsf{w} \\cdot \\mathsf{A} is a tensor field of order k-1 over U . To see this, note that \\mathsf{w}\\cdot\\mathsf{A} has the following Cartesian coordinate representation: for any \\mathsf{x} \\in U , \\mathsf{w} \\cdot \\mathsf{A}(\\mathsf{x}) = \\sum w_{i_1} A_{i_1i_2 \\ldots i_k}(\\mathsf{x}) \\, \\mathsf{g}_{i_2} \\otimes \\ldots \\otimes \\mathsf{g}_{i_k} \\in \\otimes^{k-1} T_{\\mathsf{x}}U. This shows that \\mathsf{w}\\cdot\\mathsf{A}:U \\to \\otimes^{k-1}TU is indeed a smooth tensor field of order k - 1 on U . The curl of the tensor field \\mathsf{w} \\cdot \\mathsf{A} is, in turn, defined using the curl of a tensor field of order k - 2 , and so on, until a vector field, whose curl is computable directly, is obtained. To illustrate the recursive definition, it is instructive to work out the curl of a second order tensor field \\mathsf{A}:U \\to \\otimes^2 TU on U . Note that, in this case, \\mathsf{w} \\cdot \\mathsf{A}:U \\to TU , where \\mathsf{w}:U \\to TU is a constant vector field on U , is a smooth vector field on U . Hence, using Cartesian coordinates, it is evident that for any \\mathsf{x} \\in U , \\nabla \\times (\\mathsf{w} \\cdot \\mathsf{A})(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_j (w_a A_{ak}(\\mathsf{x})) \\mathsf{e}_i = \\left(\\sum w_b \\mathsf{e}_b\\right) \\cdot \\left(\\sum \\epsilon_{ijk} \\partial_j A_{ak}(\\mathsf{x}) \\, \\mathsf{e}_a \\otimes \\mathsf{e}_i\\right). Using the definition \\mathsf{w} \\cdot \\text{curl }\\mathsf{A} = \\text{curl }(\\mathsf{w} \\cdot \\mathsf{A}) , it follows that \\nabla \\times \\mathsf{A}(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_j A_{lk}(\\mathsf{x}) \\, \\mathsf{e}_l \\otimes \\mathsf{e}_i. Remark Notice how the order of the terms in the dot product \\mathsf{w} \\cdot \\mathsf{A} used in the definition of the curl operation decides the final coordinate expression of the curl of a tensor field. The choice to use \\mathsf{w} \\cdot \\mathsf{A} , instead of \\mathsf{A} \\cdot \\mathsf{w} is purely conventional, and is chosen with an eye towards certain applications. Integration of tensor fields The integration of scalar, vector and tensor fields on \\mathbb{R}^3 is now outlined. In particular, attention is focused on line, surface and volume integrals of scalar fields on \\mathbb{R}^3 . Certain important integral theorems that are very useful for continuum mechanics are finally discussed. To keep the discussion simple, the Cartesian coordinate system is adopted throughout this section. Volume integrals Given a scalar field f:U \\subseteq \\mathbb{R}^3 \\to \\mathbb{R} on an open subset U of \\mathbb{R}^3 . The (volume) integral of f over U is written as \\int_U f(\\mathsf{x})\\,dv = \\int_U f(x_1,x_2,x_3) \\, dx_1 dx_2 dx_3. The integral in the expression above is a triple integral with limits defined such that they cover U . The integral is understood in the usual Riemannian sense. Remark It is more useful to interpret the integral \\int_U f(x_1, x_2, x_3)\\,dx_1dx_2dx_3 in the sense of Lebesgue. Since this course will not focus on the analytic aspects on continuum mechanics, it is sufficient for the present purposes to interpret the integral as the limit of a Riemannian sum. Remark The volume element dv in \\int_U f(\\mathsf{x}) \\, dv can be given a precise meaning as a differential 3 -form on \\mathbb{R}^3 . Since this is outside the scope of these notes, it is sufficient to think of dv as just a useful symbol. If \\mathsf{A}:U \\to \\otimes^k TU is a tensor field of order k on U , recall that the Cartesian coordinate representation of \\mathsf{A} is given by \\mathsf{A}(\\mathsf{x}) = \\sum A_{i_1\\ldots i_k}(x_1, x_2, x_3) \\, \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}, for any \\mathsf{x} \\in U . Note that each of the components A_{i_1\\ldots i_k}:U \\to \\mathbb{R} is a scalar field on U . The (volume) integral of \\mathsf{A} over U can be defined in terms of its Cartesian components as follows: \\int_U \\mathsf{A}(\\mathsf{x}) \\, dv = \\sum \\left(\\int_U A_{i_1\\ldots i_k}(x_1, x_2, x_3)\\,dx_1dx_2dx_3\\right) \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}. Despite the fact that the Cartesian coordinate system is used to define the integral above, the change of variables formula ensures that the value of the integral does not depend on the specific choice of coordinate system. The expression for the integral in a general coordinate system will be presented in a later section. Remark It is important to note that the definition of the integral of tensor fields is, in general, not defined over differentiable manifolds. What are integrable are special tensor fields called differential forms . In the special case of Euclidean spaces \\mathbb{R}^n , and in particular, in \\mathbb{R}^3 , this problem does not arise, and the integral of tensor fields is defined in terms of the integral of its component fields, as done here. Line integrals A curve in \\mathbb{R}^3 is a smooth and injective map of the form \\mathsf\\gamma:I\\subseteq\\mathbb{R} \\to \\mathbb{R}^3 , where I is an open subset of \\mathbb{R} . Thus, for every t \\in I , there is a point (\\gamma_1(t), \\gamma_2(t), \\gamma_3(t)) \\in \\mathbb{R}^3 and the set of all such points constitute the curve \\mathsf\\gamma . Let the line C = \\mathsf\\gamma(I) \\subseteq \\mathbb{R}^3 be the image of the curve in \\mathbb{R}^3 . Note that a curve is not be identified with its image here. The tangent vector to the curve \\mathsf\\gamma at \\mathsf\\gamma(t) is the vector \\dot{\\mathsf\\gamma}(t) = (\\dot{\\gamma}_1(t), \\dot{\\gamma}_2(t), \\dot{\\gamma}_3(t)) \\in T_{\\mathsf\\gamma(t)}C , where \\dot{\\gamma}_i(t) denotes the derivative of the function \\gamma_i:I \\to \\mathbb{R} at t . It is assumed that \\dot{\\mathsf\\gamma} \\neq \\mathsf{0} for every t \\in I . A curve that has this property is called a regular curve . Remark The tangent space T_{\\mathsf{x}}C to the curve C at \\mathsf{x} \\in C is defined in a manner similar to the tangent space of an open subset of \\mathbb{R}^3 . Thus, the tangent space to C at \\mathsf{x} consists of all possible tangent vectors to C at \\mathsf{x} . Note that T_{\\mathsf{x}}C \\subset T_{\\mathsf{x}}\\mathbb{R}^3 . Given a scalar field f:U \\subseteq \\mathbb{R}^3 \\to \\mathbb{R} where U is open and C \\subseteq U , the line integral of f over C is defined as follows: \\int_C f(\\mathsf{x}) \\, ds = \\int_I (f \\circ \\mathsf\\gamma)(t) \\lVert \\dot{\\mathsf\\gamma}(t)\\rVert \\, dt. Remark The line element ds used in the line integral \\int_C f(\\mathsf{x})\\,ds can be given a precise definition as a differential 1 -form on \\mathbb{R}^3 . For the present purposes, though, it is sufficient to think of ds as just a useful symbol. Note that the definition of the line integral provided here depends on the choice of a curve \\mathsf\\gamma . Such a curve is called a parametrization of the line C . It can be shown that the value of the line integral is independent of the choice of parametrization, thereby establishing that the definition of the integral is well-defined. The line integral of tensor fields over a regular curve is defined in terms of the line integral of its component scalar fields, as in the case of the volume integral. Surface integrals To define surface integrals, it is necessary to introduce a few basic concepts. A parametrized surface in \\mathbb{R}^3 is a smooth and injective map of the form \\mathsf{a}:R \\subseteq \\mathbb{R}^2 \\to \\mathbb{R}^3 . The components of \\mathsf{a} are the maps \\mathsf{a}_i:R \\to \\mathbb{R}^3 , where i = 1,2,3 . Let the surface S = \\mathsf{a}(R) \\subseteq \\mathbb{R}^3 be the image of the parametrized surface \\mathsf{a} in \\mathbb{R}^3 . For any (r,s) \\in R , the tangents to the surface S at \\mathsf{a}(r,s) \\in S are defined as the vectors \\mathsf{t}_1(r,s), \\mathsf{t}_2(r,s) \\in T_{\\mathsf{a}(r,s)}S given by \\mathsf{t}_i(r,s) = \\partial_i \\mathsf{a}(r,s) = \\sum \\partial_i a_j(r,s) \\mathsf{e}_j, \\qquad i = 1,2. The map \\mathsf{a}:R \\to \\mathbb{R}^3 is called a regular surface if \\mathsf{t}_1(r,s) \\times \\mathsf{t}_2(r,s) \\neq \\mathsf{0} for every (r,s) \\in R . Remark The tangent space to the surface S at \\mathsf{x} \\in S is defined along the same lines as the tangent to a curve in \\mathbb{R}^3 or an open subset of \\mathbb{R}^3 . Thus T_{\\mathsf{x}}S consists of all possible tangent vectors to S at the point \\mathsf{x} . Note that T_{\\mathsf{x}}S \\subset T_{\\mathsf{x}}\\mathbb{R}^3 . Let \\mathsf{t}_1(r,s),\\mathsf{t}_2(r,s) \\in T_{\\mathsf{a}(r,s)}S be the tangent vectors at \\mathsf{a}(r,s) \\in S , where (r,s) \\in R , to a regular surface S . The unit normal to the surface S at \\mathsf{a}(r,s) is defined as the vector \\mathsf{n}(r,s) \\in T_{\\mathsf{a}(r,s)}\\mathbb{R}^3 given by \\mathsf{n}(r,s) = \\frac{\\mathsf{t}_1(r,s) \\times \\mathsf{t}_2(r,s)}{\\lVert\\mathsf{t}_1(r,s) \\times \\mathsf{t}_2(r,s)\\rVert}. Note that it is necessary for the surface S to be regular for the unit normal to be well-defined. It is noted, without proof, that different parametrizations of S yield the same unit normal vector, up to a sign, at any point on S . One of the two possible unit normal vectors at point on S is defined as the positive unit normal, and the set of all parametrizations that yield this positive unit normal are said to constitute an orientation on S . If f:U \\subseteq \\mathbb{R}^3 \\to \\mathbb{R} , where U is open in \\mathbb{R}^3 , is a smooth scalar field, and \\mathsf{a}:R \\subseteq \\mathbb{R}^2 \\to \\mathbb{R}^3 is a regular surface such that S = \\mathsf{a}(R) \\subseteq U , then the surface integral of f over S is defined as follows: \\int_S f(\\mathsf{x}) \\, da = \\int_R (f \\circ \\mathsf{a})(r,s) \\lVert \\partial_1 \\mathsf{a}(r,s) \\times \\partial_2 \\mathsf{a}(r,s)\\rVert \\, dr ds. As in the case of the line integral, it can be shown that this definition of the surface integral of the scalar field f is independent of the choice of the parametrization \\mathsf{a} . Remark The area element da in \\int_S f(\\mathsf{x}) \\, da can be given a precise definition as a differential 2 -form on \\mathbb{R}^3 . As remarked before, it is sufficient to consider da as just a useful symbol for this course. The surface integral of tensor fields over a regular surface is defined in terms of the surface integral of its component scalar fields, as in the case of the volume integral. Divergence and Gauss theorems To conclude this section, two important theorems are presented without proof. The divergence theorem provides a means to convert volume integrals into an integral over the surface bounding the volume, and Stokes\u2019 theorem provides a means to reduce a surface integral to a line integral over the bounding curve of the surface. Suppose that \\mathsf{A}:U \\to \\otimes^k TU is a tensor field of order k on an open subset U \\subseteq \\mathbb{R}^3 . Suppose further that the boundary of U is the set \\partial U . Then, the divergence theorem states that \\int_U \\nabla \\mathsf{A}(\\mathsf{x}) \\, dv = \\int_{\\partial U} \\mathsf{A}(\\mathsf{x}) \\otimes \\mathsf{n}(\\mathsf{x}) \\, da, where \\mathsf{n}(\\mathsf{x}) is the outward unit normal at \\mathsf{x} \\in \\partial U . It is to be noted that the statement of the divergence theorem presented here is valid only for tensor fields defined on Euclidean spaces. A few special cases are now presented to illustrate the divergence theorem. First, if f:U \\to \\mathbb{R} , where U \\subseteq \\mathbb{R}^3 is open, is a scalar field, then the divergence theorem takes the form \\int_U \\nabla f(\\mathsf{x}) \\, dv = \\int_{\\partial U} f(\\mathsf{x})\\mathsf{n}(\\mathsf{x})\\,da. If \\mathsf{v}:U \\to TU is a vector field on U , the statement of the divergence theorem reads \\int_U \\nabla \\mathsf{v}(\\mathsf{x})\\,dv = \\int_{\\partial U} \\mathsf{v}(\\mathsf{x}) \\otimes \\mathsf{n}(\\mathsf{x})\\,da. Taking the trace this equation yields the familiar form of the divergence theorem: \\int_U \\nabla \\cdot \\mathsf{v}(\\mathsf{x}) \\, dv = \\int_{\\partial U} \\mathsf{v}(\\mathsf{x}) \\cdot \\mathsf{n}(\\mathsf{x}) \\, da. Finally, if \\mathsf{A}:U \\to \\otimes^2 TU represents a second order tensor field on U , the foregoing argument can be extended to yield the following form of the divergence theorem: \\int_U \\nabla \\cdot \\mathsf{A}(\\mathsf{x}) \\, dv = \\int_{\\partial U} \\mathsf{A}(\\mathsf{x})\\cdot\\mathsf{n}(\\mathsf{x})\\,da. This statement of the divergence theorem will also prove to be quite handy later on in the study of continuum mechanics. A second theorem that is useful in applications relates surface and line integrals. Specifically, if \\mathsf{v}:U \\to TU is a vector field on an open subset U of \\mathbb{R}^3 , and S is a regular surface in U with boundary \\partial S , then Stokes\u2019 theorem states that \\int_S \\nabla \\times \\mathsf{v}(\\mathsf{x}) \\cdot \\mathsf{n}(\\mathsf{x}) \\, da = \\int_{\\partial S} \\mathsf{v}(\\mathsf{x}) \\cdot \\mathsf{t}(\\mathsf{x}) \\, ds. Here \\mathsf{t}(\\mathsf{x}) \\in T_{\\mathsf{x}}\\partial S is the tangent vector at the point \\mathsf{x} \\in \\partial S on the boundary curve \\partial S of S . Remark There is an elegant theory using the notion of differential forms that generalizes these theorems to the general case of differentiable manifolds. The generalized Stokes\u2019 theorem unifies both the theorems presented above and takes a remarkably simple form in the language of differential forms.","title":"Euclidean Tensor Analysis"},{"location":"tensor_analysis_R3/#coordinate-systems","text":"A coordinate system on \\mathbb{R}^3 is an open subset U \\subseteq \\mathbb{R}^3 , together with a smooth injective map \\mathsf\\phi:\\mathbb{R}^3 \\to \\mathbb{R}^3 such that \\mathsf\\phi^{-1}:\\mathsf\\phi(U) \\to U is also smooth; such a map is called a diffeomorphism .. The map \\mathsf\\phi is called a coordinate chart and the open set U is called a coordinate patch on \\mathbb{R}^3 . Given any \\mathsf{x} \\in U , the triple of real numbers \\mathsf\\phi(\\mathsf{x}) \\in \\mathbb{R}^3 are called the curvilinear coordinates of \\mathsf{x} . What is special about \\mathbb{R}^3 (, and this is true for \\mathbb{R}^n too,) is that it admits a global coordinate system (\\mathbb{R}^3, \\mathsf\\iota) , also called the Cartesian coordinate system , where \\mathsf\\iota:\\mathbb{R}^3 \\to \\mathbb{R}^3 is the identity map defined as follows: for any \\mathsf{x} \\in \\mathbb{R}^3 , \\mathsf\\iota(\\mathsf{x}) = \\mathsf{x} . The coordinates of \\mathsf{x} with respect to this global coordinate system are called the Cartesian coordinates of \\mathsf{x} . Remark A few terminologies inspired by the more general case of a manifold are introduced here. A coordinate system (U,\\mathsf\\phi) on \\mathbb{R}^3 is said to be compatible with the Cartesian coordinate system on \\mathbb{R}^3 if the maps \\mathsf\\phi:U \\to \\mathsf\\phi(U) and \\mathsf\\phi^{-1}:\\mathsf\\phi(U) \\to U are smooth and invertible. Note that all coordinate systems considered hereafter are considered to be compatible with the Cartesian coordinate system on \\mathbb{R}^3 . An atlas on \\mathbb{R}^3 is a collection of compatible coordinate systems ((U_\\alpha,\\mathsf\\phi_\\alpha))_{\\alpha \\in A} , where A is some index set, such that \\cup_{\\alpha \\in A} U_\\alpha = \\mathbb{R}^3 . Note that adding a compatible coordinate system to an atlas yields a larger atlas. Consider now the Cartesian coordinate system on \\mathbb{R}^3 and construct the maximal atlas compatible with it; this amounts to adding every compatible coordinate system to the atlas consisting of the global Cartesian coordinate system. This maximal atlas is called the standard differentiable structure on \\mathbb{R}^3 . Remark The fact that \\mathbb{R}^3 admits a global coordinate system significantly simplifies the development of calculus on \\mathbb{R}^3 . There, however, exist many sets of practical importance that cannot be covered using a global coordinate system. The modern theory of differentiable manifolds was developed precisely to address these issues, and develop an appropriate extension of the tools of calculus to these sets. Suppose that (U,\\mathsf\\phi) is a coordinate system on \\mathbb{R}^3 that is compatible with the Cartesian coordinate system on \\mathbb{R}^3 . The curvilinear coordinates of \\mathsf{x} \\in U with respect to the coordinate system (U,\\mathsf\\phi) is given by the triple of real numbers \\mathsf{y} \\in \\mathbb{R}^3 , where \\mathsf{y} = \\mathsf\\phi(\\mathsf{x}). The inverse map \\mathsf\\phi^{-1}:\\mathsf\\phi(U) \\to U permits the computation of \\mathsf{x} given \\mathsf{y} : \\mathsf{x} = \\mathsf\\phi^{-1}(\\mathsf{y}). A useful simplified notation that will be adopted in the following development is the following: the foregoing relations will often be written as \\mathsf{y} = \\mathsf{y}(\\mathsf{x}), \\qquad \\mathsf{x} = \\mathsf{x}(\\mathsf{y}). Notice that this is a deliberate abuse of notation: \\mathsf{x} stands for both a point in U and a function that takes a point in \\mathsf\\phi(U) and returns a real number. Despite the ambiguity this introduces, this notation is often adopted in practice since it significantly simplifies the appearance of many equations. The ambiguity will not cause any problems as long as the appropriate meaning of the symbols are inferred from the context. To quantify precisely the notion that the coordinate system (U,\\mathsf\\phi) is compatible with the Cartesian coordinate system, note that the map \\mathsf\\phi:U \\to \\mathbb{R}^3 is smooth, and hence, its Fr\u00e9chet derivative exists at each \\mathsf{x} \\in U . This implies, in particular that the determinant of the Jacobian matrix is non-zero: \\text{det} \\begin{bmatrix} \\partial_1 y_1(\\mathsf{x}) & \\partial_2 y_1(\\mathsf{x}) & \\partial_3 y_1(\\mathsf{x})\\\\ \\partial_1 y_2(\\mathsf{x}) & \\partial_2 y_2(\\mathsf{x}) & \\partial_3 y_2(\\mathsf{x})\\\\ \\partial_1 y_3(\\mathsf{x}) & \\partial_2 y_3(\\mathsf{x}) & \\partial_3 y_3(\\mathsf{x})\\end{bmatrix} \\neq 0. Here, \\mathsf{y}(\\mathsf{x}) = (y_1(\\mathsf{x}), y_2(\\mathsf{x}), y_3(\\mathsf{x})) . A similar condition holds for the inverse map \\mathsf{x}:\\mathsf\\phi(U) \\to \\mathbb{R}^3 .","title":"Coordinate systems"},{"location":"tensor_analysis_R3/#tangent-spaces","text":"A concept that will prove to be very useful later on is that of a tangent space . Rather than defining this precisely, it suffices for the purposes of this course to present a qualitative definition that provides some intuition about what tangent spaces are. Let U \\subseteq \\mathbb{R}^3 be an open subset of \\mathbb{R}^3 . At each \\mathsf{x} \\in U , consider the set T_{\\mathsf{x}}U defined as follows: T_{\\mathsf{x}}U = \\{\\mathsf{v} \\in \\mathbb{R}^3 \\,|\\, \\text{there exists a (smooth) curve passing through $\\mathsf{x}$ with tangent $\\mathsf{v}$ at $\\mathsf{x}$}\\}. This set is called the tangent space at \\mathsf{x} to \\mathbb{R}^3 . The point \\mathsf{x} is called the base point of the tangent space. In the special case of \\mathbb{R}^3 , given any \\mathsf{v} \\in \\mathbb{R}^3 , the straight line \\{\\mathsf{x} + t\\mathsf{v}\\,|\\,t \\in \\mathbb{R}\\} is a smooth curve that passes through \\mathsf{x} with tangent \\mathsf{v} at \\mathsf{x} . This shows that T_{\\mathsf{x}}\\mathbb{R}^3 \\cong \\mathbb{R}^3 , and is hence a linear space. Thus the tangent space at any point in \\mathsf{x} \\in \\mathbb{R}^3 is a local copy of the whole of \\mathbb{R}^3 attached to \\mathsf{x} , and hence need not be distinguished from \\mathbb{R}^3 . For conceptual clarity, however, the tangent space will be explicitly indicated whenever appropriate. Further, each tangent space is assumed to be an inner product space, with the inner product being identical to the standard inner product on \\mathbb{R}^3 . The union of all tangent spaces to U is called the tangent bundle of U , and is written as TU : TU = \\cup_{\\mathsf{x} \\in U} T_{\\mathsf{x}}U. Notice that this is a disjoint union. Remark For the special case of U \\subseteq \\mathbb{R}^3 , where U is open, the tangent space T_{\\mathsf{x}}U to U at \\mathsf{x} \\in U can be equivalently characterized as follows: T_{\\mathsf{x}}U = \\{(\\mathsf{x},\\mathsf{v}) \\,|\\, \\mathsf{v} \\in \\mathbb{R}^3\\}. The set T_{\\mathsf{x}}U defined above acquires a linear structure with addition +:T_{\\mathsf{x}}U \\times T_{\\mathsf{x}} \\to T_{\\mathsf{x}}U and scalar multiplication \\cdot:\\mathbb{R} \\times T_{\\mathsf{x}} \\to T_{\\mathsf{x}}U defined as follows: for any \\mathsf{v}, \\mathsf{w} \\in \\mathbb{R}^3 and a \\in \\mathbb{R} , (\\mathsf{x},\\mathsf{v}) + (\\mathsf{x},\\mathsf{w}) = (\\mathsf{x}, \\mathsf{v} + \\mathsf{w}), \\qquad a(\\mathsf{x}, \\mathsf{v}) = (\\mathsf{x}, a\\mathsf{v}). Notice now this definition does not affect the base point \\mathsf{x} : this is necessary to ensure that vector addition and scalar multiplication satisfy the closure property. With this definition of the tangent space, it is easy to check that the tangent bundle is given by TU = U \\times \\mathbb{R}^3 . It follows from the the foregoing discussion that each T_{\\mathsf{x}}U can be thought as a local copy of \\mathbb{R}^3 . Thus, the tangent bundle TU can be envisaged as a copy of \\mathbb{R}^3 attached to each point \\mathsf{x} \\in U . Each of the tangent spaces T_{\\mathsf{x}}U , where \\mathsf{x} \\in U , can be equipped with a basis of \\mathbb{R}^3 that could, in general, vary as \\mathsf{x} varies over U . The simplest choice is to use the same standard basis (\\mathsf{e}_i) of \\mathbb{R}^3 as the basis for every tangent space T_{\\mathsf{x}}U . With this choice, any \\mathsf{h} \\in T_{\\mathsf{x}}\\mathbb{R}^3 admits a representation of the form \\mathsf{h} = \\sum h_i \\mathsf{e}_i . Note that other choices of bases are possible for each tangent space T_{\\mathsf{x}}\\mathbb{R}^3 ; this will in fact be the case when curvilinear coordinate systems are considered in a later section.","title":"Tangent spaces"},{"location":"tensor_analysis_R3/#tensor-fields","text":"Having defined the basic concepts of coordinate systems and tangent spaces, the stage is set to introduce the most important quantities from the point of view of continuum mechanics: tensor fields . In a loose sense, the term field is used here in the sense of a quantity that varies smoothly over a region of \\mathbb{R}^3 . The basic idea behind a tensor field is to associate a tensor to every point in some region of \\mathbb{R}^3 , and to do so in a manner that is smooth . These ideas are developed precisely in this section. Scalar fields, which are tensor fields of order 0 are discussed first, followed by vector fields, which are tensor fields of order 1 , and finally tensor fields of order k are discussed. Throughout this section U \\subseteq \\mathbb{R}^3 denotes an open subset of \\mathbb{R}^3 . For the coordinate representation of various fields, the global Cartesian coordinate system is used for \\mathbb{R}^3 and the standard basis of \\mathbb{R}^3 is used for each T_{\\mathsf{x}}U for every \\mathsf{x} \\in U . Remark The fact that \\mathbb{R}^3 has a linear structure will turn out be very convenient for the ensuing development and simplify many of the calculations. Some of the terminology associated with the more general case will however be used when appropriate. A scalar field is a function of the form f:U\\subseteq\\mathbb{R}^3 \\to \\mathbb{R} . Thus, the scalar field f associates with each \\mathsf{x} \\in U the real number f(\\mathsf{x}) . Note that the continuity, differentiability and smoothness of f can be defined according to the definitions given in the previous section in the context of nonlinear maps between vector spaces. A vector field on U \\subseteq \\mathbb{R}^3 is a map of the form \\mathsf{v}:U \\to TU such that \\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U for every \\mathsf{x} \\in U . The vector field \\mathsf{v} can be expressed with respect to the Cartesian coordinate system as follows: \\mathsf{v}(\\mathsf{x}) = \\sum v_i(\\mathsf{x}) \\mathsf{e}_i, where each v_i:U \\to \\mathbb{R} is a scalar field. The continuity/differentiability/smoothness of the vector field \\mathsf{v} is defined in terms of the corresponding property of the scalar fields v_i . A tensor field of order k on U \\subseteq \\mathbb{R}^3 is defined similarly as a map of the form \\mathsf{A}:U \\to \\otimes^k \\, TU, such that \\mathsf{A}(\\mathsf{x}) \\in \\otimes^k \\, T_{\\mathsf{x}}U for every \\mathsf{x} \\in U . In writing these expressions, the following notations have been introduced: \\begin{split} \\otimes^k T_{\\mathsf{x}}U &= \\{ \\mathsf{T}:\\times^k \\, T_{\\mathsf{x}}U \\to \\mathbb{R} \\,|\\, \\mathsf{T}\\text{ is multilinear }\\} = \\mathcal{T}^k(T_{\\mathsf{x}}U),\\\\ \\otimes^k TU &= \\cup_{\\mathsf{x} \\in U} \\left( \\otimes^k T_{\\mathsf{x}}U \\right) = \\cup_{\\mathsf{x} \\in U} \\mathcal{T}^k(T_{\\mathsf{x}}U). \\end{split} Thus, a tensor field of order k associates with each point in U a tensor of order k over the tangent space at that point. In terms of the Cartesian coordinate system, it is evident that \\mathsf{A}(\\mathsf{x}) = \\sum A_{i_1\\ldots i_k}(\\mathsf{x}) \\, \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}, where each A_{i_1 \\ldots i_k}:U \\to \\mathbb{R} is a scalar field. As in the case of vector fields, the continuity/differentiability/smoothness of tensor fields are naturally defined in terms of the corresponding properties of their component scalar fields. Remark Note that a tensor field of order 0 is a scalar field since, for any \\mathsf{x} \\in U , \\mathcal{T}^0(T_{\\mathsf{x}}U) \\cong \\mathbb{R} . Similarly, a tensor field of order 1 is a vector field since, for any \\mathsf{x} \\in U , \\mathcal{T}^1(T_{\\mathsf{x}}U) \\cong T_{\\mathsf{x}}U . Notice that this is true since each tangent space T_{\\mathsf{x}}U is a finite dimensional inner product space. Thus, it suffices to study tensor fields of order k on U , where k \\ge 0 , since this includes scalar and vector fields too. For pedagogical reasons, however, scalar, vector and tensor fields will be considered successively in these notes.","title":"Tensor fields"},{"location":"tensor_analysis_R3/#covariant-derivative-and-gradient","text":"How does a tensor field vary across its domain of definition, and, in particular, how can this change be quantified precisely? This is the fundamental question that is addressed in this and the next few subsections. The fundamental quantity, towards this end, is what is called the covariant derivative of the tensor field. This is a generalization of the familiar notion of the directional derivative encountered earlier. Let f:U \\subseteq \\mathbb{R}^3 \\to \\mathbb{R} , where U is an open subset of \\mathbb{R}^3 be a smooth scalar field on U . The directional derivative , also called the covariant derivative , of f at \\mathsf{x} \\in U along any \\mathsf{v} \\in T_{\\mathsf{x}}U is defined as the scalar \\nabla_{\\mathsf{v}}f(\\mathsf{x}) \\in \\mathbb{R} such that \\nabla_{\\mathsf{v}}f(\\mathsf{x}) = \\frac{d}{dt}\\bigg\\vert_{t=0} f(\\mathsf{x} + t\\mathsf{v}). Note that in the term f(\\mathsf{x} + t\\mathsf{v}) in the definition above, \\mathsf{x} \\in U and \\mathsf{v} \\in T_{\\mathsf{x}}U . Since U and T_{\\mathsf{x}}U are not the same spaces, the quantity \\mathsf{x} + t\\mathsf{v} is ill-defined, strictly speaking. The saving grace, in this occasion, is the fact that U is an open subset of \\mathbb{R}^3 , which as a linear structure, and since T_{\\mathsf{x}}U \\cong \\mathbb{R}^3 . Thus, \\mathsf{x} + t\\mathsf{v} is to be understood as the point in \\mathbb{R}^3 whose coordinates are obtained by summing the triples of real numbers \\mathsf{x} and t\\mathsf{v} . Note that this point lies inside U when t is sufficiently small, as noted earlier. Remark In the more general case when U is not a linear set, the foregoing definition breaks down. The modern theory of differentiable manifolds provides a more rigorous definition of the covariant derivative in more general cases where the argument just given breaks down by introducing an additional structure called a connection on a manifold. The gradient of f is defined as the vector field \\nabla f:U \\to TU such that, at every \\mathsf{x} \\in U , \\nabla f(\\mathsf{x}) \\cdot \\mathsf{v} = \\nabla_{\\mathsf{v}}f(\\mathsf{x}), for any \\mathsf{v} \\in T_{\\mathsf{x}}U . Note that \\nabla f(\\mathsf{x}) \\in T_{\\mathsf{x}}U by definition. Since \\mathsf{v} \\in T_{\\mathsf{x}}U , the dot product \\nabla f(\\mathsf{x}) \\cdot \\mathsf{v} is indeed well-defined. Remark In this course, the alternative notation \\text{grad }f(\\mathsf{x}) will often be used to indicate \\nabla f(\\mathsf{x}) . The directional derivative of f at \\mathsf{x} \\in U along \\mathsf{v} \\in T_{\\mathsf{x}}U can be written in terms of the Cartesian coordinate system on \\mathbb{R}^3 as follows: \\begin{split} \\nabla_{\\mathsf{v}}f(\\mathsf{x}) &= \\frac{d}{dt}\\bigg\\vert_{t=0} f(x_1 + tv_1, x_2 + tv_2, x_3 + tv_3)\\\\ &= \\sum \\partial_i f(\\mathsf{x}) v_i. \\end{split} Noting that \\sum \\partial_i f(\\mathsf{x})v_i = \\left(\\sum \\partial_i f(\\mathsf{x}) \\, \\mathsf{e}_i\\right)\\cdot\\left(\\sum v_j \\mathsf{e}_j\\right), it follows that the Cartesian coordinate representation of the gradient of f is given by \\nabla f(\\mathsf{x}) = \\sum \\partial_i f(\\mathsf{x})\\,\\mathsf{e}_i . The covariant derivative and gradient of a smooth vector field \\mathsf{v}:U \\to TU are defined similarly. The covariant derivative of \\mathsf{v} at \\mathsf{x} \\in U along \\mathsf{w} \\in T_{\\mathsf{x}}U is defined as the vector \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U such that \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) = \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{v}(\\mathsf{x} + t\\mathsf{w}). Note that \\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U , whereas \\mathsf{v}(\\mathsf{x} + t\\mathsf{w}) \\in T_{\\mathsf{x} + t\\mathsf{w}}U . Strictly speaking, the vectors \\mathsf{v}(\\mathsf{x}) and \\mathsf{v}(\\mathsf{x} + t\\mathsf{w}) cannot be compared since they belong to different vector spaces. However, the fact that both T_{\\mathsf{x}}U and T_{\\mathsf{x} + t\\mathsf{w}}U are isomorphic to \\mathbb{R}^3 comes to the rescue again. What is actually compared are the corresponding images of these vectors in \\mathbb{R}^3 , which is a valid algebraic operation. !!! info \"Remark\" Using the characterization of \\mathsf{v}(\\mathsf{x}) \\in T_{\\mathsf{x}}U as the pair (\\mathsf{x}, \\mathsf{v}(\\mathsf{x})) , the derivative in the definition of the covariant derivative of \\mathsf{v} can be understood in the following sense: \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) = \\lim_{t \\to 0} \\frac{(\\mathsf{x},\\mathsf{v}(\\mathsf{x} + t\\mathsf{w})) - (\\mathsf{x}, \\mathsf{v}(\\mathsf{x}))}{t} = \\left(\\mathsf{x}, \\lim_{t \\to 0} \\frac{\\mathsf{v}(\\mathsf{x} + t\\mathsf{w}) - \\mathsf{v}(\\mathsf{x})}{t}\\right) \\in T_{\\mathsf{x}}U. Notice how the vector \\mathsf{v}(\\mathsf{x} + t\\mathsf{w}) has been parallely shifted from the tangent space T_{\\mathsf{x} + t\\mathsf{w}}U to T_{\\mathsf{x}}U . Such a shift is possible because of the fact that the Euclidean space \\mathbb{R}^3 is flat . The gradient of the vector field \\mathsf{v} is defined as the second order tensor field \\nabla \\mathsf{v}:U \\to \\otimes^2 TU such that, at every \\mathsf{x} \\in U , \\nabla \\mathsf{v}(\\mathsf{x}) \\cdot \\mathsf{w} = \\nabla_{\\mathsf{w}} \\mathsf{v}(\\mathsf{x}), for every \\mathsf{w} \\in T_{\\mathsf{x}}U . Recall that the term \\nabla \\mathsf{v}(\\mathsf{x}) \\cdot \\mathsf{w} in the definition of the gradient above stands for \\mathcal{C}_{2,3}(\\nabla \\mathsf{v}(\\mathsf{x}) \\otimes \\mathsf{w}) . The Cartesian coordinate representation of the covariant derivative of the vector field \\mathsf{v}:U \\to TU at \\mathsf{x} \\in U along \\mathsf{w} \\in T_{\\mathsf{x}}U is computed as follows: \\begin{split} \\nabla_{\\mathsf{w}}\\mathsf{v}(\\mathsf{x}) &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\sum v_i(x_1 + tw_1, x_2 + tw_2, x_3 + tw_3) \\mathsf{e}_i\\\\ &= \\sum \\partial_j v_i(\\mathsf{x}) w_j \\mathsf{e}_i\\\\ &= \\left(\\sum \\partial_j v_i(\\mathsf{x}) \\, \\mathsf{e}_i \\otimes \\mathsf{e}_j\\right)\\cdot\\left(\\sum w_k \\mathsf{e}_k\\right). \\end{split} In deriving this expression, use has been made of the fact that the standard basis \\mathsf{e}_i of T_{\\mathsf{x}}U does not change as \\mathsf{x} varies over U . This calculation thus shows that the gradient of the vector field \\mathsf{v} has the following Cartesian coordinate representation: \\nabla \\mathsf{v}(\\mathsf{x}) = \\sum \\partial_j v_i(\\mathsf{x}) \\, \\mathsf{e}_i \\otimes \\mathsf{e}_j. This calculation also shows that the components of the gradient of a vector field with respect to the Cartesian coordinate system can be represented in matrix form as follows: = \\begin{bmatrix} \\partial_1 v_1(\\mathsf{x}) & \\partial_2 v_1(\\mathsf{x}) & \\partial_3 v_1(\\mathsf{x})\\\\ \\partial_1 v_2(\\mathsf{x}) & \\partial_2 v_2(\\mathsf{x}) & \\partial_3 v_2(\\mathsf{x})\\\\ \\partial_1 v_3(\\mathsf{x}) & \\partial_2 v_3(\\mathsf{x}) & \\partial_3 v_3(\\mathsf{x})\\end{bmatrix}. As remarked earlier, the alternative notation \\text{grad }\\mathsf{v}(\\mathsf{x}) will often be used for \\nabla \\mathsf{v}(\\mathsf{x}) . The definition of the covariant derivative of a vector field can be readily extended to the covariant derivative of tensor fields. Suppose that \\mathsf{A}:U \\to \\otimes^k TU is a smooth tensor field of order k on U . The covariant derivative of \\mathsf{A} at \\mathsf{x} \\in U along \\mathsf{v} \\in T_{\\mathsf{x}}U is defined as the k^{\\text{th}} order tensor \\nabla_{\\mathsf{v}} \\mathsf{A}(\\mathsf{x}) \\in \\otimes^k T_{\\mathsf{x}}U such that \\nabla_{\\mathsf{v}} \\mathsf{A}(\\mathsf{x}) = \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{A}(\\mathsf{x} + t\\mathsf{v}). The gradient of the tensor field \\mathsf{A} is defined as the tensor field \\nabla \\mathsf{A}:U \\to \\otimes^{k + 1} TU of order k + 1 on U such that, at every \\mathsf{x} \\in U , \\nabla \\mathsf{A}(\\mathsf{x}) \\cdot \\mathsf{v} = \\nabla_{\\mathsf{v}} \\mathsf{A}(\\mathsf{x}), for every \\mathsf{v} \\in T_{\\mathsf{x}}U . As before, the Cartesian coordinate representation of \\nabla \\mathsf{A}(\\mathsf{x}) can be worked out to be \\nabla \\mathsf{A}(\\mathsf{x}) = \\sum \\partial_j A_{i_1 \\ldots i_k}(\\mathsf{x}) \\, \\mathsf{e}_{i_1} \\otimes \\ldots \\mathsf{e}_{i_k} \\otimes \\mathsf{e}_j. Notice how the special structure of \\mathbb{R}^3 , and the special choice of the same basis for every tangent space of U have resulted in the simple expressions for gradient of tensor fields.","title":"Covariant derivative and Gradient"},{"location":"tensor_analysis_R3/#divergence","text":"Suppose that \\mathsf{A}:U \\to \\otimes^k TU is a smooth tensor field of order k , where k \\ge 1 , over an open subset U \\subseteq \\mathbb{R}^3 . The divergence of \\mathsf{A} is defined as the smooth tensor field \\nabla \\cdot \\mathsf{A}:U \\to \\otimes^{k-1}TU of order k - 1 on U such that, for any \\mathsf{x} \\in U , \\nabla \\cdot \\mathsf{A} (\\mathsf{x}) = \\mathcal{C}_{k,k+1} \\nabla \\mathsf{A}(\\mathsf{x}). Note that the gradient \\nabla \\mathsf{A}:U \\to \\otimes^{k+1} TU of \\mathsf{A} is a tensor field of order k + 1 on U , whence its contraction is a tensor field of order k - 1 , as required. Note also since \\mathsf{A} is a tensor field of order k , there are k possible contractions of \\nabla \\mathsf{A} . Each of these defines a distinct notion of divergence, and the convention adopted here to call the (k,k+1) contraction of \\nabla \\mathsf{A} as the divergence of \\mathsf{A} is just a matter of convention that will turn out to be helpful later on. Remark The notation \\text{div }\\mathsf{A} will also be used in this course to denote the divergence \\nabla \\cdot \\mathsf{A} of the tensor field \\mathsf{A} . In terms of the Cartesian coordinate system on \\mathbb{R}^3 , it is straightforward to check that \\nabla \\cdot \\mathsf{A}(\\mathsf{x}) = \\sum \\partial_a A_{i_1 \\ldots i_{k - 1}a}(\\mathsf{x}) \\, \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_{k-1}}. It is helpful to work out a few examples to illustrate the definition of the divergence of a tensor field. Suppose that \\mathsf{v}:U \\to TU is a smooth vector field on U . The divergence of \\mathsf{v} is a scalar field on U that is easily computed as follows: for any \\mathsf{x} \\in U , \\nabla \\cdot \\mathsf{v}(\\mathsf{x}) = \\mathcal{C}_{1,2} \\nabla \\mathsf{v}(\\mathsf{x}) = \\sum \\partial_i v_i(\\mathsf{x}). Similarly, given a smooth second order tensor field \\mathsf{A}:U \\to \\otimes^2 TU on U , its divergence is a vector field on U that is obtained as follows: for any \\mathsf{x} \\in U , \\nabla \\cdot \\mathsf{A}(\\mathsf{x}) = \\mathcal{C}_{2,3} \\nabla \\mathsf{A}(\\mathsf{x}) = \\sum \\partial_j A_{ij}(\\mathsf{x}) \\mathsf{e}_i. These expressions agree with the familiar expressions for divergence from elementary calculus.","title":"Divergence"},{"location":"tensor_analysis_R3/#curl","text":"The curl operation, specific to tensor fields over \\mathbb{R}^3 , is introduced now. The curl of a tensor field of order k , where k > 1 is defined recursively in terms of the curl of a tensor field of order k - 1 . Hence, the curl of a vector field is discussed first, followed by the general definition and an illustration of how to compute the curl of a second order tensor field. Suppose that \\mathsf{v}:U \\to \\otimes^k TU is a smooth vector field over an open subset U \\subseteq \\mathbb{R}^3 . The curl of \\mathsf{v} is defined as the smooth vector field \\nabla \\times \\mathsf{A}:U \\to \\otimes TU such that, for any \\mathsf{x} \\in U , \\mathsf{w} \\cdot \\nabla \\times \\mathsf{v}(\\mathsf{x}) = \\nabla \\cdot (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}). In the definition above, \\mathsf{w}:U \\to TU is a constant vector field on U : this can be thought of as the set \\cup_{\\mathsf{x} \\in U} (\\mathsf{x}, \\mathsf{w}) . Since \\mathsf{w} is a constant vector field, its dependence on \\mathsf{x} will be suppressed. The vector field \\mathsf{v} \\times \\mathsf{w}:U \\to \\otimes TU is defined as follows: for any \\mathsf{x} \\in U , \\mathsf{v} \\times \\mathsf{w}(\\mathsf{x}) = \\mathsf{v}(\\mathsf{x}) \\times \\mathsf{w} . Remark The alternative notation \\text{curl }\\mathsf{v} will often be used to denote \\nabla \\times \\mathsf{v} . To see how this definition of the curl of a vector field matches up with the familiar expression from undergraduate calculus, it is helpful to work out the Cartesian coordinate represention of \\nabla \\times \\mathsf{v} . Since the curl of \\mathsf{v} is computed from the divergence of \\mathsf{v} , which, in turn, is computed from the gradient of \\mathsf{v} , the gradient of the vector field \\mathsf{v} \\times \\mathsf{w}:U \\to TU is computed first: for any \\mathsf{x} \\in U and \\mathsf{z} \\in T_{\\mathsf{x}}U , \\begin{split} \\nabla (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) \\cdot \\mathsf{z} &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\mathsf{v}(\\mathsf{x} + t\\mathsf{z}) \\times \\mathsf{w}\\\\ &= \\frac{d}{dt}\\bigg\\vert_{t=0} \\sum \\epsilon_{ijk} v_j(x_1 + tz_1, x_2 + tz_2, x_3 + tz_3) \\, w_k \\mathsf{e}_i\\\\ &= \\left(\\sum \\epsilon_{ijk} \\partial_a v_j(\\mathsf{x}) w_k \\mathsf{e}_i \\otimes \\mathsf{e}_a\\right) \\cdot \\left(\\sum z_b \\mathsf{e}_b\\right). \\end{split} This calculation thus yields the Cartesian coordinate representation of \\nabla (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) as \\nabla (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_a v_j(\\mathsf{x}) w_k \\mathsf{e}_i \\otimes \\mathsf{e}_a. The Cartesian coordinate representation of the divergence of the vector field \\mathsf{v} \\times \\mathsf{w} immediately follows as \\begin{split} \\nabla \\cdot (\\mathsf{v} \\times \\mathsf{w})(\\mathsf{x}) &= \\sum \\epsilon_{ijk} \\partial_i v_j(\\mathsf{x}) w_k\\\\ &= \\left(\\sum \\epsilon_{kij} \\partial_i v_j(\\mathsf{x}) \\, \\mathsf{e}_k\\right) \\cdot \\left(\\sum w_l \\mathsf{e}_l\\right). \\end{split} Comparing the final expression just obtained with \\mathsf{w} \\cdot \\nabla \\times \\mathsf{v}(\\mathsf{x}) yields the following expression for the Cartesian coordinate representation of the curl of the vector field \\mathsf{v} : \\nabla \\times \\mathsf{v}(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_j v_k(\\mathsf{x}) \\, \\mathsf{e}_i, which is the familiar expression for the curl of a vector field from undergraduate calculus. Remark The foregoing result can more easily be obtained by using the Cartesian coordinate expression for the divergence of a vector field. The reason for illustrating it in the longer version is to show the following dependence tree of the various differential quantities introduced in this section: \\text{Covariant derivative} \\to \\text{Gradient} \\to \\text{Divergence} \\to \\text{Curl}. The covariant derivative, which is a generalization of the directional derivative, is thus the fundamental starting point for all these computations. Before introducing the definition of the curl of a tensor field on U , it is pertinent to point out two useful identities. First, given any smooth vector field \\mathsf{v}:U \\to TU , the following result holds: \\nabla \\cdot \\nabla \\times \\mathsf{v} = \\mathsf{0} , where \\mathsf{0}:U \\to TU is the vector field that takes every \\mathsf{x} \\in U to the zero vector \\mathsf{0} \\in T_{\\mathsf{x}}U . This is most easily seen by using Cartesian coordinates: for any \\mathsf{x} \\in U , \\nabla \\cdot \\nabla \\times \\mathsf{v}(\\mathsf{x}) = \\sum \\partial_i \\epsilon_{ijk} \\partial_j v_k(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_i \\partial_j v_k(\\mathsf{x}) = 0. The last step follows from the equality of mixed partial derivatives and the properties of the Levi-Civita symbols. A second identity that is useful in practice is the following: if f:U \\to \\mathbb{R} is a smooth scalar field, then \\nabla \\times \\nabla f = \\mathsf{0} . This is also easily proved in a Cartesian coordinate setting: for any \\mathsf{x} \\in U , \\nabla \\times \\nabla f(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_j \\partial_k f(\\mathsf{x}) \\mathsf{e}_i = 0. Remark Despite the fact that a special coordinate system, namely the Cartesian coordinate system, was used to prove the identities \\text{div curl }\\mathsf{v} = \\mathsf{0} and \\text{curl grad }f = \\mathsf{0} , it should be noted that the final expression obtained in the coordinate setting, when cast back into the coordinate-free form, is still valid in any coordinate system. This trick will be used throughout this course to simplify a variety of calculations since it is significantly easier to work in a Cartesian coordinate setting. The curl of a smooth tensor field \\mathsf{A}:U \\to \\otimes^k TU of order k over U , is defined as the smooth tensor field \\nabla \\times \\mathsf{A}:U \\to \\otimes^k TU of order k on U such that, for any \\mathsf{x} \\in U , \\mathsf{w} \\cdot \\nabla \\times \\mathsf{A}(\\mathsf{x}) = \\nabla \\times (\\mathsf{w} \\cdot \\mathsf{A})(\\mathsf{x}). Here, \\mathsf{w}:U \\to TU is a constant vector field on U , as before. Notice that this is a recursive definition. The quantity \\mathsf{w} \\cdot \\mathsf{A} is a tensor field of order k-1 over U . To see this, note that \\mathsf{w}\\cdot\\mathsf{A} has the following Cartesian coordinate representation: for any \\mathsf{x} \\in U , \\mathsf{w} \\cdot \\mathsf{A}(\\mathsf{x}) = \\sum w_{i_1} A_{i_1i_2 \\ldots i_k}(\\mathsf{x}) \\, \\mathsf{g}_{i_2} \\otimes \\ldots \\otimes \\mathsf{g}_{i_k} \\in \\otimes^{k-1} T_{\\mathsf{x}}U. This shows that \\mathsf{w}\\cdot\\mathsf{A}:U \\to \\otimes^{k-1}TU is indeed a smooth tensor field of order k - 1 on U . The curl of the tensor field \\mathsf{w} \\cdot \\mathsf{A} is, in turn, defined using the curl of a tensor field of order k - 2 , and so on, until a vector field, whose curl is computable directly, is obtained. To illustrate the recursive definition, it is instructive to work out the curl of a second order tensor field \\mathsf{A}:U \\to \\otimes^2 TU on U . Note that, in this case, \\mathsf{w} \\cdot \\mathsf{A}:U \\to TU , where \\mathsf{w}:U \\to TU is a constant vector field on U , is a smooth vector field on U . Hence, using Cartesian coordinates, it is evident that for any \\mathsf{x} \\in U , \\nabla \\times (\\mathsf{w} \\cdot \\mathsf{A})(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_j (w_a A_{ak}(\\mathsf{x})) \\mathsf{e}_i = \\left(\\sum w_b \\mathsf{e}_b\\right) \\cdot \\left(\\sum \\epsilon_{ijk} \\partial_j A_{ak}(\\mathsf{x}) \\, \\mathsf{e}_a \\otimes \\mathsf{e}_i\\right). Using the definition \\mathsf{w} \\cdot \\text{curl }\\mathsf{A} = \\text{curl }(\\mathsf{w} \\cdot \\mathsf{A}) , it follows that \\nabla \\times \\mathsf{A}(\\mathsf{x}) = \\sum \\epsilon_{ijk} \\partial_j A_{lk}(\\mathsf{x}) \\, \\mathsf{e}_l \\otimes \\mathsf{e}_i. Remark Notice how the order of the terms in the dot product \\mathsf{w} \\cdot \\mathsf{A} used in the definition of the curl operation decides the final coordinate expression of the curl of a tensor field. The choice to use \\mathsf{w} \\cdot \\mathsf{A} , instead of \\mathsf{A} \\cdot \\mathsf{w} is purely conventional, and is chosen with an eye towards certain applications.","title":"Curl"},{"location":"tensor_analysis_R3/#integration-of-tensor-fields","text":"The integration of scalar, vector and tensor fields on \\mathbb{R}^3 is now outlined. In particular, attention is focused on line, surface and volume integrals of scalar fields on \\mathbb{R}^3 . Certain important integral theorems that are very useful for continuum mechanics are finally discussed. To keep the discussion simple, the Cartesian coordinate system is adopted throughout this section.","title":"Integration of tensor fields"},{"location":"tensor_analysis_R3/#volume-integrals","text":"Given a scalar field f:U \\subseteq \\mathbb{R}^3 \\to \\mathbb{R} on an open subset U of \\mathbb{R}^3 . The (volume) integral of f over U is written as \\int_U f(\\mathsf{x})\\,dv = \\int_U f(x_1,x_2,x_3) \\, dx_1 dx_2 dx_3. The integral in the expression above is a triple integral with limits defined such that they cover U . The integral is understood in the usual Riemannian sense. Remark It is more useful to interpret the integral \\int_U f(x_1, x_2, x_3)\\,dx_1dx_2dx_3 in the sense of Lebesgue. Since this course will not focus on the analytic aspects on continuum mechanics, it is sufficient for the present purposes to interpret the integral as the limit of a Riemannian sum. Remark The volume element dv in \\int_U f(\\mathsf{x}) \\, dv can be given a precise meaning as a differential 3 -form on \\mathbb{R}^3 . Since this is outside the scope of these notes, it is sufficient to think of dv as just a useful symbol. If \\mathsf{A}:U \\to \\otimes^k TU is a tensor field of order k on U , recall that the Cartesian coordinate representation of \\mathsf{A} is given by \\mathsf{A}(\\mathsf{x}) = \\sum A_{i_1\\ldots i_k}(x_1, x_2, x_3) \\, \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}, for any \\mathsf{x} \\in U . Note that each of the components A_{i_1\\ldots i_k}:U \\to \\mathbb{R} is a scalar field on U . The (volume) integral of \\mathsf{A} over U can be defined in terms of its Cartesian components as follows: \\int_U \\mathsf{A}(\\mathsf{x}) \\, dv = \\sum \\left(\\int_U A_{i_1\\ldots i_k}(x_1, x_2, x_3)\\,dx_1dx_2dx_3\\right) \\mathsf{e}_{i_1} \\otimes \\ldots \\otimes \\mathsf{e}_{i_k}. Despite the fact that the Cartesian coordinate system is used to define the integral above, the change of variables formula ensures that the value of the integral does not depend on the specific choice of coordinate system. The expression for the integral in a general coordinate system will be presented in a later section. Remark It is important to note that the definition of the integral of tensor fields is, in general, not defined over differentiable manifolds. What are integrable are special tensor fields called differential forms . In the special case of Euclidean spaces \\mathbb{R}^n , and in particular, in \\mathbb{R}^3 , this problem does not arise, and the integral of tensor fields is defined in terms of the integral of its component fields, as done here.","title":"Volume integrals"},{"location":"tensor_analysis_R3/#line-integrals","text":"A curve in \\mathbb{R}^3 is a smooth and injective map of the form \\mathsf\\gamma:I\\subseteq\\mathbb{R} \\to \\mathbb{R}^3 , where I is an open subset of \\mathbb{R} . Thus, for every t \\in I , there is a point (\\gamma_1(t), \\gamma_2(t), \\gamma_3(t)) \\in \\mathbb{R}^3 and the set of all such points constitute the curve \\mathsf\\gamma . Let the line C = \\mathsf\\gamma(I) \\subseteq \\mathbb{R}^3 be the image of the curve in \\mathbb{R}^3 . Note that a curve is not be identified with its image here. The tangent vector to the curve \\mathsf\\gamma at \\mathsf\\gamma(t) is the vector \\dot{\\mathsf\\gamma}(t) = (\\dot{\\gamma}_1(t), \\dot{\\gamma}_2(t), \\dot{\\gamma}_3(t)) \\in T_{\\mathsf\\gamma(t)}C , where \\dot{\\gamma}_i(t) denotes the derivative of the function \\gamma_i:I \\to \\mathbb{R} at t . It is assumed that \\dot{\\mathsf\\gamma} \\neq \\mathsf{0} for every t \\in I . A curve that has this property is called a regular curve . Remark The tangent space T_{\\mathsf{x}}C to the curve C at \\mathsf{x} \\in C is defined in a manner similar to the tangent space of an open subset of \\mathbb{R}^3 . Thus, the tangent space to C at \\mathsf{x} consists of all possible tangent vectors to C at \\mathsf{x} . Note that T_{\\mathsf{x}}C \\subset T_{\\mathsf{x}}\\mathbb{R}^3 . Given a scalar field f:U \\subseteq \\mathbb{R}^3 \\to \\mathbb{R} where U is open and C \\subseteq U , the line integral of f over C is defined as follows: \\int_C f(\\mathsf{x}) \\, ds = \\int_I (f \\circ \\mathsf\\gamma)(t) \\lVert \\dot{\\mathsf\\gamma}(t)\\rVert \\, dt. Remark The line element ds used in the line integral \\int_C f(\\mathsf{x})\\,ds can be given a precise definition as a differential 1 -form on \\mathbb{R}^3 . For the present purposes, though, it is sufficient to think of ds as just a useful symbol. Note that the definition of the line integral provided here depends on the choice of a curve \\mathsf\\gamma . Such a curve is called a parametrization of the line C . It can be shown that the value of the line integral is independent of the choice of parametrization, thereby establishing that the definition of the integral is well-defined. The line integral of tensor fields over a regular curve is defined in terms of the line integral of its component scalar fields, as in the case of the volume integral.","title":"Line integrals"},{"location":"tensor_analysis_R3/#surface-integrals","text":"To define surface integrals, it is necessary to introduce a few basic concepts. A parametrized surface in \\mathbb{R}^3 is a smooth and injective map of the form \\mathsf{a}:R \\subseteq \\mathbb{R}^2 \\to \\mathbb{R}^3 . The components of \\mathsf{a} are the maps \\mathsf{a}_i:R \\to \\mathbb{R}^3 , where i = 1,2,3 . Let the surface S = \\mathsf{a}(R) \\subseteq \\mathbb{R}^3 be the image of the parametrized surface \\mathsf{a} in \\mathbb{R}^3 . For any (r,s) \\in R , the tangents to the surface S at \\mathsf{a}(r,s) \\in S are defined as the vectors \\mathsf{t}_1(r,s), \\mathsf{t}_2(r,s) \\in T_{\\mathsf{a}(r,s)}S given by \\mathsf{t}_i(r,s) = \\partial_i \\mathsf{a}(r,s) = \\sum \\partial_i a_j(r,s) \\mathsf{e}_j, \\qquad i = 1,2. The map \\mathsf{a}:R \\to \\mathbb{R}^3 is called a regular surface if \\mathsf{t}_1(r,s) \\times \\mathsf{t}_2(r,s) \\neq \\mathsf{0} for every (r,s) \\in R . Remark The tangent space to the surface S at \\mathsf{x} \\in S is defined along the same lines as the tangent to a curve in \\mathbb{R}^3 or an open subset of \\mathbb{R}^3 . Thus T_{\\mathsf{x}}S consists of all possible tangent vectors to S at the point \\mathsf{x} . Note that T_{\\mathsf{x}}S \\subset T_{\\mathsf{x}}\\mathbb{R}^3 . Let \\mathsf{t}_1(r,s),\\mathsf{t}_2(r,s) \\in T_{\\mathsf{a}(r,s)}S be the tangent vectors at \\mathsf{a}(r,s) \\in S , where (r,s) \\in R , to a regular surface S . The unit normal to the surface S at \\mathsf{a}(r,s) is defined as the vector \\mathsf{n}(r,s) \\in T_{\\mathsf{a}(r,s)}\\mathbb{R}^3 given by \\mathsf{n}(r,s) = \\frac{\\mathsf{t}_1(r,s) \\times \\mathsf{t}_2(r,s)}{\\lVert\\mathsf{t}_1(r,s) \\times \\mathsf{t}_2(r,s)\\rVert}. Note that it is necessary for the surface S to be regular for the unit normal to be well-defined. It is noted, without proof, that different parametrizations of S yield the same unit normal vector, up to a sign, at any point on S . One of the two possible unit normal vectors at point on S is defined as the positive unit normal, and the set of all parametrizations that yield this positive unit normal are said to constitute an orientation on S . If f:U \\subseteq \\mathbb{R}^3 \\to \\mathbb{R} , where U is open in \\mathbb{R}^3 , is a smooth scalar field, and \\mathsf{a}:R \\subseteq \\mathbb{R}^2 \\to \\mathbb{R}^3 is a regular surface such that S = \\mathsf{a}(R) \\subseteq U , then the surface integral of f over S is defined as follows: \\int_S f(\\mathsf{x}) \\, da = \\int_R (f \\circ \\mathsf{a})(r,s) \\lVert \\partial_1 \\mathsf{a}(r,s) \\times \\partial_2 \\mathsf{a}(r,s)\\rVert \\, dr ds. As in the case of the line integral, it can be shown that this definition of the surface integral of the scalar field f is independent of the choice of the parametrization \\mathsf{a} . Remark The area element da in \\int_S f(\\mathsf{x}) \\, da can be given a precise definition as a differential 2 -form on \\mathbb{R}^3 . As remarked before, it is sufficient to consider da as just a useful symbol for this course. The surface integral of tensor fields over a regular surface is defined in terms of the surface integral of its component scalar fields, as in the case of the volume integral.","title":"Surface integrals"},{"location":"tensor_analysis_R3/#divergence-and-gauss-theorems","text":"To conclude this section, two important theorems are presented without proof. The divergence theorem provides a means to convert volume integrals into an integral over the surface bounding the volume, and Stokes\u2019 theorem provides a means to reduce a surface integral to a line integral over the bounding curve of the surface. Suppose that \\mathsf{A}:U \\to \\otimes^k TU is a tensor field of order k on an open subset U \\subseteq \\mathbb{R}^3 . Suppose further that the boundary of U is the set \\partial U . Then, the divergence theorem states that \\int_U \\nabla \\mathsf{A}(\\mathsf{x}) \\, dv = \\int_{\\partial U} \\mathsf{A}(\\mathsf{x}) \\otimes \\mathsf{n}(\\mathsf{x}) \\, da, where \\mathsf{n}(\\mathsf{x}) is the outward unit normal at \\mathsf{x} \\in \\partial U . It is to be noted that the statement of the divergence theorem presented here is valid only for tensor fields defined on Euclidean spaces. A few special cases are now presented to illustrate the divergence theorem. First, if f:U \\to \\mathbb{R} , where U \\subseteq \\mathbb{R}^3 is open, is a scalar field, then the divergence theorem takes the form \\int_U \\nabla f(\\mathsf{x}) \\, dv = \\int_{\\partial U} f(\\mathsf{x})\\mathsf{n}(\\mathsf{x})\\,da. If \\mathsf{v}:U \\to TU is a vector field on U , the statement of the divergence theorem reads \\int_U \\nabla \\mathsf{v}(\\mathsf{x})\\,dv = \\int_{\\partial U} \\mathsf{v}(\\mathsf{x}) \\otimes \\mathsf{n}(\\mathsf{x})\\,da. Taking the trace this equation yields the familiar form of the divergence theorem: \\int_U \\nabla \\cdot \\mathsf{v}(\\mathsf{x}) \\, dv = \\int_{\\partial U} \\mathsf{v}(\\mathsf{x}) \\cdot \\mathsf{n}(\\mathsf{x}) \\, da. Finally, if \\mathsf{A}:U \\to \\otimes^2 TU represents a second order tensor field on U , the foregoing argument can be extended to yield the following form of the divergence theorem: \\int_U \\nabla \\cdot \\mathsf{A}(\\mathsf{x}) \\, dv = \\int_{\\partial U} \\mathsf{A}(\\mathsf{x})\\cdot\\mathsf{n}(\\mathsf{x})\\,da. This statement of the divergence theorem will also prove to be quite handy later on in the study of continuum mechanics. A second theorem that is useful in applications relates surface and line integrals. Specifically, if \\mathsf{v}:U \\to TU is a vector field on an open subset U of \\mathbb{R}^3 , and S is a regular surface in U with boundary \\partial S , then Stokes\u2019 theorem states that \\int_S \\nabla \\times \\mathsf{v}(\\mathsf{x}) \\cdot \\mathsf{n}(\\mathsf{x}) \\, da = \\int_{\\partial S} \\mathsf{v}(\\mathsf{x}) \\cdot \\mathsf{t}(\\mathsf{x}) \\, ds. Here \\mathsf{t}(\\mathsf{x}) \\in T_{\\mathsf{x}}\\partial S is the tangent vector at the point \\mathsf{x} \\in \\partial S on the boundary curve \\partial S of S . Remark There is an elegant theory using the notion of differential forms that generalizes these theorems to the general case of differentiable manifolds. The generalized Stokes\u2019 theorem unifies both the theorems presented above and takes a remarkably simple form in the language of differential forms.","title":"Divergence and Gauss theorems"}]}